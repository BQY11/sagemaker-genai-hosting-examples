{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aded5ace-7694-47d0-a8d9-ec0ea6a3bd54",
   "metadata": {},
   "source": [
    "# Run DeepSeek R1 Llama 70B efficient on Amazon SageMaker AI with SGLang\n",
    "\n",
    "> This notebook has been tested on the Python 3 kernel of a SageMaker Jupternotebook instance on a ml.m5.xlarge instance with 50GB of disk size\n",
    "\n",
    "Amazon SageMaker AI provides the ability to build Docker containers to run on SageMaker endpoints, where they listen for health checks on /ping and receive real-time inference requests on /invocations. Using SageMaker AI for inference offers several benefits:\n",
    "\n",
    "- **Scalability**: SageMaker AI can automatically scale your inference endpoints up and down based on demand, ensuring your models can handle varying workloads.\n",
    "- **High Availability**: SageMaker AI manages the infrastructure and maintains the availability of your inference endpoints, so you don't have to worry about managing the underlying resources.\n",
    "- **Monitoring and Logging**: SageMaker AI provides built-in monitoring and logging capabilities, making it easier to track the performance and health of your inference endpoints.\n",
    "- **Security**: SageMaker AI integrates with other AWS services, such as AWS Identity and Access Management (IAM), to provide robust security controls for your inference workloads.\n",
    "\n",
    "Note that SageMaker provides [pre-built SageMaker AI Docker images](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) that can help you quickly start with the model inference on SageMaker. It also allows you to [bring your own Docker container](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html) and use it inside SageMaker AI for training and inference. To be compatible with SageMaker AI, your container must have the following characteristics:\n",
    "\n",
    "- Your container must have a web server listening on port 8080.\n",
    "- Your container must accept POST requests to the /invocations and /ping real-time endpoints.\n",
    "\n",
    "In this notebook, we'll demonstrate how to adapt the [**SGLang**](https://github.com/sgl-project/sglang) framework to run on SageMaker AI endpoints. SGLang is a serving framework for large language models that provides state-of-the-art performance, including a fast backend runtime for efficient serving with RadixAttention, extensive model support, and an active open-source community. For more information refer to https://docs.sglang.ai/index.html and https://github.com/sgl-project/sglang.\n",
    "\n",
    "By using SGLang and building a custom Docker container, you can run advanced AI models like the [DeepSeek R1 Llama 70B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) on a SageMaker AI endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7a83c-d175-4443-b9cf-26d8bad5fedb",
   "metadata": {},
   "source": [
    "## Prepare the SGLang SageMaker container\n",
    "\n",
    "SageMaker AI makes extensive use of Docker containers for build and runtime tasks. Using containers, you can train machine learning algorithms and deploy models quickly and reliably at any scale. See [this link](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image) to understand how SageMaker AI runs your inference image. \n",
    "\n",
    "- For model inference, SageMaker AI runs the container as:\n",
    "```\n",
    "docker run image serve\n",
    "```\n",
    "\n",
    "- You can provide your entrypoint script as `exec` form to provide instruction of how to perform the inference process, for example:\n",
    "```\n",
    "ENTRYPOINT [\"python\", \"inference.py\"]\n",
    "```\n",
    "\n",
    "- When deploying ML models, one option is to archive and compress the model artifacts into a `tar.gz` format and provided the s3 path of the model artifacts as the `ModelDataUrl` in the [`CreateModel`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html) API request. SageMaker AI will copy the model artifacts from the S3 location \n",
    " and decompresses this tar file into `/opt/ml/model` directory before your container starts for use by your inference code. However, for deploying large models, SageMaker AI allows you to [deploy uncompressed models](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html). In this example, we will show you how to use the uncompressed DeepSeek R1 Distilled Llama 70B model.\n",
    "\n",
    "- To receive inference requests, the container must have a web server listening on port `8080` and must accept `POST` requests to the `/invocations` and `/ping` endpoints.\n",
    "\n",
    "The below diagram shows on a high-level, how you should prepare your own container image to be compatible for SageMaker AI hosting. \n",
    "\n",
    "![inference](./img/sagemaker-real-time-inference.png)\n",
    "\n",
    "\n",
    "If you already have a docker image, you can see more instructions for [adapting your own inference container for SageMaker AI](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html). Also it is important to note that, SageMaker AI provided containers automatically implements a web server for serving requests that responds to `/invocations` and `/ping` (for healthcheck) requests. You can find more about the [prebuilt SageMaker AI docker images for deep learning in our SageMaker doc](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e5664-d4e8-404b-91d9-2c9f3845d4a1",
   "metadata": {},
   "source": [
    "#### Create the entrypoint serve file \n",
    "The `serve` file will used as the `exec` form to be executed at the container starting time. The main command to start sglang in the SageMaker docker image is \n",
    "```\n",
    "python3 -m sglang.launch_server --model-path <your model path> --host 0.0.0.0 --port 8080\n",
    "```\n",
    "Here the `model-path` can be set as `/opt/ml/model` as this is where SageMaker AI will copy the model artifacts from s3 to the endpoint and use `port` **8080** as required by SageMaker hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea56ce-7062-4b5e-8194-3ecb67079ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serve\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Starting server\"\n",
    "\n",
    "SERVER_ARGS=\"--host 0.0.0.0 --port 8080\"\n",
    "\n",
    "if [ -n \"$TENSOR_PARALLEL_DEGREE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --tp-size ${TENSOR_PARALLEL_DEGREE}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$DATA_PARALLEL_DEGREE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --dp-size ${DATA_PARALLEL_DEGREE}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$EXPERT_PARALLEL_DEGREE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --ep-size ${EXPERT_PARALLEL_DEGREE}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$MEM_FRACTION_STATIC\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --mem-fraction-static ${MEM_FRACTION_STATIC}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$QUANTIZATION\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --quantization ${QUANTIZATION}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$CHUNKED_PREFILL_SIZE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --chunked-prefill-size ${CHUNKED_PREFILL_SIZE}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$MODEL_ID\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --model-path ${MODEL_ID}\"\n",
    "else\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --model-path /opt/ml/model\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$TORCH_COMPILE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --enable-torch-compile\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$TORCHAO_CONFIG\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --torchao-config ${TORCHAO_CONFIG}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$KV_CACHE_DTYPE\" ]; then\n",
    "    SERVER_ARGS=\"{$SERVER_ARGS} --kv-cache-dtype ${KV_CACHE_DTYPE}\"\n",
    "fi\n",
    "\n",
    "python3 -m sglang.launch_server $SERVER_ARGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076da43-2458-403c-9dde-0c27107f4f0b",
   "metadata": {},
   "source": [
    "SGLang has provided the based [Dockerfile here](https://github.com/sgl-project/sglang/blob/main/docker/Dockerfile). You can directly extend the base image with\n",
    "\n",
    "```\n",
    "# Extend from the base sglang image\n",
    "FROM lmsysorg/sglang:latest\n",
    "```\n",
    "\n",
    "In this example, we have copied the whole base Dockerfile and added the below lines to make it compatible with SageMaker\n",
    "```\n",
    "COPY serve /usr/bin/serve\n",
    "RUN chmod 777 /usr/bin/serve\n",
    "\n",
    "ENTRYPOINT [ \"/usr/bin/serve\" ]\n",
    "```\n",
    "You can add additional layers in the container image to accomodate your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ad6dc-ae4e-4ac6-8c55-42359a04dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "ARG CUDA_VERSION=12.5.1\n",
    "\n",
    "FROM nvcr.io/nvidia/tritonserver:24.04-py3-min\n",
    "\n",
    "ARG BUILD_TYPE=all\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "RUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n",
    "    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n",
    "    && apt update -y \\\n",
    "    && apt install software-properties-common -y \\\n",
    "    && add-apt-repository ppa:deadsnakes/ppa -y && apt update \\\n",
    "    && apt install python3.10 python3.10-dev -y \\\n",
    "    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 \\\n",
    "    && update-alternatives --set python3 /usr/bin/python3.10 && apt install python3.10-distutils -y \\\n",
    "    && apt install curl git sudo libibverbs-dev -y \\\n",
    "    && apt install -y rdma-core infiniband-diags openssh-server perftest ibverbs-providers libibumad3 libibverbs1 libnl-3-200 libnl-route-3-200 librdmacm1 \\\n",
    "    && curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python3 get-pip.py \\\n",
    "    && python3 --version \\\n",
    "    && python3 -m pip --version \\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\n",
    "    && apt clean\n",
    "\n",
    "# For openbmb/MiniCPM models\n",
    "RUN pip3 install datamodel_code_generator\n",
    "\n",
    "WORKDIR /sgl-workspace\n",
    "\n",
    "ARG CUDA_VERSION\n",
    "RUN python3 -m pip install --upgrade pip setuptools wheel html5lib six \\\n",
    "    && git clone --depth=1 https://github.com/sgl-project/sglang.git \\\n",
    "    && if [ \"$CUDA_VERSION\" = \"12.1.1\" ]; then \\\n",
    "         python3 -m pip install torch --index-url https://download.pytorch.org/whl/cu121; \\\n",
    "       elif [ \"$CUDA_VERSION\" = \"12.4.1\" ]; then \\\n",
    "         python3 -m pip install torch --index-url https://download.pytorch.org/whl/cu124; \\\n",
    "       elif [ \"$CUDA_VERSION\" = \"12.5.1\" ]; then \\\n",
    "         python3 -m pip install torch --index-url https://download.pytorch.org/whl/cu124; \\\n",
    "       elif [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n",
    "         python3 -m pip install torch --index-url https://download.pytorch.org/whl/cu118; \\\n",
    "         python3 -m pip install sgl-kernel -i https://docs.sglang.ai/whl/cu118; \\\n",
    "       else \\\n",
    "         echo \"Unsupported CUDA version: $CUDA_VERSION\" && exit 1; \\\n",
    "       fi \\\n",
    "    && cd sglang \\\n",
    "    && if [ \"$BUILD_TYPE\" = \"srt\" ]; then \\\n",
    "         if [ \"$CUDA_VERSION\" = \"12.1.1\" ]; then \\\n",
    "           python3 -m pip --no-cache-dir install -e \"python[srt]\" --find-links https://flashinfer.ai/whl/cu121/torch2.5/flashinfer-python; \\\n",
    "         elif [ \"$CUDA_VERSION\" = \"12.4.1\" ]; then \\\n",
    "           python3 -m pip --no-cache-dir install -e \"python[srt]\" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python; \\\n",
    "         elif [ \"$CUDA_VERSION\" = \"12.5.1\" ]; then \\\n",
    "           python3 -m pip --no-cache-dir install -e \"python[srt]\" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python; \\\n",
    "         elif [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n",
    "           python3 -m pip --no-cache-dir install -e \"python[srt]\" --find-links https://flashinfer.ai/whl/cu118/torch2.5/flashinfer-python; \\\n",
    "           python3 -m pip install sgl-kernel -i https://docs.sglang.ai/whl/cu118; \\\n",
    "         else \\\n",
    "           echo \"Unsupported CUDA version: $CUDA_VERSION\" && exit 1; \\\n",
    "         fi; \\\n",
    "       else \\\n",
    "         if [ \"$CUDA_VERSION\" = \"12.1.1\" ]; then \\\n",
    "           python3 -m pip --no-cache-dir install -e \"python[all]\" --find-links https://flashinfer.ai/whl/cu121/torch2.5/flashinfer-python; \\\n",
    "         elif [ \"$CUDA_VERSION\" = \"12.4.1\" ]; then \\\n",
    "           python3 -m pip --no-cache-dir install -e \"python[all]\" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python; \\\n",
    "         elif [ \"$CUDA_VERSION\" = \"12.5.1\" ]; then \\\n",
    "           python3 -m pip --no-cache-dir install -e \"python[all]\" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python; \\\n",
    "         elif [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n",
    "           python3 -m pip --no-cache-dir install -e \"python[all]\" --find-links https://flashinfer.ai/whl/cu118/torch2.5/flashinfer-python; \\\n",
    "           python3 -m pip install sgl-kernel -i https://docs.sglang.ai/whl/cu118; \\\n",
    "         else \\\n",
    "           echo \"Unsupported CUDA version: $CUDA_VERSION\" && exit 1; \\\n",
    "         fi; \\\n",
    "       fi\n",
    "\n",
    "ENV DEBIAN_FRONTEND=interactive\n",
    "\n",
    "COPY serve /usr/bin/serve\n",
    "RUN chmod 777 /usr/bin/serve\n",
    "\n",
    "ENTRYPOINT [ \"/usr/bin/serve\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93158bf1-2cb4-4de3-8f82-9a351eb7dc8d",
   "metadata": {},
   "source": [
    "Next, we will need to create an ECR repository for the custom docker image and build the image locally and push to the ECR repository. Note that you need to make sure the IAM role you used here has permission to push to ECR. \n",
    "\n",
    "The below cell might take sometime, please be patient. If you have already built the docker image from other development environment, please feel free to skip the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3001d-88c2-4610-847e-b018dffb2074",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
    "REGION=$(aws configure get region)\n",
    "REPOSITORY_NAME=sglang-sagemaker\n",
    "\n",
    "# Create ECR repository if needed\n",
    "if aws ecr describe-repositories --repository-names \"${REPOSITORY_NAME}\" &>/dev/null; then\n",
    "    echo \"Repository ${REPOSITORY_NAME} already exists\"\n",
    "else\n",
    "    echo \"Creating ECR repository ${REPOSITORY_NAME}...\"\n",
    "    aws ecr create-repository \\\n",
    "        --repository-name \"${REPOSITORY_NAME}\" \\\n",
    "        --region \"${REGION}\"\n",
    "fi\n",
    "\n",
    "#build docker image and push to ECR repository\n",
    "docker build -t sglang .\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n",
    "docker tag sglang:latest $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest\n",
    "docker push $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7620d7-05d4-49be-9f4c-9dab1e25f17d",
   "metadata": {},
   "source": [
    "### Create SageMaker AI endpoint for DeepSeek R1 distilled Llama 70B model\n",
    "In this example, we will use the DeepSeek R1 distilled Llama 70B model artifacts directly [SageMaker Jumpstart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html). This way, it saves you time to download the model from HuggingFace and upload to S3.\n",
    "\n",
    "SageMaker JumpStart provides pretrained, open-source models for a wide range of problem types to help you get started with machine learning. You can incrementally train and tune these models before deployment. JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker AI.\n",
    "\n",
    "You can see the Deepseek model on SageMaker Jumpstart in SageMaker AI Studio as shown below:\n",
    "\n",
    "![deepseek-jumpstart](./img/jumpstart-deepseek-model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49095ae2-0f51-49e3-acf4-e554fff52cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "model_id, model_version = \"deepseek-llm-r1-distill-llama-70b\", \"*\"\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "model_data=model.model_data['S3DataSource']['S3Uri']\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66560cdc-9dda-478a-bac2-88ed2e6feef3",
   "metadata": {},
   "source": [
    "Then we will create the [SageMaker model](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/model.py#L149) with the custom docker image and model data available on s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6322e5d-23ad-4eb1-9d50-c88bd0ee4996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.session import Session\n",
    "session = Session()\n",
    "region = session._region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "ecr_uri = f'{session.account_id()}.dkr.ecr.{region}.amazonaws.com/sglang-sagemaker:latest'\n",
    "\n",
    "model = Model(\n",
    "    model_data={\"S3DataSource\": {\n",
    "                    \"S3Uri\": model_data,\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"CompressionType\": \"None\",\n",
    "                },\n",
    "    },\n",
    "    role=role,\n",
    "    image_uri=ecr_uri,\n",
    "    env={\n",
    "        'TENSOR_PARALLEL_DEGREE': '8'\n",
    "    },\n",
    "    predictor_cls=Predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7448b-0e70-4bfa-9d39-31757c19de6f",
   "metadata": {},
   "source": [
    "You can simply call the [`deploy` function](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/model.py#L149) from the SageMaker Model class to deploy the model to an endpoint and it will return a [`Predictor`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/base_predictor.py#L98) object to perform invocation against this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0182ef-7f2b-4ded-b407-a8411bd391e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.g5.48xlarge', # you can also change to p4d.24xlarge\n",
    "    serializer=JSONSerializer(), \n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b803a7-dbce-43d3-bae3-ad44f439a50b",
   "metadata": {},
   "source": [
    "OpenAI API chat completion interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da592f-e09b-4071-8d6d-f9063112f576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = predictor.predict({\n",
    "    'model':'mymodel',\n",
    "    'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    'temperature':0,\n",
    "    'max_tokens':200,\n",
    "    'top_logprobs': 2,\n",
    "    'logprobs': True\n",
    "})\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e285c-b98c-4e23-87dd-739898c32dec",
   "metadata": {},
   "source": [
    "#### Invoke endpoint with boto3\n",
    "Note that you can also invoke the endpoint with boto3. If you have an existing endpoint, you don't need to recreate the `predictor` and can follow below example to invoke the endpoint with an endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817557b-8204-4171-bcaa-54395f8b8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "endpoint_name = predictor.endpoint_name # you can manually set the endpoint name with an existing endpoint\n",
    "\n",
    "prompt = {\n",
    "    'model':'mymodel',\n",
    "    'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    'temperature':0,\n",
    "    'max_tokens':512,\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015339db-9d8c-481b-bca9-5f7955991a5b",
   "metadata": {},
   "source": [
    "#### Streaming response from the endpoint\n",
    "Additionally, SGLang allows you to invoke the endpoint and receive streaming response. Below is an example of how to interact with the endpoint with streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f8ce7-fcf1-4276-a1ae-98dc3ef7638b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "# Example class that processes an inference stream:\n",
    "class SmrInferenceStream:\n",
    "    \n",
    "    def __init__(self, sagemaker_runtime, endpoint_name):\n",
    "        self.sagemaker_runtime = sagemaker_runtime\n",
    "        self.endpoint_name = endpoint_name\n",
    "        # A buffered I/O stream to combine the payload parts:\n",
    "        self.buff = io.BytesIO() \n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def stream_inference(self, request_body):\n",
    "        # Gets a streaming inference response \n",
    "        # from the specified model endpoint:\n",
    "        response = self.sagemaker_runtime\\\n",
    "            .invoke_endpoint_with_response_stream(\n",
    "                EndpointName=self.endpoint_name, \n",
    "                Body=json.dumps(request_body), \n",
    "                ContentType=\"application/json\"\n",
    "        )\n",
    "        # Gets the EventStream object returned by the SDK:\n",
    "        event_stream = response['Body']\n",
    "        for event in event_stream:\n",
    "            # Passes the contents of each payload part\n",
    "            # to be concatenated:\n",
    "            self._write(event['PayloadPart']['Bytes'])\n",
    "            # Iterates over lines to parse whole JSON objects:\n",
    "            for line in self._readlines():\n",
    "                line = line.decode('utf-8')[len('data: '):]\n",
    "                # print(line)\n",
    "                try:\n",
    "                    resp = json.loads(line)\n",
    "                except:\n",
    "                    continue\n",
    "                if len(line)>0 and type(resp) == dict:\n",
    "                    # if len(resp.get('choices')) == 0:\n",
    "                    #     continue\n",
    "                    part = resp.get('choices')[0]['delta']['content']\n",
    "                    \n",
    "                else:\n",
    "                    part = resp\n",
    "                # Returns parts incrementally:\n",
    "                yield part\n",
    "    \n",
    "    # Writes to the buffer to concatenate the contents of the parts:\n",
    "    def _write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "\n",
    "    # The JSON objects in buffer end with '\\n'.\n",
    "    # This method reads lines to yield a series of JSON objects:\n",
    "    def _readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            self.read_pos += len(line)\n",
    "            yield line[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f29fdb-e4be-439c-a6e4-dc05facc6b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    'model':'mymodel',\n",
    "    'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    'temperature':0,\n",
    "    'max_tokens':512,\n",
    "    # 'top_logprobs': 2,\n",
    "    # 'logprobs': True,\n",
    "    'stream': True,\n",
    "    'stream_options': {'include_usage': True}\n",
    "}\n",
    "\n",
    "smr_inference_stream = SmrInferenceStream(\n",
    "    sagemaker_runtime, predictor.endpoint_name)\n",
    "stream = smr_inference_stream.stream_inference(request_body)\n",
    "for part in stream:\n",
    "    print(part, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276cf57c-b1df-428a-a017-b348e7a07e5e",
   "metadata": {},
   "source": [
    "#### Run an Inference Recommendations Job\n",
    "\n",
    "The Python SDK method for Inference Recommender is `.right_size()`. This will create an inference recommender job to load test your model across a number of configurations with a sample payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc127327-9d82-462b-a0ed-0860d7707997",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = predictor.serializer.serialize({\n",
    "    'model':'mymodel',\n",
    "    'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    'temperature':0,\n",
    "    'max_tokens':200,\n",
    "    'top_logprobs': 2,\n",
    "    'logprobs': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e4ae4-1b47-455b-87a5-07844413b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_raw = json.dumps(raw)\n",
    "!echo {json_raw} > samplepayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17bc54-c5e1-4a3e-b365-f6f657c0fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat samplepayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ecaed-ce04-45b5-9f57-6f40eddd5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf payload.tar.gz samplepayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712dbce2-0134-4c48-8f81-36e13ef563e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = session.upload_data('./payload.tar.gz', key_prefix=f'deepseek-r1-sglang-payload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b301e3e-2ce4-44dd-8ec9-591c51462171",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce36af-e277-4a91-bd35-d634ff0c3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.parameter import CategoricalParameter\n",
    "from sagemaker.inference_recommender import Phase, ModelLatencyThreshold\n",
    "\n",
    "\n",
    "model.right_size(payload, \n",
    "                         supported_content_types=['application/json'],\n",
    "                         supported_instance_types=['ml.p4d.24xlarge', 'ml.g5.48xlarge', \"ml.g6.48xlarge\", \"ml.g6e.48xlarge\"],\n",
    "                         framework='PYTORCH',\n",
    "                         job_duration_in_seconds=7200,\n",
    "                         hyperparameter_ranges=[{\n",
    "                             'instance_types': CategoricalParameter(['ml.p4d.24xlarge', 'ml.g5.48xlarge', \"ml.g6.48xlarge\", \"ml.g6e.48xlarge\"]),\n",
    "                             'TENSOR_PARALLEL_DEGREE': CategoricalParameter(['8'])\n",
    "                         }],\n",
    "                         phases=[Phase(150, 1, 1), Phase(150, 2, 1), Phase(150, 7, 1)],\n",
    "                         traffic_type='PHASES',\n",
    "                         model_latency_thresholds=[ModelLatencyThreshold('P99', 10000)],\n",
    "                         max_invocations=480,\n",
    "                         log_level=\"Quiet\",\n",
    "                         max_parallel_tests=4\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacb334-1383-4bab-a118-683d3328b929",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Make sure to delete the endpoint and other artifacts that were created to avoid unnecessary cost. You can also go to SageMaker AI console to delete all the resources created in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696e16ca-ebb6-4d2a-84d0-69743e74793f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
