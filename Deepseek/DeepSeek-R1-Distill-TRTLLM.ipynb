{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy deepseek-ai/DeepSeek-R1-Distill-* models on Amazon SageMaker using LMI container\n",
    "\n",
    "Let's get started deploying one of the most capable open-source reasoning models available today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: [DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepSeek-R1 is an open-source reasoning model developed by [DeepSeek](https://www.deepseek.com/). It is designed to handle tasks requiring logical inference, mathematical problem-solving, and real-time decision-making. Notably, DeepSeek-R1 achieves performance comparable to leading Foundation Models across various benchmarks, including math, code, and reasoning tasks. \n",
    "\n",
    "The DeepSeek-R1 series includes several variants, each with distinct training methodologies and objectives:\n",
    "\n",
    "1. **DeepSeek-R1-Zero**: This model was trained entirely through reinforcement learning (RL) without any supervised fine-tuning (SFT). While it developed strong reasoning capabilities, it faced challenges such as less readable outputs and occasional mixing of languages within responses, making it less practical for real-world applications. \n",
    "\n",
    "\n",
    "2. **DeepSeek-R1**: To address the limitations of R1-Zero, DeepSeek-R1 was developed using a hybrid approach that combines reinforcement learning with supervised fine-tuning. This method incorporated curated datasets to improve the model's readability and coherence, effectively reducing issues like language mixing and fragmented reasoning. As a result, DeepSeek-R1 is more suitable for practical use. \n",
    "\n",
    "\n",
    "3. **DeepSeek-R1 Distilled Models**: These are smaller, more efficient versions of the original DeepSeek-R1 model, created through a process called distillation. Distillation involves training a compact model to replicate the behavior of a larger model, thereby retaining much of its reasoning power while reducing computational demands. DeepSeek has released several distilled models based on different architectures, such as Qwen and Llama, with varying parameter sizes (e.g., 1.5B, 7B, 14B, 32B, and 70B). These distilled models offer a balance between performance and resource efficiency, making them accessible for a wider range of applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below captures the DeepSeek R1 non-distilled model variants,\n",
    "\n",
    "| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** | **Suggested Instances for Hosting** |\n",
    "| :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n",
    "| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   | `ml.p5.48xlarge`, `p5e.48xlarge` |\n",
    "| DeepSeek-R1   | 671B | 37B |  128K   | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   | `ml.p5.48xlarge`, `p5e.48xlarge` |\n",
    "\n",
    "The table below captures the DeepSeek R1 distilled model variants,\n",
    "\n",
    "| **Model** | **Base Model** | **Download** | **Suggested Instances for Hosting** |\n",
    "| :------------: | :------------: | :------------: | :------------: |\n",
    "| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   | `ml.g4dn.xlarge`, `ml.g5.xlarge`, `ml.g6.xlarge`, `ml.g6e.xlarge`   |\n",
    "| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   | `ml.g5.2xlarge`, `ml.g6.2xlarge`, `ml.g6e.2xlarge` |\n",
    "| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   | `ml.g5.2xlarge`, `ml.g6.2xlarge`, `ml.g6e.2xlarge`   |\n",
    "| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   | `ml.g4dn.12xlarge`, `ml.g5.12xlarge`, `ml.g6.12xlarge`, `ml.g6e.12xlarge`   |\n",
    "| DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   | `ml.g4dn.12xlarge`, `ml.g5.12xlarge`, `ml.g6.12xlarge`, `ml.g6e.12xlarge` |\n",
    "| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   | `ml.g5.48xlarge`, `ml.g6.48xlarge`, `ml.g6e.48xlarge`, `ml.p4d.24xlarge`  |\n",
    "\n",
    "> âš  **Warning:** This is not an exhaustive list of compatible instances, please refer to the SageMaker supported instance list here: https://aws.amazon.com/sagemaker-ai/pricing/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy QwQ to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"sagemaker>=2.237.1\" --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import huggingface_hub\n",
    "from pathlib import Path\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve the LMI DLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"0.30.0\"\n",
    "inference_image = sagemaker.image_uris.retrieve(\"djl-tensorrtllm\", region=region, version=version)\n",
    "print(f\"Inference image: {inference_image}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy deepseek-ai/DeepSeek-R1-Distill-* to Amazon SageMaker\n",
    "\n",
    "To deploy a model to Amazon SageMaker we create a `Model` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `ml.p4d.24xlarge` instance type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can deploy any distilled models using this notebook. All you need to do is to change \"HF_MODEL_ID\" parameter in the cell below to any of the following:\n",
    "- deepseek-ai/DeepSeek-R1-Distill-Llama-70B\n",
    "- deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
    "- ~~deepseek-ai/DeepSeek-R1-Distill-Qwen-32B~~\n",
    "- ~~deepseek-ai/DeepSeek-R1-Distill-Qwen-14B~~\n",
    "- ~~deepseek-ai/DeepSeek-R1-Distill-Qwen-7B~~\n",
    "- ~~deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B~~\n",
    "\n",
    "Version of TRT-LLM in LMI container 0.30 does NOT support Qwen architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ahead of time compilation (one time activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "\n",
    "hf_local_download_dir = Path.cwd() / \"model_repo\"\n",
    "hf_local_download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "allow_patterns = [\"*.json\", \"*.safetensors\", \"*.pt\", \"*.txt\", \"*.model\", \"*.tiktoken\", \"*.gguf\"]\n",
    "\n",
    "# - Leverage the snapshot library to download the model since the model is stored in repository using LFS\n",
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=model_id,\n",
    "    local_dir=hf_local_download_dir,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf model_repo/.ipynb_checkpoints\n",
    "!rm -rf model_repo/.cache\n",
    "!rm -rf model_repo/.gitattributes\n",
    "!rm -rf model_repo/original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = sess.upload_data(\n",
    "    path=hf_local_download_dir.as_posix(),\n",
    "    bucket=bucket,\n",
    "    key_prefix=\"inference-model\",\n",
    ")\n",
    "model_uri = model_uri + \"/\" #need to point towards the uncompressed model artifacts\n",
    "model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {model_uri} #verify model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"inference-model-trt\"\n",
    "model_name = sagemaker.utils.name_from_base(prefix)\n",
    "output_location = f\"s3://{bucket}/{prefix}/\"\n",
    "instance_type = \"ml.p4d.24xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = model_name\n",
    "job_timeout = 7200\n",
    "\n",
    "response = sm_client.create_optimization_job(\n",
    "    OptimizationJobName=job_name,\n",
    "    RoleArn=role,\n",
    "    ModelSource={\n",
    "        'S3': {\n",
    "            'S3Uri': model_uri,\n",
    "        }\n",
    "    },\n",
    "    DeploymentInstanceType=instance_type,\n",
    "    OptimizationEnvironment={},\n",
    "    OptimizationConfigs=[\n",
    "        {\n",
    "            'ModelCompilationConfig': {\n",
    "                'Image': inference_image,\n",
    "                'OverrideEnvironment': {\n",
    "                    \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "                    \"OPTION_MAX_INPUT_LEN\": \"4096\",\n",
    "                    \"OPTION_MAX_OUTPUT_LEN\": \"4096\",\n",
    "                    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "                    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"8\",\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': output_location\n",
    "    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': job_timeout,\n",
    "        'MaxWaitTimeInSeconds': job_timeout,\n",
    "        'MaxPendingTimeInSeconds': job_timeout\n",
    "    },\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.wait_for_optimization_job(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    \"HF_MODEL_ID\": output_location,\n",
    "    \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "    \"OPTION_MAX_INPUT_LEN\": \"4096\",\n",
    "    \"OPTION_MAX_OUTPUT_LEN\": \"4096\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"8\",\n",
    "}\n",
    "\n",
    "lmi_model = sagemaker.Model(\n",
    "    image_uri = inference_image,\n",
    "    env = env,\n",
    "    role = role,\n",
    "    name = model_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `Model` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.p4d.24xlarge` instance type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LMI will automatically:***\n",
    "- convert model to TensorRT-LLM artifacts (if it was converted ahead of time by running optimization job)\n",
    "- distribute and shard the model across all GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "llm = lmi_model.deploy(\n",
    "  initial_instance_count = 1,\n",
    "  instance_type = instance_type,\n",
    "  container_startup_health_check_timeout = 3600, \n",
    "  endpoint_name = model_name,\n",
    ")\n",
    "\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = model_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_deployment = \"\"\"\n",
    "How to deploy the DeepSeek R1 model on Amazon SageMaker using LMI container?\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful ML assistant who is an expert in SageMaker hosting.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Create a recipe here.\n",
    "\n",
    "{recipe_deployment}\n",
    "\n",
    "Provide the summary directly, without any introduction or preamble. Do not start the response with \"Here is a...\".<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.predict(\n",
    "    {\n",
    "        \"inputs\": prompt_template,\n",
    "        \"parameters\": {\n",
    "            \"do_sample\":True,\n",
    "            \"max_new_tokens\":1024,\n",
    "            \"top_p\":0.9,\n",
    "            \"temperature\":0.6,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['generated_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
