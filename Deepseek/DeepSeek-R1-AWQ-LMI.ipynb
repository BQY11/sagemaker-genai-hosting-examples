{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# Deploy quantized (AWQ) version of DeepSeek R1 on Amazon SageMaker AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831b169-0f08-4ea6-b102-d11ff5780d24",
   "metadata": {},
   "source": [
    "## Introduction: [DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb765c-6c31-4cfe-bcad-ca81f3f62329",
   "metadata": {},
   "source": [
    "DeepSeek-R1 is an open-source reasoning model developed by [DeepSeek](https://www.deepseek.com/). It is designed to handle tasks requiring logical inference, mathematical problem-solving, and real-time decision-making. Notably, DeepSeek-R1 achieves performance comparable to leading Foundation Models across various benchmarks, including math, code, and reasoning tasks. \n",
    "\n",
    "The DeepSeek-R1 series includes several variants, each with distinct training methodologies and objectives:\n",
    "\n",
    "1. **DeepSeek-R1-Zero**: This model was trained entirely through reinforcement learning (RL) without any supervised fine-tuning (SFT). While it developed strong reasoning capabilities, it faced challenges such as less readable outputs and occasional mixing of languages within responses, making it less practical for real-world applications. \n",
    "\n",
    "\n",
    "2. **DeepSeek-R1**: To address the limitations of R1-Zero, DeepSeek-R1 was developed using a hybrid approach that combines reinforcement learning with supervised fine-tuning. This method incorporated curated datasets to improve the model's readability and coherence, effectively reducing issues like language mixing and fragmented reasoning. As a result, DeepSeek-R1 is more suitable for practical use. \n",
    "\n",
    "\n",
    "3. **DeepSeek-R1 Distilled Models**: These are smaller, more efficient versions of the original DeepSeek-R1 model, created through a process called distillation. Distillation involves training a compact model to replicate the behavior of a larger model, thereby retaining much of its reasoning power while reducing computational demands. DeepSeek has released several distilled models based on different architectures, such as Qwen and Llama, with varying parameter sizes (e.g., 1.5B, 7B, 14B, 32B, and 70B). These distilled models offer a balance between performance and resource efficiency, making them accessible for a wider range of applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a581d4f-db69-41a0-954c-32355e916317",
   "metadata": {},
   "source": [
    "The table below captures the DeepSeek R1 non-distilled model variants,\n",
    "\n",
    "| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** | **Suggested Instances for Hosting** |\n",
    "| :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n",
    "| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   | `ml.p5e.48xlarge` |\n",
    "| DeepSeek-R1   | 671B | 37B |  128K   | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   | `ml.p5e.48xlarge` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0015ae-37b5-497a-bc25-85ffb1f8c46a",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy the model to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdb7fb-319f-4db9-92ea-0c7a94212365",
   "metadata": {},
   "source": [
    "## 2. Retrieve the LMI DLC\n",
    "\n",
    "See [this](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91002f-04c1-450f-a414-ea262870a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_image = sagemaker.image_uris.retrieve(framework=\"djl-lmi\", region=region, version=\"0.30.0\")\n",
    "#\n",
    "# Temporary Override:\n",
    "#\n",
    "vllm_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\"\n",
    "\n",
    "print(f\"LMI-vLLM image: {vllm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea528dc-519a-4fee-92a4-ed0829d68897",
   "metadata": {},
   "source": [
    "## 3. Deploy cognitivecomputations/DeepSeek-R1-AWQ to Amazon SageMaker\n",
    "\n",
    "To deploy a model to Amazon SageMaker we create a `Model` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `ml.p4de.24xlarge` instance type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b45a5-0c97-470e-84e2-5bb1d01f089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_prefix = \"models/DeepSeek-R1-AWQ\"\n",
    "gpu_instance_type = \"ml.p4de.24xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634e71f-a190-408f-bcb9-a314cd057e30",
   "metadata": {},
   "source": [
    "## Deploy using LMI container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee4fdf8-5d21-47f9-bad3-2773b7edd140",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "option.model_id=cognitivecomputations/DeepSeek-R1-AWQ\n",
    "option.rolling_batch=vllm\n",
    "option.dtype=fp16\n",
    "option.quantize=awq_marlin\n",
    "option.trust_remote_code=True\n",
    "option.tensor_parallel_degree=max\n",
    "option.gpu_memory_utilization=.87\n",
    "option.kv_cache_dtype=fp8_e4m3\n",
    "option.max_model_len=17600\n",
    "option.max_rolling_batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e35679-5704-4e9b-8f0c-7e1811e0aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "vllm==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a44ea-559c-4082-9b4d-ba58fae91f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "tar czvf config.tar.gz ./serving.properties ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea97c0c-d546-41fe-93c8-b2714afe59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_uri = sess.upload_data(\"config.tar.gz\", bucket, model_config_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1341f2e-6539-490d-9574-600d030e54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = sagemaker.utils.name_from_base(\"DeepSeek-R1-AWQ\")\n",
    "endpoint_name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf134bf-5c10-433c-a7a0-6a3a24717979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.Model(name = model_name, \n",
    "                        image_uri = vllm_image, \n",
    "                        model_data = config_uri,\n",
    "                        role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece16cc-0ecf-4ba5-9de5-159bc1a9fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(initial_instance_count = 1,\n",
    "             instance_type = gpu_instance_type,\n",
    "             endpoint_name = endpoint_name,\n",
    "             container_startup_health_check_timeout = 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b71b61-5ea8-421d-9e12-756114943465",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5da90-94ee-4920-a942-2008d5dd0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is Amazon SageMaker?\"\n",
    "\n",
    "res = llm.predict({\"inputs\": prompt, \"parameters\": {\"temperature\": 0.9, \"max_tokens\": 256}})\n",
    "print(res[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0230b-f05c-4326-8449-88d0585fb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"\"\"\n",
    "A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively.\n",
    "\n",
    "User: 9.11 and 9.8, which is greater?\n",
    "Assistant: <think>\n",
    "Think step by step\n",
    "</think>\n",
    "<answer>\n",
    "[Solution will be provided here]\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14100c0-555d-4a83-b8d5-61e1b9b83925",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = smr_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question_1}],\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stream\": True\n",
    "    })\n",
    ")\n",
    "\n",
    "for event in response['Body']:\n",
    "    try:\n",
    "        line = event['PayloadPart']['Bytes'].decode(\"utf-8\")\n",
    "        chunk = json.loads(line)\n",
    "        if 'choices' in chunk and len(chunk['choices']) > 0:\n",
    "            content = chunk['choices'][0].get('delta', {}).get('content', '')\n",
    "            finish_reason = chunk['choices'][0].get('delta', {}).get('finish_reason', '')\n",
    "            print(content, end='', flush=True)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON:\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233db60-a406-4897-b7e7-8adb09435067",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"\"\"\n",
    "A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively.\n",
    "\n",
    "User: Plan a 1 week trip to Europe in March, I like historical sites\n",
    "Assistant: <think>\n",
    "Think step by step\n",
    "</think>\n",
    "<answer>\n",
    "[Solution will be provided here]\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12ab6c-c2d9-4cf2-80ce-f8159a7170e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = smr_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question_2}],\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stream\": True\n",
    "    })\n",
    ")\n",
    "\n",
    "for event in response['Body']:\n",
    "    try:\n",
    "        line = event['PayloadPart']['Bytes'].decode()\n",
    "        chunk = json.loads(line)\n",
    "        if 'choices' in chunk and len(chunk['choices']) > 0:\n",
    "            content = chunk['choices'][0].get('delta', {}).get('content', '')\n",
    "            finish_reason = chunk['choices'][0].get('delta', {}).get('finish_reason', '')\n",
    "            print(content, end='', flush=True)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON:\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ffa5e0-664a-44b8-84b3-fec9e40cc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294fba7e-0c51-4c4b-9f48-d977525456f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
