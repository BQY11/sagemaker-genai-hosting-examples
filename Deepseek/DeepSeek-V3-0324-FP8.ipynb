{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4cae57",
   "metadata": {},
   "source": [
    "\n",
    "# Deploying DeepSeek-V3-0324 on SageMaker\n",
    "\n",
    "This notebook demonstrates deploying and running inference with the DeepSeek-V3-0324 model from DeepSeek launched on 03/24/2025. At the time of the launch, this model is the best performing non-reasoning model. \n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, we'll upgrade the SageMaker SDK to ensure compatibility with the latest features, particularly those needed for large language model deployment and streaming inference.\n",
    "\n",
    "> **Note**: The `--quiet` and `--no-warn-conflicts` flags are used to minimize unnecessary output while installing dependencies.\n",
    "\n",
    "> ⚠️ **Important**: After running the installation cell below, you may need to restart your notebook kernel to ensure the updated packages are properly loaded. To do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e49ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1af08a",
   "metadata": {},
   "source": [
    "# Deploying and Interacting with DeepSeek-R1 LLM on SageMaker\n",
    "\n",
    "This notebook demonstrates how to deploy and interact with the DeepSeek-R1 language model using Amazon SageMaker. We'll cover:\n",
    "\n",
    "1. Setting up SageMaker resources and permissions\n",
    "2. Deploying the model using SageMaker LMI (Large Model Inference Container powered by Vllm)\n",
    "3. Implementing a streaming chat interface\n",
    "\n",
    "\n",
    "## Setup SageMaker Environment\n",
    "\n",
    "First, we'll import the necessary libraries and initialize our SageMaker session. This includes:\n",
    "- `boto3` for AWS API interactions\n",
    "- `sagemaker` SDK for model deployment and management\n",
    "- Setting up IAM roles and session objects\n",
    "\n",
    "The code below establishes these basic requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca4505-aa3b-477b-9b90-aa3ef8f26576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb6e81",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying DeepSeek-V3, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "- **P5 Instance**: AWS's latest GPU instance type optimized for large model inference\n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.p5en.48xlarge` or `ml.p5e.48xlarge` instances which offer:\n",
    "  - 8 NVIDIA H200 GPUs\n",
    "  - 1128 GB of memory\n",
    "  - High network bandwidth for optimal inference performance\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region. Replace `us-east-2` with your region if different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a91b2-29c3-4056-944f-130759ec5f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define region where you have capacity\n",
    "REGION = 'us-east-2'  \n",
    "\n",
    "#Select the latest container. Check the link for the latest available version https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers \n",
    "CONTAINER_VERSION = '0.32.0-lmi14.0.0-cu126'\n",
    "\n",
    "# Construct container URI\n",
    "container_uri = f'763104351884.dkr.ecr.{REGION}.amazonaws.com/djl-inference:{CONTAINER_VERSION}'\n",
    "\n",
    "# Select instance type\n",
    "instance_type = \"ml.p5en.48xlarge\"  # Alternative: \"ml.p5e.48xlarge\"\n",
    "\n",
    "# Validate region and print configuration\n",
    "if REGION != sess.boto_region_name:\n",
    "    print(f\"⚠️ Warning: Container region ({REGION}) differs from session region ({sess.boto_region_name})\")\n",
    "else:\n",
    "    print(f\"✅ Region validation passed: {REGION}\")\n",
    "    \n",
    "print(f\"📦 Container URI: {container_uri}\")\n",
    "print(f\"🖥️ Instance Type: {instance_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21209ad",
   "metadata": {},
   "source": [
    "## Configure Model Serving Properties\n",
    "\n",
    "Now we'll create a `serving.properties` file that configures how the model will be served. This configuration is crucial for optimal performance and memory utilization.\n",
    "\n",
    "Key configurations explained:\n",
    "- **Engine**: Python backend for model serving\n",
    "- **Model Settings**:\n",
    "  - Using DeepSeek-V3-0324 model from Hugging Face\n",
    "  - Maximum sequence length of 32768 tokens\n",
    "- **Performance Optimizations**:\n",
    "  - Tensor parallelism across all available GPUs\n",
    "  - 87% GPU memory utilization target\n",
    "  - vLLM rolling batch with max size of 16 for efficient batching\n",
    "  \n",
    "### Understanding KV Cache and Context Window\n",
    "\n",
    "The `max_model_len` parameter controls the maximum sequence length the model can handle, which directly affects the size of the KV (Key-Value) cache in GPU memory. For P5 instances, you can progressively increase this value to find the optimal balance:\n",
    "\n",
    "1. Start with a conservative value (current: 32768)\n",
    "2. Monitor GPU memory usage\n",
    "3. Incrementally increase if memory permits\n",
    "4. Target the model's full context window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053bf61-bac7-4452-a84b-33687a1ccb14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.trust_remote_code=True\n",
    "option.tensor_parallel_degree=max\n",
    "option.gpu_memory_utilization=.87\n",
    "option.max_model_len=32768\n",
    "option.model_id=deepseek-ai/DeepSeek-V3-0324\n",
    "option.max_rolling_batch_size=16\n",
    "option.rolling_batch=vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b51f0",
   "metadata": {},
   "source": [
    "## Configure vLLM Requirements\n",
    "\n",
    "(Optional) The `requirements.txt` file specifies the vLLM version needed for model inference. vLLM inference framework provides optimized serving capabilities.\n",
    "\n",
    "### Version Considerations\n",
    "- **vLLM 0.7.1**: Currently specified stable version\n",
    "\n",
    "### Performance Impact\n",
    "Different vLLM versions can affect:\n",
    "- Inference speed\n",
    "- Memory utilization\n",
    "- Batch processing efficiency\n",
    "- Compatibility with other libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31065875-6ef0-47ab-ab8b-a7e5d24ea69f",
   "metadata": {},
   "source": [
    "## Package Model Artifacts\n",
    "\n",
    "Now we'll create a deployment package containing our configuration files. This involves:\n",
    "1. Creating a model directory\n",
    "2. Moving configuration files into it\n",
    "3. Creating a compressed tarball for SageMaker deployment\n",
    "\n",
    "> **Note**: SageMaker expects model artifacts in a compressed format (`.tar.gz`) with a specific structure for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231559c-2702-4d9d-a1bd-4387e70cddad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5406858-9c1f-4844-8072-3bb3d0909c1e",
   "metadata": {},
   "source": [
    "## Upload Model Artifacts to S3\n",
    "\n",
    "Before deploying to SageMaker, we need to upload our model artifacts to Amazon S3. This process:\n",
    "1. Determines the S3 bucket location (using SageMaker default bucket)\n",
    "2. Defines a prefix path for organization\n",
    "3. Uploads the packaged model artifacts\n",
    "\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c838e-4ade-4724-a270-0496728b77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfeb11-df3b-4b21-bb96-21c2390fad60",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Model Data** (`model_data`): Our configuration files in S3\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n",
    "\n",
    "### Required Permissions\n",
    "The IAM role needs:\n",
    "- S3 read access for model artifacts\n",
    "- CloudWatch permissions for logging\n",
    "- ECR permissions to pull the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecd197-91b2-4f36-bcec-2afb8c7c370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(image_uri=container_uri,\n",
    "              model_data=code_artifact,\n",
    "              role=role,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba06783-5e04-4a7e-9dc9-0346535e85bc",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (P5 instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.p5en.48xlarge` for high-performance inference\n",
    "- **Health Check Timeout**: 2800 seconds (≈47 minutes)\n",
    "  - Extended timeout needed for large model loading\n",
    "  - Includes time for container setup and model initialization\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take 30-45 minutes for large models\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2741a6-b7e1-4b75-8cb2-95555fd27968",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = sagemaker.utils.name_from_base(\"DeepSeek-V3\")\n",
    "\n",
    "print(endpoint_name)\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout = 2800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c18c1-abe4-4ad8-a298-4f047c0a1ce6",
   "metadata": {},
   "source": [
    "## Implement Streaming Chat Interface\n",
    "\n",
    "This section implements a streaming chat interface for real-time interaction with the DeepSeek-R1 model. The implementation includes:\n",
    "\n",
    "1. **Streaming Infrastructure**:\n",
    "   - Custom `LineIterator` for efficient stream processing\n",
    "   - Real-time token processing\n",
    "   - Performance monitoring (tokens per second)\n",
    "\n",
    "2. **Chat Formatting**:\n",
    "   - DeepSeek-R1 specific template\n",
    "   - Chat history management\n",
    "   - Special token handling\n",
    "\n",
    "3. **Performance Features**:\n",
    "   - Live response streaming\n",
    "   - Token speed monitoring\n",
    "   - Memory-efficient processing\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### Chat Template Format\n",
    "<｜begin▁of▁sentence｜> <｜User｜>{user_message}<｜Assistant｜>{assistant_response}\n",
    "\n",
    "\n",
    "#### Streaming Parameters\n",
    "- `max_new_tokens`: 8192 (default)\n",
    "- `do_sample`: True for sampling-based generation\n",
    "- Real-time TPS (Tokens Per Second) monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e48c0-49f2-497c-846e-5e07d9997076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# SageMaker Runtime client\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "# Replace with your SageMaker endpoint name if needed\n",
    "#endpoint_name = \"DeepSeek-V3-2025-03-26-05-10-49-961\"\n",
    "\n",
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\" + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "\n",
    "def format_deepseek_chat_template(user_input, chat_history=None):\n",
    "    \"\"\"\n",
    "    Format input according to DeepSeek R1 chat template\n",
    "    \n",
    "    Args:\n",
    "    - user_input (str): Current user message\n",
    "    - chat_history (list, optional): Previous conversation turns\n",
    "    \n",
    "    Returns:\n",
    "    - str: Formatted chat input with special tokens\n",
    "    \"\"\"\n",
    "    # Start with the beginning of sentence token\n",
    "    formatted_input = \"<｜begin▁of▁sentence｜>\"\n",
    "    \n",
    "    # Add chat history if provided\n",
    "    if chat_history:\n",
    "        for turn in chat_history:\n",
    "            formatted_input += f\"<｜User｜>{turn['user']}<｜Assistant｜>{turn['assistant']}\"\n",
    "    \n",
    "    # Add current user input\n",
    "    formatted_input += f\"<｜User｜>{user_input}<｜Assistant｜>\"\n",
    "    \n",
    "    return formatted_input\n",
    "\n",
    "def stream_chat_response(endpoint_name, inputs, max_new_tokens=8192):\n",
    "    # Format the input using the DeepSeek chat template\n",
    "    formatted_inputs = format_deepseek_chat_template(inputs)\n",
    "    \n",
    "    body = {\n",
    "        \"inputs\": formatted_inputs,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"do_sample\": True,\n",
    "        },\n",
    "        \"stream\": True,\n",
    "    }\n",
    "\n",
    "    resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(body),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "\n",
    "    event_stream = resp[\"Body\"]\n",
    "    start_json = b\"{\"\n",
    "    full_response = \"\"\n",
    "    start_time = time.time()\n",
    "    token_count = 0\n",
    "\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b\"\" and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode(\"utf-8\"))\n",
    "            token_text = data[\"token\"][\"text\"]\n",
    "            full_response += token_text\n",
    "            token_count += 1\n",
    "\n",
    "            # Calculate tokens per second\n",
    "            elapsed_time = time.time() - start_time\n",
    "            tps = token_count / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "            # Clear the output and reprint everything\n",
    "            clear_output(wait=True)\n",
    "            print(\"Bot:\", full_response)\n",
    "            print(f\"\\nTokens per Second: {tps:.2f}\", end=\"\")\n",
    "\n",
    "    print(\"\\n\") # Add a newline after response is complete\n",
    "    return full_response\n",
    "\n",
    "def chat(endpoint_name):\n",
    "    print(\"Welcome to the SageMaker Streaming Chat! Type 'exit' to quit.\")\n",
    "    chat_history = []\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        bot_response = stream_chat_response(endpoint_name, user_input)\n",
    "        \n",
    "        # Update chat history\n",
    "        chat_history.append({\n",
    "            'user': user_input,\n",
    "            'assistant': bot_response\n",
    "        })\n",
    "\n",
    "\n",
    "# Start the chat\n",
    "chat(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46144d3-7837-4b20-ab63-c77ae3c6316d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
