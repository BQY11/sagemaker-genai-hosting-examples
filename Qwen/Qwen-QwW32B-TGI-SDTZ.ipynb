{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7620d7-05d4-49be-9f4c-9dab1e25f17d",
   "metadata": {},
   "source": [
    "# ðŸš€ Deploy Qwen QwQ 32B Reasoning Model on Amazon SageMaker AI with Auto Scale Down To Zero\n",
    "\n",
    "## Introduction: [Qwen QwQ 32B](https://huggingface.co/Qwen/QwQ-32B)\n",
    "\n",
    "[QwQ](https://huggingface.co/Qwen/QwQ-32B) is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.\n",
    "\n",
    "- **Mathematical Reasoning**: Achieves an impressive 90.6% on MATH-500, outperforming both Claude 3.5 (78.3%) and matching OpenAI's o1-mini (90.0%)\n",
    "- **Advanced Mathematics**: Scores 50.0% on AIME (American Invitational Mathematics Examination), significantly 'higher than Claude 3.5 (16.0%)\n",
    "- **Scientific Reasoning**: Demonstrates strong performance on GPQA with 65.2%, on par with Claude 3.5 (65.0%)\n",
    "- **Programming**: Achieves 50.0% on LiveCodeBench, showing competitive performance with leading proprietary models\n",
    "\n",
    "> [NOTE]\n",
    "> QwQ-32B is released under the Apache 2.0 license, making it suitable for both research and commercial applications.\n",
    "\n",
    "Let's get started deploying one of the most capable open-source reasoning models available today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495a9c3-93c3-450f-82d7-6d91aa30492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker boto3 --force-reinstall --no-cache-dir --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49095ae2-0f51-49e3-acf4-e554fff52cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import (\n",
    "    HuggingFaceModel, \n",
    "    get_huggingface_llm_image_uri\n",
    ")\n",
    "\n",
    "boto_region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66560cdc-9dda-478a-bac2-88ed2e6feef3",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint \n",
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "We begin by creating the endpoint configuration and set MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use.\n",
    "\n",
    "There are a few parameters we want to setup for our endpoint. We first start by setting the variant name, and instance type we want our endpoint to use. In addition we set the *model_data_download_timeout_in_seconds* and *container_startup_health_check_timeout_in_seconds* to have some guardrails for when we deploy inference components to our endpoint. In addition we will use Managed Instance Scaling which allows SageMaker to scale the number of instances based on the requirements of the scaling of your inference components. We set a *MinInstanceCount* and *MinInstanceCount* variable to size this according to the workload you want to service and also maintain controls around cost. Lastly, we set *RoutingStrategy* for the endpoint to optimally tune how to route requests to instances and inference components for the best performance.\n",
    "\n",
    "The suggested instance types to host the QwQ 32B model can be `ml.g5.12xlarge`, `ml.g6.12xlarge`, `ml.g6e.12xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6322e5d-23ad-4eb1-9d50-c88bd0ee4996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set an unique endpoint config name\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Demo endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 3\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7448b-0e70-4bfa-9d39-31757c19de6f",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "Next, we create our endpoint using the above endpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0182ef-7f2b-4ded-b407-a8411bd391e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "print(f\"Demo endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c3d36-138d-4aa1-b334-ae347bd6f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab9858-7545-4d63-8431-a806acc03391",
   "metadata": {},
   "source": [
    "## Deploy using HuggingFace TGI Container\n",
    "Hugging Face Large Language Model (LLM) Inference Deep Learning Container (DLC) on Amazon SageMaker enables developers to efficiently deploy and serve open-source LLMs at scale. This DLC is powered by Text Generation Inference (TGI), an open-source, purpose-built solution optimized for high-performance text generation tasks. \n",
    "\n",
    "**Key Features of HuggingFace TGI Containers:**\n",
    "\n",
    "* **Tensor Parallelism**: Distributes computation across multiple GPUs, allowing the deployment of large models that exceed the memory capacity of a single GPU.\n",
    "* **Dynamic Batching**: Aggregates multiple incoming requests into a single batch, enhancing throughput and resource utilization.\n",
    "* **Optimized Transformers Code**: Utilizes advanced techniques like flash-attention to improve inference speed and efficiency for popular model architectures like DeepSeek, Llama, Falcon, Mistal, Mixtral and many more.\n",
    "\n",
    "**Benefits for Deploying LLMs with HuggingFace TGI on Amazon SageMaker:**\n",
    "\n",
    "* **Simplified Deployment**: TGI containers provide a low-code interface, allowing users to specify configurations like model parallelization and optimization settings through straightforward configuration files. \n",
    "* **Performance Optimization**: By leveraging optimized inference libraries and techniques, such as tensor parallelism and dynamic batching, these containers enhance inference performance, reducing latency and improving throughput. \n",
    "* **Scalability**: Designed to handle large models, TGI containers enable efficient scaling across multiple GPUs or specialized hardware like AWS Inferentia, ensuring that even the most demanding models can be deployed effectively.\n",
    "\n",
    "For a more exhaustive list, please refer to this [TGI Release Page](https://github.com/aws/deep-learning-containers/releases?q=tgi+AND+gpu&expanded=true)\n",
    "\n",
    "### Create Model Artifact\n",
    "We will be deploying the Qwen 32B model using the TGI container. In order to do so you need to set the image you would like to use with the proper configuartion. You can also create a SageMaker model to be referenced when you create your inference component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f8028-c77e-44c5-92ac-f35ed3e46f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgi_inference_image_uri = get_huggingface_llm_image_uri(\n",
    "     \"huggingface\", \n",
    "     version=\"2.3.1\"\n",
    ")\n",
    "print(f\"Using TGI Image: {tgi_inference_image_uri}\")\n",
    "qwen_qwq_32b = \"Qwen/QwQ-32B\"\n",
    "qwen_tgi_model = {\n",
    "    \"Image\": tgi_inference_image_uri,\n",
    "    \"Environment\": {\n",
    "        \"HF_MODEL_ID\": qwen_qwq_32b,\n",
    "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
    "        \"MESSAGES_API_ENABLED\": \"true\",\n",
    "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "        \"SM_NUM_GPUS\": \"4\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"8192\",\n",
    "        \"MAX_INPUT_TOKENS\": \"4096\",\n",
    "        'HF_HUB_ENABLE_HF_TRANSFER': \"1\",\n",
    "        \"PORT\": \"8080\"\n",
    "    },\n",
    "}\n",
    "model_name_tgi = f\"qwen-qwq-32b-tgi-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "# create SageMaker Model\n",
    "sagemaker_client.create_model(\n",
    "    ModelName=model_name_tgi,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[qwen_tgi_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e6094-9ea6-4481-be2e-9ee2d00b92bd",
   "metadata": {},
   "source": [
    "We can now create the Inference Components which will deployed on the endpoint that you specify. Please note here that you can provide a SageMaker model or a container to specification. If you provide a container, you will need to provide an image and artifactURL as parameters. In this example we set it to the model name we prepared in the cells above. You can also set the `ComputeResourceRequirements` to supply SageMaker what should be reserved for each copy of the inference component. You can also set the copy count of the number of Inference Components you would like to deploy. These can be managed and scaled as the capabilities become available. \n",
    "\n",
    "Note that in this example we set the `NumberOfAcceleratorDevicesRequired` to a value of `4`. By doing so we reserve 4 accelerators for each copy of this inference component so that we can use tensor parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e7d25-c98e-4be5-ad32-c518601c6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_component_name_qwen = f\"{prefix}-IC-qwen-32b-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name_tgi,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 4,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd376f-d3f5-4f8e-a4b5-ea11ce9fa60e",
   "metadata": {},
   "source": [
    "Wait until the inference component is `InService`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56190add-87e3-4081-ae69-2d603911522e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_qwen\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e285c-b98c-4e23-87dd-739898c32dec",
   "metadata": {},
   "source": [
    "#### Invoke endpoint with boto3\n",
    "Note that you can also invoke the endpoint with boto3. If you have an existing endpoint, you don't need to recreate the `predictor` and can follow below example to invoke the endpoint with an endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817557b-8204-4171-bcaa-54395f8b8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "prompt = {\n",
    "    'messages':[\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "],\n",
    "    'temperature':0,\n",
    "    'max_tokens':512,\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015339db-9d8c-481b-bca9-5f7955991a5b",
   "metadata": {},
   "source": [
    "#### Streaming response from the endpoint\n",
    "Additionally, SGLang allows you to invoke the endpoint and receive streaming response. Below is an example of how to interact with the endpoint with streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f8ce7-fcf1-4276-a1ae-98dc3ef7638b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "# Example class that processes an inference stream:\n",
    "class SmrInferenceStream:\n",
    "    \n",
    "    def __init__(self, sagemaker_runtime, endpoint_name, inference_component_name=None):\n",
    "        self.sagemaker_runtime = sagemaker_runtime\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.inference_component_name = inference_component_name\n",
    "        # A buffered I/O stream to combine the payload parts:\n",
    "        self.buff = io.BytesIO() \n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def stream_inference(self, request_body):\n",
    "        # Gets a streaming inference response \n",
    "        # from the specified model endpoint:\n",
    "        response = self.sagemaker_runtime\\\n",
    "            .invoke_endpoint_with_response_stream(\n",
    "                EndpointName=self.endpoint_name, \n",
    "                InferenceComponentName=self.inference_component_name,\n",
    "                Body=json.dumps(request_body), \n",
    "                ContentType=\"application/json\"\n",
    "        )\n",
    "        # Gets the EventStream object returned by the SDK:\n",
    "        event_stream = response['Body']\n",
    "        for event in event_stream:\n",
    "            # Passes the contents of each payload part\n",
    "            # to be concatenated:\n",
    "            self._write(event['PayloadPart']['Bytes'])\n",
    "            # Iterates over lines to parse whole JSON objects:\n",
    "            for line in self._readlines():\n",
    "                line = line.decode('utf-8')[len('data: '):]\n",
    "                # print(line)\n",
    "                try:\n",
    "                    resp = json.loads(line)\n",
    "                except:\n",
    "                    continue\n",
    "                if len(line)>0 and type(resp) == dict:\n",
    "                    # if len(resp.get('choices')) == 0:\n",
    "                    #     continue\n",
    "                    part = resp.get('choices')[0]['delta']['content']\n",
    "                    \n",
    "                else:\n",
    "                    part = resp\n",
    "                # Returns parts incrementally:\n",
    "                yield part\n",
    "    \n",
    "    # Writes to the buffer to concatenate the contents of the parts:\n",
    "    def _write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "\n",
    "    # The JSON objects in buffer end with '\\n'.\n",
    "    # This method reads lines to yield a series of JSON objects:\n",
    "    def _readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            self.read_pos += len(line)\n",
    "            yield line[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f29fdb-e4be-439c-a6e4-dc05facc6b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"},\n",
    "    ],\n",
    "    'temperature':0,\n",
    "    'max_tokens':512,\n",
    "    'stream': True,\n",
    "    'stream_options': {'include_usage': True}\n",
    "}\n",
    "\n",
    "smr_inference_stream = SmrInferenceStream(\n",
    "    sagemaker_runtime, endpoint_name, inference_component_name_qwen)\n",
    "stream = smr_inference_stream.stream_inference(request_body)\n",
    "for part in stream:\n",
    "    print(part, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a3556-3476-4239-a8e3-198b177fe710",
   "metadata": {},
   "source": [
    "## Automatically Scale To Zero\n",
    "### Scaling policies\n",
    "Once the endpoint is deployed and InService, you can then add the necessary scaling policies:\n",
    "\n",
    "* A [target tracking](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html) policy that can scale in the copy count for our inference component model copies to zero, and from 1 to n. \n",
    "* A [step scaling policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html) policy that will allow the endpoint to scale out from zero.\n",
    "\n",
    "These policies work together to provide cost-effective scaling - the endpoint can scale to zero when idle and automatically scale out as needed to handle incoming requests.\n",
    "\n",
    "### Scaling policy for inference components copies (target tracking)\n",
    "We start with creating our target tracking policies for scaling the CopyCount of our inference component\n",
    "\n",
    "#### Register a new autoscaling target\n",
    "After you create your SageMaker endpoint and inference components, you register a new auto scaling target for Application Auto Scaling. In the following code block, you set **MinCapacity**  to **0**, which is required for your endpoint to scale down to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b8656-32de-4df8-934b-cf3da677a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client = sagemaker_session.boto_session.client(\"application-autoscaling\")\n",
    "cloudwatch_client = sagemaker_session.boto_session.client(\"cloudwatch\")\n",
    "\n",
    "# Autoscaling parameters\n",
    "resource_id = f\"inference-component/{inference_component_name_qwen}\"\n",
    "service_namespace = \"sagemaker\"\n",
    "scalable_dimension = \"sagemaker:inference-component:DesiredCopyCount\"\n",
    "\n",
    "min_copy_count = 0\n",
    "max_copy_count = 3\n",
    "\n",
    "aas_client.register_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    MinCapacity=min_copy_count,\n",
    "    MaxCapacity=max_copy_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829fb2a-f650-4a1d-b51d-34a0c97ff57a",
   "metadata": {},
   "source": [
    "#### Configure Target Tracking Scaling Policy\n",
    "Once you have registered your new scalable target, the next step is to define your target tracking policy. In the code example that follows, we set the TargetValue to 5. This setting instructs the auto-scaling system to increase capacity when the number of concurrent requests per model reaches or exceeds 5. Here we are taking advantage of the more granular auto scaling metric `PredefinedMetricType`: `SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution` to more accurately monitor and react to changes in inference traffic. Take a look this [blog](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-launches-faster-auto-scaling-for-generative-ai-models/) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787fd40-e612-45f0-800e-275fcabe4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client.describe_scalable_targets(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceIds=[resource_id],\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "\n",
    "# The policy name for the target traking policy\n",
    "target_tracking_policy_name = f\"Target-tracking-policy-qwen-qwq-scale-to-zero-aas-{inference_component_name_qwen}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=target_tracking_policy_name,\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\",\n",
    "        },\n",
    "        # Low TPS + load TPS\n",
    "        \"TargetValue\": 5,  # you need to adjust this value based on your use case\n",
    "        \"ScaleInCooldown\": 300,  # default\n",
    "        \"ScaleOutCooldown\": 300,  # default\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbd21c-b7db-46fd-be9e-613c7c934415",
   "metadata": {},
   "source": [
    "Application Auto Scaling creates two CloudWatch alarms per scaling target. The first triggers scale-out actions after 30 seconds (using 3 sub-minute data point), while the second triggers scale-in after 15 minutes (using 90 sub-minute data points). The time to trigger the scaling action is usually 1â€“2 minutes longer than those minutes because it takes time for the endpoint to publish metrics to CloudWatch, and it also takes time for AutoScaling to react. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c70d755-a295-4178-b6a1-a2a7cc74bef2",
   "metadata": {},
   "source": [
    "### Scale out from zero policy (step scaling policy )\n",
    "To enable your endpoint to scale out from zero instances, do the following:\n",
    "\n",
    "#### Configure Step Scaling Policy\n",
    "Create a step scaling policy that defines when and how to scale out from zero. This policy will add 1 model copy when triggered, enabling SageMaker to provision the instances required to handle incoming requests after being idle.  The following shows you how to define a step scaling policy. Here we have configured to scale out from 0 to 1 model copy (\"ScalingAdjustment\": 1), depending on your use case you can adjust ScalingAdjustment as required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927f06f-5c00-4f84-b119-c22ebbabb492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy name for the step scaling policy\n",
    "step_scaling_policy_name = f\"Step-scaling-policy-qwen-qwq-scale-to-zero-aas-{inference_component_name_qwen}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=step_scaling_policy_name,\n",
    "    PolicyType=\"StepScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    StepScalingPolicyConfiguration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"MetricAggregationType\": \"Maximum\",\n",
    "        \"Cooldown\": 60,\n",
    "        \"StepAdjustments\":\n",
    "          [\n",
    "             {\n",
    "               \"MetricIntervalLowerBound\": 0,\n",
    "               \"ScalingAdjustment\": 1\n",
    "             }\n",
    "          ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbaa800-d78f-4d43-a4c3-f347dc017eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = aas_client.describe_scaling_policies(\n",
    "    PolicyNames=[step_scaling_policy_name],\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "step_scaling_policy_arn = resp['ScalingPolicies'][0]['PolicyARN']\n",
    "print(f\"step_scaling_policy_arn: {step_scaling_policy_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d65e9b-73a2-4c8b-90da-e2bb70cdf5e7",
   "metadata": {},
   "source": [
    "#### Create the CloudWatch alarm that will trigger our policy\n",
    "\n",
    "Finally, create a CloudWatch alarm with the metric **NoCapacityInvocationFailures**. When triggered, the alarm initiates the previously defined scaling policy. For more information about the NoCapacityInvocationFailures metric, see [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-inference-component).\n",
    "\n",
    "We have also set the following:\n",
    "- EvaluationPeriods to 1 \n",
    "- DatapointsToAlarm to 1 \n",
    "- ComparisonOperator to  GreaterThanOrEqualToThreshold\n",
    "\n",
    "This results in 1 min waiting for the step scaling policy to trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8bb8e-1077-4c1d-ae48-3b6bc97bb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alarm name for the step scaling alarm\n",
    "step_scaling_alarm_name = f\"step-scaling-alarm-qwen-qwq-scale-to-zero-aas-{inference_component_name_qwen}\"\n",
    "\n",
    "cloudwatch_client.put_metric_alarm(\n",
    "    AlarmName=step_scaling_alarm_name,\n",
    "    AlarmActions=[step_scaling_policy_arn],  # Replace with your actual ARN\n",
    "    MetricName='NoCapacityInvocationFailures',\n",
    "    Namespace='AWS/SageMaker',\n",
    "    Statistic='Maximum',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'InferenceComponentName',\n",
    "            'Value': inference_component_name_qwen  # Replace with actual InferenceComponentName\n",
    "        }\n",
    "    ],\n",
    "    Period=30, # Set a lower period \n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=1,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='missing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c6c8a-eb20-4045-9d2c-41eb38eeeab4",
   "metadata": {},
   "source": [
    "From cloudwatch console, you can check the alarms created.\n",
    "![cloudwatch alarms](./img/cloudwatch-alarms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd40ab-024d-49ba-a323-53cf96730981",
   "metadata": {},
   "source": [
    "### Testing the behaviour\n",
    "Notice the `MinInstanceCount: 0` setting in the Endpoint configuration, which allows the endpoint to scale down to zero instances. With the scaling policy, CloudWatch alarm, and minimum instances set to zero, your SageMaker Inference Endpoint will now be able to automatically scale down to zero instances when not in use, helping you optimize your costs and resource utilization.\n",
    "\n",
    "### IC copy count scales in to zero\n",
    "We'll pause for a few minutes without making any invocations to our model. Based on our target tracking policy, when our SageMaker endpoint doesn't receive requests for about 10 to 15 minutes, it will automatically scale down to zero the number of model copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ba456-d5a6-463e-93b0-b63640a7cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(600)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name_qwem)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd903f-b5fb-4512-8d35-0af23186b49a",
   "metadata": {},
   "source": [
    "### Endpoint's instances scale in to zero\n",
    "\n",
    "After 10 additional minutes of inactivity, SageMaker automatically terminates all underlying instances of the endpoint, eliminating all associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75442160-2dc0-451e-8fdb-10679a01beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 10mins instances will scale down to 0\n",
    "time.sleep(600)\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fdba1e-991e-4580-9f51-6717a2c722ca",
   "metadata": {},
   "source": [
    "### Invoke the endpoint with a sample prompt\n",
    "\n",
    "If we try to invoke our endpoint while instances are scaled down to zero, we get a validation error: `An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc8a68-973a-42dc-baf1-d816d5c54e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67f1b2-a0be-4de4-a04f-6678cc290b0c",
   "metadata": {},
   "source": [
    "### Scale out from zero kicks in\n",
    "However, after 1 minutes our step scaling policy should kick in. SageMaker will then start provisioning a new instance and deploy our inference component model copy to handle requests. This demonstrates the endpoint's ability to automatically scale out from zero when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9181823-98f9-4c6e-a396-4bdc8318bc00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6104269-72b6-42a1-83b7-43b70ce54e34",
   "metadata": {},
   "source": [
    "#### verify that our endpoint has succesfully scaled out from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c30e8-d4ab-4c53-980b-f7a4dcd67966",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacb334-1383-4bab-a118-683d3328b929",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "- Deregister scalable target\n",
    "- Delete cloudwatch alarms\n",
    "- Delete scaling policies\n",
    "  \n",
    "Make sure to delete the endpoint and other artifacts that were created to avoid unnecessary cost. You can also go to SageMaker AI console to delete all the resources created in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696e16ca-ebb6-4d2a-84d0-69743e74793f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Deregister the scalable target for AAS\n",
    "    aas_client.deregister_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=scalable_dimension,\n",
    "    )\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] deregistered. âœ…\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] not found!.\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "\n",
    "# Delete CloudWatch alarms created for Step scaling policy\n",
    "try:\n",
    "    cloudwatch_client.delete_alarms(AlarmNames=[step_scaling_alarm_name])\n",
    "    print(f\"Deleted CloudWatch step scaling scale-out alarm [b]{step_scaling_alarm_name} âœ…\")\n",
    "except cloudwatch_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"CloudWatch scale-out alarm [b]{step_scaling_alarm_name}[/b] not found.\")\n",
    "\n",
    "\n",
    "# Delete step scaling policies\n",
    "print(\"---\" * 10)\n",
    "\n",
    "try:\n",
    "    aas_client.delete_scaling_policy(\n",
    "        PolicyName=step_scaling_policy_name,\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    )\n",
    "    print(f\"Deleted scaling policy [i green]{step_scaling_policy_name} âœ…\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scaling policy [i]{step_scaling_policy_name}[/i] not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71881f-1458-4557-9a13-9d764377e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f12ce-8c71-47c6-a741-2b1286d68fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
