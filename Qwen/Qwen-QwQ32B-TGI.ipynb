{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8fd722b-7c0b-4813-ae7f-963215645fb9",
   "metadata": {},
   "source": [
    "# ðŸš€ Deploy Qwen QwQ 32B Large Language Model from HuggingFace Hub on Amazon SageMaker AI with Inference Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a414500-d1e8-40d8-ac2c-861f385014fc",
   "metadata": {},
   "source": [
    "## Introduction: [Qwen QwQ 32B](https://huggingface.co/Qwen/QwQ-32B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90891c3-7f38-4ff6-9e0e-765d74da014e",
   "metadata": {},
   "source": [
    "[QwQ](https://huggingface.co/Qwen/QwQ-32B) is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e8cc62-53a9-4542-a566-56f47ada5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "aiobotocore 2.19.0 requires botocore<1.36.4,>=1.36.0, but you have botocore 1.37.7 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.3 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4f9750-ff90-4985-a978-52b22dbab4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import (\n",
    "    HuggingFaceModel, \n",
    "    get_huggingface_llm_image_uri\n",
    ")\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe509d4a-91e5-4826-8cd5-8be23ffe875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix: DEMO-1741260388-e8ce\n"
     ]
    }
   ],
   "source": [
    "boto_region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3541ae2-d761-4601-8c8f-d79126241e3b",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint Configuration\n",
    "There are a few parameters we want to setup for our endpoint. We first start by setting the variant name, and instance type we want our endpoint to use. In addition we set the *model_data_download_timeout_in_seconds* and *container_startup_health_check_timeout_in_seconds* to have some guardrails for when we deploy inference components to our endpoint. In addition we will use Managed Instance Scaling which allows SageMaker to scale the number of instances based on the requirements of the scaling of your inference components. We set a *MinInstanceCount* and *MinInstanceCount* variable to size this according to the workload you want to service and also maintain controls around cost. Lastly, we set *RoutingStrategy* for the endpoint to optimally tune how to route requests to instances and inference components for the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2663f7fc-ed34-4254-9d2a-757322c0af65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo endpoint config name: DEMO-1741260388-e8ce-endpoint-config\n",
      "Initial instance count: 1\n",
      "Max instance count: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:537124949553:endpoint-config/DEMO-1741260388-e8ce-endpoint-config',\n",
       " 'ResponseMetadata': {'RequestId': '32f6a1e7-a183-4fcc-a756-bc47a146a42e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '32f6a1e7-a183-4fcc-a756-bc47a146a42e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '117',\n",
       "   'date': 'Thu, 06 Mar 2025 11:26:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set an unique endpoint config name\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Demo endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "initial_instance_count = 1\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc29c4f-6b4a-4b69-ac9b-e0ed2301fe94",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint\n",
    "We can now use the EndpointConfiguration created in the last step to create and endpoint with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5f03b96-a8e4-4979-8706-54f595504ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo endpoint name: DEMO-1741260388-e8ce-endpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-west-2:537124949553:endpoint/DEMO-1741260388-e8ce-endpoint',\n",
       " 'ResponseMetadata': {'RequestId': '614c9997-8f9b-48f3-bc48-913565d7b3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '614c9997-8f9b-48f3-bc48-913565d7b3e8',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '97',\n",
       "   'date': 'Thu, 06 Mar 2025 11:26:30 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "print(f\"Demo endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a91bb389-3461-41ea-81a4-e3cdc4cf278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'DEMO-1741260388-e8ce-endpoint',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-west-2:537124949553:endpoint/DEMO-1741260388-e8ce-endpoint',\n",
       " 'EndpointConfigName': 'DEMO-1741260388-e8ce-endpoint-config',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1,\n",
       "   'RoutingConfig': {'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'}}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2025, 3, 6, 11, 26, 30, 806000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 3, 6, 11, 28, 49, 81000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '4aedc044-a35f-4a10-aa5c-9a76818d63f9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4aedc044-a35f-4a10-aa5c-9a76818d63f9',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '468',\n",
       "   'date': 'Thu, 06 Mar 2025 11:29:02 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de0812-e139-4176-92ce-1dc8c33f40c5",
   "metadata": {},
   "source": [
    "## Deploy using HuggingFace TGI Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89707ad-bfbd-47eb-98f1-5784acfc2226",
   "metadata": {},
   "source": [
    "Hugging Face Large Language Model (LLM) Inference Deep Learning Container (DLC) on Amazon SageMaker enables developers to efficiently deploy and serve open-source LLMs at scale. This DLC is powered by Text Generation Inference (TGI), an open-source, purpose-built solution optimized for high-performance text generation tasks. \n",
    "\n",
    "**Key Features of HuggingFace TGI Containers:**\n",
    "\n",
    "* **Tensor Parallelism**: Distributes computation across multiple GPUs, allowing the deployment of large models that exceed the memory capacity of a single GPU.\n",
    "* **Dynamic Batching**: Aggregates multiple incoming requests into a single batch, enhancing throughput and resource utilization.\n",
    "* **Optimized Transformers Code**: Utilizes advanced techniques like flash-attention to improve inference speed and efficiency for popular model architectures like DeepSeek, Llama, Falcon, Mistal, Mixtral and many more.\n",
    "\n",
    "**Benefits for Deploying LLMs with HuggingFace TGI on Amazon SageMaker:**\n",
    "\n",
    "* **Simplified Deployment**: TGI containers provide a low-code interface, allowing users to specify configurations like model parallelization and optimization settings through straightforward configuration files. \n",
    "* **Performance Optimization**: By leveraging optimized inference libraries and techniques, such as tensor parallelism and dynamic batching, these containers enhance inference performance, reducing latency and improving throughput. \n",
    "* **Scalability**: Designed to handle large models, TGI containers enable efficient scaling across multiple GPUs or specialized hardware like AWS Inferentia, ensuring that even the most demanding models can be deployed effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be607155-f91b-4fb2-ace7-ffd91a5d5ca6",
   "metadata": {},
   "source": [
    "Choose an appropriate model name and endpoint name when hosting your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5f036-3e30-404e-a73d-e700e9f42d01",
   "metadata": {},
   "source": [
    "For a more exhaustive list, please refer to this [TGI Release Page](https://github.com/aws/deep-learning-containers/releases?q=tgi+AND+gpu&expanded=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb3d05f0-a653-4665-a580-60c2047cc04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/06/25 11:29:02] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to only available Python version: py311                   <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#610\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">610</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/06/25 11:29:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to only available Python version: py311                   \u001b]8;id=947683;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=645752;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#610\u001b\\\u001b[2m610\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to only supported image scope: gpu.                       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#534\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">534</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to only supported image scope: gpu.                       \u001b]8;id=138659;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=411024;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#534\u001b\\\u001b[2m534\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TGI Image: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.4.0-tgi2.3.1-gpu-py311-cu124-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "tgi_inference_image_uri = get_huggingface_llm_image_uri(\n",
    "     \"huggingface\", \n",
    "     version=\"2.3.1\"\n",
    ")\n",
    "print(f\"Using TGI Image: {tgi_inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a43e98-a0cd-47c8-8ab9-92dee8bdb087",
   "metadata": {},
   "source": [
    "Create a new [SageMaker HuggingFaceModel](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79091841-f9d2-4572-9846-877cdf7b19b0",
   "metadata": {},
   "source": [
    "## Create Model Artifact\n",
    "We will be deploying the Qwen 32B model using the TGI container. In order to do so you need to set the image you would like to use with the proper configuartion. You can also create a SageMaker model to be referenced when you create your inference component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f1a57a0-787e-4d3c-8cc6-e6c1207db698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelArn': 'arn:aws:sagemaker:us-west-2:537124949553:model/qwen-qwq-32b-tgi-250306-112909',\n",
       " 'ResponseMetadata': {'RequestId': '1e48e9d6-f811-4e73-ab9f-ceea0c9be2e4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1e48e9d6-f811-4e73-ab9f-ceea0c9be2e4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '92',\n",
       "   'date': 'Thu, 06 Mar 2025 11:29:09 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_qwq_32b = \"Qwen/QwQ-32B\"\n",
    "qwen_tgi_model = {\n",
    "    \"Image\": tgi_inference_image_uri,\n",
    "    \"Environment\": {\n",
    "        \"HF_MODEL_ID\": qwen_qwq_32b,\n",
    "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
    "        \"MESSAGES_API_ENABLED\": \"true\",\n",
    "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "        \"SM_NUM_GPUS\": \"4\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"8192\",\n",
    "        \"MAX_INPUT_TOKENS\": \"4096\",\n",
    "        'HF_HUB_ENABLE_HF_TRANSFER': \"1\",\n",
    "        \"PORT\": \"8080\"\n",
    "    },\n",
    "}\n",
    "model_name_tgi = f\"qwen-qwq-32b-tgi-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "# create SageMaker Model\n",
    "sagemaker_client.create_model(\n",
    "    ModelName=model_name_tgi,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[qwen_tgi_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1cc94-bafe-4386-892f-72a73cc5517e",
   "metadata": {},
   "source": [
    "We can now create the Inference Components which will deployed on the endpoint that you specify. Please note here that you can provide a SageMaker model or a container to specification. If you provide a container, you will need to provide an image and artifactURL as parameters. In this example we set it to the model name we prepared in the cells above. You can also set the 'ComputeResourceRequirements' to supply SageMaker what should be reserved for each copy of the inference component. You can also set the copy count of the number of Inference Components you would like to deploy. These can be managed and scaled as the capabilities become available. \n",
    "\n",
    "Note that in this example we set the `NumberOfAcceleratorDevicesRequired` to a value of `4`. By doing so we reserve 4 accelerators for each copy of this inference component so that we can use tensor parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a5f6c59-09ac-40dd-aaac-09ed9417b90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'InferenceComponentArn': 'arn:aws:sagemaker:us-west-2:537124949553:inference-component/DEMO-1741260388-e8ce-IC-qwen-32b-250306-112911',\n",
       " 'ResponseMetadata': {'RequestId': 'c39b1f0c-4b1c-476f-b6a3-6542d8b7a1d3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c39b1f0c-4b1c-476f-b6a3-6542d8b7a1d3',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '135',\n",
       "   'date': 'Thu, 06 Mar 2025 11:29:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_component_name_qwen = f\"{prefix}-IC-qwen-32b-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name_tgi,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 4,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75543eec-30d5-445f-bc66-60c16433557f",
   "metadata": {},
   "source": [
    "Wait until the inference component is InService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27d97077-47f2-4a5d-b18e-cd5961947d30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_qwen\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13c37cb2-2d08-4b6a-9893-cf772c1a996b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEMO-1741260388-e8ce-IC-qwen-32b-250306-112911'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_component_name_qwen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5950723-93cd-42a4-a4e6-c707df01e74c",
   "metadata": {},
   "source": [
    "### Inference with SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cb8bfb3-a863-47e6-8da5-d8f151c4bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"object\":\"chat.completion\",\"id\":\"\",\"created\":1741261295,\"model\":\"Qwen/QwQ-32B\",\"system_fingerprint\":\"2.3.1-native\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Okay, let\\'s see. The user is asking how many R\\'s are in the word \\\\\"STRAWBERRY\\\\\". Hmm, first I need to make sure I spell STRAWBERRY correctly. Let me write it out: S-T-R-A-W-B-E-R-R-Y. Wait, let me check again. S, T, then R. So after the T comes R. Then A, W, B, E, and then another R? Let me count the letters one by one.\\\\n\\\\nBreaking it down: S (1), T (2), R (3), A (4), W (5), B (6), E (7), R (8), R (9), Y (10). Oh, so after the E there are two R\\'s in a row. So the letters R appear at positions 3, 8, and 9? Wait, no. Wait a second, maybe I miscounted. Let me go through it again step by step.\\\\n\\\\nS-T-R-A-W-B-E-R-R-Y. Let\\'s list each letter with its position:\\\\n\\\\n1. S\\\\n2. T\\\\n3. R\\\\n4. A\\\\n5. W\\\\n6. B\\\\n7. E\\\\n8. R\\\\n9. R\\\\n10. Y\\\\n\\\\nSo the R\\'s are at positions 3, 8, and 9. That would be three R\\'s. Wait, but sometimes people might misspell STRAWBERRY. Let me confirm the correct spelling. Is it STRAWBERRY with two R\\'s or three? Let me think. The word is spelled S-T-R-A-W-B-E-R-R-Y. Yes, that\\'s correct. After the E, there are two R\\'s, so total of three R\\'s? Wait, first R is the third letter, then two more after the E. So that\\'s 1 + 2 = 3 R\\'s total. Hmm. But maybe I made a mistake here. Let me check another way. Let\\'s write it out and circle the R\\'s.\\\\n\\\\nS T R A W B E R R Y. So the first R is after T, then after E comes R and another R before Y. So that\\'s three R\\'s. Wait, but maybe I\\'m overcounting. Let me list each letter:\\\\n\\\\nLetters: S, T, R, A, W, B, E, R, R, Y. So R appears at the 3rd, 8th, and 9th\"},\"logprobs\":null,\"finish_reason\":\"length\"}],\"usage\":{\"prompt_tokens\":44,\"completion_tokens\":512,\"total_tokens\":556}}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "]\n",
    "\n",
    "payload = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "\n",
    "response_model = sagemaker_runtime_client.invoke_endpoint(\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "response_qwq_tgi = response_model[\"Body\"].read().decode(\"utf8\")\n",
    "response_qwq_tgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3570bf2d-ddff-47c6-9e19-d54d1662bdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's see. The user is asking how many R's are in the word \"STRAWBERRY\". Hmm, first I need to make sure I spell STRAWBERRY correctly. Let me write it out: S-T-R-A-W-B-E-R-R-Y. Wait, let me check again. S, T, then R. So after the T comes R. Then A, W, B, E, and then another R? Let me count the letters one by one.\n",
      "\n",
      "Breaking it down: S (1), T (2), R (3), A (4), W (5), B (6), E (7), R (8), R (9), Y (10). Oh, so after the E there are two R's in a row. So the letters R appear at positions 3, 8, and 9? Wait, no. Wait a second, maybe I miscounted. Let me go through it again step by step.\n",
      "\n",
      "S-T-R-A-W-B-E-R-R-Y. Let's list each letter with its position:\n",
      "\n",
      "1. S\n",
      "2. T\n",
      "3. R\n",
      "4. A\n",
      "5. W\n",
      "6. B\n",
      "7. E\n",
      "8. R\n",
      "9. R\n",
      "10. Y\n",
      "\n",
      "So the R's are at positions 3, 8, and 9. That would be three R's. Wait, but sometimes people might misspell STRAWBERRY. Let me confirm the correct spelling. Is it STRAWBERRY with two R's or three? Let me think. The word is spelled S-T-R-A-W-B-E-R-R-Y. Yes, that's correct. After the E, there are two R's, so total of three R's? Wait, first R is the third letter, then two more after the E. So that's 1 + 2 = 3 R's total. Hmm. But maybe I made a mistake here. Let me check another way. Let's write it out and circle the R's.\n",
      "\n",
      "S T R A W B E R R Y. So the first R is after T, then after E comes R and another R before Y. So that's three R's. Wait, but maybe I'm overcounting. Let me list each letter:\n",
      "\n",
      "Letters: S, T, R, A, W, B, E, R, R, Y. So R appears at the 3rd, 8th, and 9th\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(response_deepseek_tgi)['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4086b-8973-46f2-bfa8-116abb31d9e2",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8a4721e-27a1-4cfd-b81d-7afab700be15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '38aabee9-aedd-48d1-9e28-aaec1bc7c94e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '38aabee9-aedd-48d1-9e28-aaec1bc7c94e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Tue, 04 Mar 2025 04:33:35 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name_qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35aa602-f27a-4649-a6c3-8366d30f2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71cbd7-1e48-4c45-a123-aa79e08456f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
