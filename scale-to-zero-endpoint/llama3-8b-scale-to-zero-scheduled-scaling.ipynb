{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660292bc-cea8-4bb9-965c-684ef3368679",
   "metadata": {},
   "source": [
    "# Scheduled your SageMaker Inference Endpoints to scale in to Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09668c9e-4794-4aa0-8a12-6fb134a66f8a",
   "metadata": {},
   "source": [
    "In some scenario, you might observe consistent weekly traffic patterns: steady workload Monday through Friday, and no traffic on weekends. You can optimize costs and performance by configuring scheduled actions that align with these patterns:\n",
    "\n",
    "* **Weekend Scale-in (Friday Evening)**: Configure a scheduled action to reduce the number of model copies to zero. This will instruct SageMaker to scale the number instance behind the endpoint to zero, completely eliminating costs during weekend no usage period. \n",
    "* **Workweek Scale-out (Monday Morning)**: Set up a complementary scheduled action to restore the required model capacity, for the Inference component on Monday morning, ensuring your application is ready for weekday operations.\n",
    "\n",
    "This demo notebook demonstrates how you can schedule the scale in of your SageMaker endpoint to zero instances during idle periods, eliminating the previous requirement of maintaining at least one running instance. \n",
    "\n",
    "**Note:** Scale-to-zero is only supported when using inference components. for more information on Inference Components see “[Reduce model deployment costs by 50% on average using the latest features of Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/reduce-model-deployment-costs-by-50-on-average-using-sagemakers-latest-features/)” blog.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c926cc-d7b0-4eb7-8ac7-6d6d84c57c7d",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27b126-cf7f-4e87-bb6b-c9c328212b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 huggingface_hub --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1718a-1f73-45f0-bb79-e3d0d7761f5b",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4a700-336b-4bc6-acfd-374fddf614a0",
   "metadata": {},
   "source": [
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "We begin by creating the endpoint configuration and set MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an unique name for our endpoint config\n",
    "endpoint_config_name = f\"{prefix}-llama3-8b-scale-to-zero-sc-config\"\n",
    "print(f\"Endpoint config name: {endpoint_config_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e65d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure variant name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 2\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18901bc5-7df0-4675-aedf-a624792a00f6",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "Next, we create our endpoint using the endpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-llama3-8b-scale-to-zero-sc-endpoint\"\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac73b99-c239-4b3d-b5c4-ac7fa548db40",
   "metadata": {},
   "source": [
    "#### We wait for our endpoint to go InService. This step can take ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d6b67-7dae-4c02-890e-c3c405b6521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = desc[\"EndpointStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e89259-9e8f-4a45-8dab-74f2c04f4a2a",
   "metadata": {},
   "source": [
    "## import the required libraries and set some variables for the model that we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2998320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "from rich import print\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358758d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session()\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(boto_session) # sagemaker session for interacting with different AWS APIs\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "model_bucket = sagemaker_session.default_bucket()  # bucket to house model artifacts\n",
    "\n",
    "region = sagemaker_session._region_name\n",
    "account_id = sagemaker_session.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bb0a5-e8a1-4e95-9ec1-2532d770b6f1",
   "metadata": {},
   "source": [
    "#### Set the relevant Model Configurations and select the relevant Large Model Inference container image\n",
    "SageMaker offers optimized [large model inference containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) that contains different frameworks for model parallelism enabling inference of LLMs on multiple GPUs. For more information on the available options, please refer to the [DJL Serving - SageMaker Large Model Inference Configurations](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/configurations_large_model_inference_containers.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ddf72c-fc39-42c4-9e06-f4f8389872f6",
   "metadata": {},
   "source": [
    "#### Select the container to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-lmi\", region=region, version=\"0.30.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b4e02-6966-4cb6-b3f4-9bdc51234960",
   "metadata": {},
   "source": [
    "#### Set the model to use. In this example, we will use the llama3_8b \n",
    "1. `HF_MODEL_ID`: The model id of a pre-trained model hosted inside a model repository on huggingface.co (https://huggingface.co/models). The container uses this model id to download the corresponding model repository on huggingface.co.\n",
    "\n",
    "2. `HF_TOKEN`: Your HuggingFace token for accessing gated model\n",
    "\n",
    "As environment variables, we provide the correct HuggingFace model ID. Additionally, we also provide our HuggingFace token since this is a gated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826c4b8-665d-42d9-83cc-351cef77cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv(\"HUGGING_FACE_HUB_TOKEN\") or getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "\n",
    "hf_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llama3model = {\n",
    "    \"Image\": inference_image_uri,\n",
    "    \"Environment\": {\n",
    "        \"HF_MODEL_ID\": hf_model_id,  # model_id from hf.co/models\n",
    "        \"HF_TOKEN\": HF_TOKEN,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe454e-1d62-462b-bcc4-0aa7b0800cc2",
   "metadata": {},
   "source": [
    "## Create an inference component for our llama3_8b model and invoke the model\n",
    "Inference components can reuse a SageMaker model that you may have already created. You also have the option to specify your artifacts and container directly when creating an inference component which we will show below. In this example we will also create a SageMaker model if you want to reference it later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fad11c-57d7-4c27-bcdb-57a8699136ca",
   "metadata": {},
   "source": [
    "### Create Inference Component (IC)\n",
    "We can now create our inference component. Note below that we specify an inference component name. You can use this name to update your inference compent or view metrics and logs on the inference component you create in CloudWatch. You will also want to set your \"ComputeResourceRequirements\". This will tell SageMaker how much of each resource you want to reserver for EACH COPY of your inference component. Finally we set the number of copies that we want to deploy. The number of copies can be managed through autoscaling policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an unique name for our IC\n",
    "inference_component_name = f\"{prefix}-llama3-8b-scale-to-zero-sc\"\n",
    "print(f\"inference component name: {inference_component_name}\")\n",
    "\n",
    "model_name = f\"{prefix}-llama3-8b-scale-to-zero-scheduled\"\n",
    "print(f\"model name: {model_name}\")\n",
    "\n",
    "sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[llama3model],\n",
    ")\n",
    "\n",
    "initial_copy_count = 1\n",
    "max_copy_count_per_instance = 4  # up to 4 llama3-8b\n",
    "\n",
    "variant_name = \"AllTraffic\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "min_memory_required_in_mb = 1024  # max memory util is up to 85%\n",
    "number_of_accelerator_devices_required = 1\n",
    "\n",
    "sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"MinMemoryRequiredInMb\": min_memory_required_in_mb,\n",
    "            \"NumberOfAcceleratorDevicesRequired\": number_of_accelerator_devices_required,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": initial_copy_count,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d673e12-8b4e-4904-90f8-caf2461f5fc5",
   "metadata": {},
   "source": [
    "#### We wait for our IC to go InService This step can take ~8 mins or longer\n",
    "Let's wait for the endpoint to be ready before proceeding with inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791a7de-4f82-4288-b842-54742058c096",
   "metadata": {},
   "source": [
    "### Invoke the endpoint with a sample prompt\n",
    "Now we can invoke our endpoint with sample text to test its functionality and see the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b1ea2-78d9-4757-904b-037681cc4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    component_name=inference_component_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": hf_model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac2748-56a6-456d-afe4-f7af03149efc",
   "metadata": {},
   "source": [
    "## Schedules using (UpdateInferenceComponentRuntimeConfig API)\n",
    "\n",
    "You can scale your endpoint to zero in two ways. The first method is to set the number of model copies to zero in your Inference component using [UpdateInferenceComponentRuntimeConfigAPI](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateInferenceComponentRuntimeConfig.html). This approach maintains your endpoint configuration while eliminating compute costs during periods of inactivity. \n",
    "```\n",
    "sagemaker_client.update_inference_component_runtime_config(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    DesiredRuntimeConfig={\n",
    "        'CopyCount': 0\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c210d-84f5-4c66-a7b5-52d66a1f70cd",
   "metadata": {},
   "source": [
    "### Create a schedule to shutdown the endpoint on friday and brings it back on Monday\n",
    "\n",
    "[Amazon EventBridge Scheduler](https://docs.aws.amazon.com/eventbridge/latest/userguide/using-eventbridge-scheduler.html) can automate SageMaker API calls using cron/rate expressions for recurring schedules or one-time invocations. To function, EventBridge Scheduler requires an execution role with appropriate permissions to invoke the target API operations on your behalf, please refer to the [documentation](https://docs.aws.amazon.com/scheduler/latest/UserGuide/setting-up.html#setting-up-execution-role) on how to create this role. The specific permissions needed depend on the target API being called.\n",
    "\n",
    "The code below creates two scheduled actions for the Inference component during 2024-2025. The first schedule scales in the CopyCount to zero every Friday at 18:00 UTC+1, while the second schedule restores model capacity every Monday at 07:00 UTC+1. The schedule swill start on November 29, 2024, end on December 31, 2025, and will be deleted after completion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2eef9-e0ff-4c89-8bd8-3845ac25174c",
   "metadata": {},
   "source": [
    "#### Weekend Scale-in (Friday Evening)\n",
    "We start with creating a schedule to scale in our endpoint to 0 every friday at 18:00 UTC+1, starting on November 29, 2024 and ending on December 31, 2025. We need to specify the target API to call [UpdateInferenceComponentRuntimeConfigAPI](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateInferenceComponentRuntimeConfig.html) in this case , and the correct parameter for that API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a23b2-3ac2-45d6-b98e-351a706c1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "scheduler = boto3.client('scheduler')\n",
    "\n",
    "flex_window = {\n",
    "    \"Mode\": \"OFF\"\n",
    "}\n",
    "\n",
    "# We specify the SageMaker target API for the scale in schedule\n",
    "scale_in_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:updateInferenceComponentRuntimeConfig\",\n",
    "    \"Input\": json.dumps({ \"DesiredRuntimeConfig\": {\"CopyCount\": 0}, \"InferenceComponentName\": inference_component_name })\n",
    "}\n",
    "\n",
    "# Scale in our endpoint to 0 every friday at 18:00 UTC+1, starting on November 29, 2024\n",
    "update_IC_scale_in_schedule = \"scale-to-zero-schedule\"\n",
    "scheduler.create_schedule(\n",
    "    Name=update_IC_scale_in_schedule,\n",
    "    ScheduleExpression=\"cron(00 18 ? * 6 2024-2025)\",\n",
    "    ScheduleExpressionTimezone=\"UTC+1\", # Set the correct timezone for your application\n",
    "    Target=scale_in_target,\n",
    "    FlexibleTimeWindow=flex_window,\n",
    "    ActionAfterCompletion=\"DELETE\",\n",
    "    StartDate=\"2024-11-29T00:00:00\",\n",
    "    EndDate=\"2025-12-31T23:59:59\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb578b58-7d33-4811-922e-1036e633119f",
   "metadata": {},
   "source": [
    "#### Workweek Scale-out (Monday Morning):\n",
    "Set up a complementary scheduled action to restore the required model capacity, for the Inference component on Monday morning 07:00 UTC+1, ensuring your application is ready for weekday operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the SageMaker target API for the scale out schedule\n",
    "scale_out_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:updateInferenceComponentRuntimeConfig\",\n",
    "    \"Input\": json.dumps({ \"DesiredRuntimeConfig\": {\"CopyCount\": 2}, \"InferenceComponentName\": inference_component_name })\n",
    "}\n",
    "\n",
    "# Scale out our endpoint every Monday at 07:00 UTC+1\n",
    "update_IC_scale_out_schedule = \"scale-out-schedule\"\n",
    "scheduler.create_schedule(\n",
    "    Name=update_IC_scale_out_schedule,\n",
    "    ScheduleExpression=\"cron(00 07 ? * 2 2024-2025)\",\n",
    "    ScheduleExpressionTimezone=\"UTC+1\", # Set the correct timezone for your application\n",
    "    Target=scale_out_target,\n",
    "    FlexibleTimeWindow=flex_window,\n",
    "    ActionAfterCompletion=\"DELETE\",\n",
    "    StartDate=\"2024-11-29T00:00:00\",\n",
    "    EndDate=\"2025-12-31T23:59:59\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338ce87-0777-4026-baf4-20803fb5cfec",
   "metadata": {},
   "source": [
    "## Schedules using (DeleteInferenceComponen API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09b8ab-cdf1-4309-814a-f8f1d39bed6e",
   "metadata": {},
   "source": [
    "The second method is to delete the Inference components by calling the [DeleteInferenceComponent API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DeleteInferenceComponent.html). This alternative approach achieves the same cost-saving benefit while completely removing the components from your configuration. The following code creates a scheduled action that automatically delete the IC every Friday at 18:00 UTC during 2024-2025. it also create a complementary scheduled action that recreate the IC every Monday at 07:00 UTC+1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6829b-4cf6-4a78-a720-b585c8e702a3",
   "metadata": {},
   "source": [
    "#### Weekend Scale-in (Friday Evening)\n",
    "The following code creates a scheduled action that automatically delete the IC every Friday at 18:00 UTC during 2024-2025. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c5d8f-274e-4629-86a9-87189911b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "scheduler = boto3.client('scheduler')\n",
    "\n",
    "flex_window = {\n",
    "    \"Mode\": \"OFF\"\n",
    "}\n",
    "\n",
    "# We specify the SageMaker target API for the scale in schedule\n",
    "scale_in_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:deleteInferenceComponent\",\n",
    "    \"Input\": json.dumps({\"InferenceComponentName\": inference_component_name })\n",
    "}\n",
    "\n",
    "# Scale in our endpoint by deleting the IC every friday at 18:00 UTC+1\n",
    "delete_IC_scale_in_schedule = \"scale-to-zero-schedule-1\"\n",
    "scheduler.create_schedule(\n",
    "    Name=delete_IC_scale_in_schedule,\n",
    "    ScheduleExpression=\"cron(00 18 ? * 6 2024-2025)\",\n",
    "    ScheduleExpressionTimezone=\"UTC+1\", # Set the correct timezone for your application\n",
    "    Target=scale_in_target,\n",
    "    FlexibleTimeWindow=flex_window,\n",
    "    ActionAfterCompletion=\"DELETE\",\n",
    "    StartDate=\"2024-11-29T00:00:00\",\n",
    "    EndDate=\"2025-12-31T23:59:59\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00800cf7-1f82-40e4-8c24-18f47449721d",
   "metadata": {},
   "source": [
    "#### Workweek Scale-out (Monday Morning):\n",
    "create a complementary scheduled action that recreate the IC every Monday at 07:00 UTC+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec751d-a157-4ef7-8363-7759a6ba6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the SageMaker target API for the scale up schedule\n",
    "input_config = {\n",
    "  \"EndpointName\": endpoint_name,\n",
    "  \"InferenceComponentName\": inference_component_name,\n",
    "  \"RuntimeConfig\": {\n",
    "    \"CopyCount\": 2\n",
    "  },\n",
    "  \"Specification\": {\n",
    "    \"ModelName\": model_name,\n",
    "    \"StartupParameters\": {\n",
    "        \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "    },\n",
    "    \"ComputeResourceRequirements\": {\n",
    "      \"MinMemoryRequiredInMb\": 1024,\n",
    "      \"NumberOfAcceleratorDevicesRequired\": 1\n",
    "    }\n",
    "  },\n",
    "  \"VariantName\": variant_name\n",
    "}\n",
    "\n",
    "scale_out_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:createInferenceComponent\",\n",
    "    \"Input\": json.dumps(input_config)\n",
    "}\n",
    "\n",
    "# Scale out our endpoint by recreating the IC every Monday at 07:00 UTC+1\n",
    "delete_IC_scale_out_schedule = \"scale-out-schedule-1\"\n",
    "scheduler.create_schedule(\n",
    "    Name=delete_IC_scale_out_schedule,\n",
    "    ScheduleExpression=\"cron(00 07 ? * 2 2024-2025)\",\n",
    "    ScheduleExpressionTimezone=\"UTC+1\", # Set the correct timezone for your application\n",
    "    Target=scale_out_target,\n",
    "    FlexibleTimeWindow=flex_window,\n",
    "    ActionAfterCompletion=\"DELETE\",\n",
    "    StartDate=\"2024-11-29T00:00:00\",\n",
    "    EndDate=\"2025-12-31T23:59:59\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6624eb-278e-4035-925e-dc0c69f7d654",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "To schedule the scale to Zero on an endpoint with multiple inference components (IC), all ICs must be either set to 0 or deleted. You can also automate this process by using EventBridge Scheduler to trigger a Lambda function that handles either deletion or zero-setting of all ICs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ffab4-db77-467f-b472-a621c229adee",
   "metadata": {},
   "source": [
    "## Optionally clean up the environment\n",
    "\n",
    "- Delete schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79145a5-03d6-407b-85f6-78796235b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all the schedule created above\n",
    "schedules = [delete_IC_scale_out_schedule, delete_IC_scale_in_schedule, update_IC_scale_out_schedule, update_IC_scale_in_schedule]\n",
    "\n",
    "for schedule in schedules:\n",
    "    try:\n",
    "        scheduler.delete_schedule(Name=schedule)\n",
    "        print(f\"Deleted schedule [b]{schedule} ✅\")\n",
    "    except scheduler.exceptions.ResourceNotFoundException:\n",
    "        print(f\"Schedule [b]{schedule}[/b] not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf611d6-e15b-4a0b-8c05-284ae1b8a07d",
   "metadata": {},
   "source": [
    "Delete inference component\n",
    "\n",
    "Delete endpoint\n",
    "\n",
    "delete endpoint-config\n",
    "\n",
    "Delete model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11e610-c8c1-4fb2-bd4d-5c71e76a1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sagemaker_client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
