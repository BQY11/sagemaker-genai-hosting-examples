{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660292bc-cea8-4bb9-965c-684ef3368679",
   "metadata": {},
   "source": [
    "# Unlock Cost Savings with New Scale-to-Zero Feature in SageMaker Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09668c9e-4794-4aa0-8a12-6fb134a66f8a",
   "metadata": {},
   "source": [
    "\n",
    "This demo notebook demonstrate how you can scale in your SageMaker endpoint to zero instances during idle periods, eliminating the previous requirement of maintaining at least one running instance.\n",
    "\n",
    "The new Scaling to Zero feature expands the possibilities for managing SageMaker Inference endpoints. It allows customers to configure the endpoints so they can scale to zero instances during periods of inactivity, providing an additional tool for resource management. Using this feature customers can closely match their compute resource usage to their actual needs, potentially reducing costs during times of low demand. This enhancement builds upon SageMaker's existing auto-scaling capabilities, offering more granular control over resource allocation. Customers can now configure their scaling policies to include scaling to zero, allowing for more precise management of their AI inference infrastructure. \n",
    "\n",
    "The Scaling to Zero feature presents new opportunities for how businesses can approach their cloud-based machine learning operations. It provides additional options for managing resources across various scenarios, from development and testing environments to production deployments with variable traffic patterns. As with any new feature, customers are encouraged to carefully evaluate how it fits into their overall architecture and operational needs, considering factors such as response times and the specific requirements of their applications.\n",
    "\n",
    "#### Determining When to Scale Down to Zero\n",
    "\n",
    "SageMaker's scale-to-zero capability is ideal for three scenarios:\n",
    "\n",
    "1. **Predictable traffic patterns:** If your inference traffic is predictable and follows a consistent schedule, you can use this scaling functionality to automatically scale in to zero during periods of low or no usage. This eliminates the need to manually delete and recreate inference components/endpoints.\n",
    "\n",
    "2. **Sporadic workloads:** For applications that experience sporadic or variable inference traffic patterns, scaling in to zero instances can provide significant cost savings. However, it's important to note that scaling out from zero instances to serving traffic is not instantaneous. During the scale-out process, any requests sent to the endpoint will fail, and these \"NoCapacityInvocationFailures\" will be captured in CloudWatch.\n",
    "\n",
    "3. **Development and testing:** The scale-to-zero functionality is also beneficial when testing and evaluating new machine learning models. During model development and experimentation, you may create temporary inference endpoints to test different configurations. However, it's easy to forget to delete these endpoints when you're done. Scaling to zero ensures these test endpoints automatically scale in to zero instances when not in use, preventing unwanted charges. This allows you to freely experiment without closely monitoring infrastructure usage or remembering to manually delete endpoints. The automatic scaling to zero provides a cost-effective way to test out ideas and iterate on your machine learning solutions.\n",
    "   \n",
    "**Note:** Scale-to-zero is only supported when using inference components. for more information on Inference Components see “[Reduce model deployment costs by 50% on average using the latest features of Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/reduce-model-deployment-costs-by-50-on-average-using-sagemakers-latest-features/)” blog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c926cc-d7b0-4eb7-8ac7-6d6d84c57c7d",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27b126-cf7f-4e87-bb6b-c9c328212b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir sagemaker==2.235.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.session.Session(boto_session) # sagemaker session for interacting with different AWS APIs\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "model_bucket = sagemaker_session.default_bucket()  # bucket to house model artifacts\n",
    "\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1718a-1f73-45f0-bb79-e3d0d7761f5b",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4a700-336b-4bc6-acfd-374fddf614a0",
   "metadata": {},
   "source": [
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "We begin by creating the endpoint configuration and set MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an unique name for our endpoint config\n",
    "endpoint_config_name = f\"{prefix}-llama3-8b-scale-to-zero-aas-config\"\n",
    "print(f\"Endpoint config name: {endpoint_config_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e65d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure variant name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 3\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18901bc5-7df0-4675-aedf-a624792a00f6",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "Next, we create our endpoint using the above endpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-llama3-8b-scale-to-zero-aas-endpoint\"\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac73b99-c239-4b3d-b5c4-ac7fa548db40",
   "metadata": {},
   "source": [
    "#### We wait for our endpoint to go InService. This step can take ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d6b67-7dae-4c02-890e-c3c405b6521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = desc[\"EndpointStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e89259-9e8f-4a45-8dab-74f2c04f4a2a",
   "metadata": {},
   "source": [
    "## Create Model Builder\n",
    "We use Amazon SageMaker Fast Model Loader. The feature works by streaming model weights directly from Amazon S3 to GPU accelerators, bypassing the typical sequential loading steps that contribute to deployment latency. In internal testing, this approach has shown to load large models up to 15 times faster compared to traditional methods. For more information on this feature, please refer to our example [notebook on GitHub](https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/Llama3.1/Llama3.1-70B-SageMaker-Fast-Model-Loader.ipynb)\n",
    "\n",
    "We'll make use of the ModelBuilder class to prepare and package the model inference components. In this example, we're using the Meta-Llama-3-8B-Instruct SageMaker JumpStart.\n",
    "\n",
    "Key configurations:\n",
    "- Model: Meta-Llama-3-8B-Instruct\n",
    "- Schema Builder: Defines input/output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab7e9e-960d-45ca-9f6d-0ef1bae46485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "import logging\n",
    "\n",
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "response = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the east coast.\"\n",
    "\n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\"\n",
    "\n",
    "model_builder = ModelBuilder(\n",
    "    model=model_id,\n",
    "    role_arn=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    schema_builder=SchemaBuilder(sample_input=prompt, sample_output=response),\n",
    "    #env_vars={\n",
    "    #   \"OPTION_TENSOR_PARALLEL_DEGREE\": \"4\",\n",
    "    #},\n",
    "    log_level=logging.WARN\n",
    ")\n",
    "\n",
    "output_path = f\"s3://{model_bucket}/llama3-8b/sharding\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228eccd3-f329-46c5-8d7f-3d4f3e8b1acc",
   "metadata": {},
   "source": [
    "Note that, if you have already run the model optimization job before and the model shards are available on s3. You can just reuse the shards and skip the optimization step\n",
    "```\n",
    "model_builder = ModelBuilder(\n",
    "             model=\"meta-textgeneration-llama-3-8b-instruct\",\n",
    "             model_metadata={\n",
    "                \"CUSTOM_MODEL_PATH\": output_path,\n",
    "            },\n",
    "           schema_builder=SchemaBuilder(sample_input=\"Test\", sample_output=\"Test\"),\n",
    "            role_arn=role,\n",
    "             instance_type=\"ml.g5.12xlarge\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff30384-88bd-4dd2-80cf-58de2c9f9882",
   "metadata": {},
   "source": [
    "## Optimize Model for Fast Loading\n",
    "Now we'll optimize the model using Fast Model Loader. This process:\n",
    "1. Prepares model shards for deployment\n",
    "2. Enables direct streaming from S3 to GPU\n",
    "3. Pre-configures tensor parallelism settings\n",
    "\n",
    "Note: The optimization process may take a while to complete. The optimized model will be stored in the specified S3 output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f09e54-b6c3-45da-9357-321e96589303",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder.optimize(\n",
    "    instance_type=\"ml.g5.12xlarge\", \n",
    "    accept_eula=True, \n",
    "    output_path=output_path, \n",
    "    sharding_config={\n",
    "            \"OverrideEnvironment\": {\n",
    "                \"OPTION_TENSOR_PARALLEL_DEGREE\": \"4\"\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a44756-e3e0-4a5d-9fea-d761c2bfdce1",
   "metadata": {},
   "source": [
    "## Build and Deploy Model\n",
    "After optimization, we'll build the final model artifacts and deploy them to a SageMaker endpoint. \n",
    "\n",
    "Key configurations:\n",
    "- Instance Type: ml.g5.12xlarge\n",
    "- Memory Request: 4096 MB\n",
    "- Number of Accelerators: 4 (for tensor parallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa240cd5-5224-498c-87a1-30aa08bd656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdbcc9f-05f8-4194-9c00-0177d3a88c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our model is sharded\n",
    "final_model._is_sharded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4939895-f8a1-46d0-b9ac-1562cbbc1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnableNetworkIsolation cannot be set to True since SageMaker Fast Model Loading of model requires network access.\n",
    "if final_model._enable_network_isolation:\n",
    "    final_model._enable_network_isolation = False\n",
    "    \n",
    "final_model._enable_network_isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c91d5c-22ad-4c70-ac4f-766a7184a1ae",
   "metadata": {},
   "source": [
    "#### Select the container image to use\n",
    "Use the latest LMI image to take advantage of caching feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\"\n",
    "print(f\"Image going to be used is ---- > {final_model.image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930b2e7-83f5-44f4-8e86-8b63ff048d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "resources_required = ResourceRequirements(\n",
    "    requests={\n",
    "        \"memory\" : 4096,\n",
    "        \"num_accelerators\": 4,\n",
    "        \"copies\": 1, # specify the number of initial copies (default is 1)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b002697-97ee-4c54-b3a9-565e28946fd0",
   "metadata": {},
   "source": [
    "#### Deploy your model to the endpoint\n",
    "\n",
    "Deploy your model with the model’s existing deploy method. We specify the name of our existing Real-time endpoint SageMaker will host the model on our existing endpoint, so it can starts making predictions on incoming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516287d6-0dc5-4f7c-9da4-5484d4428034",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.deploy(\n",
    "    instance_type=\"ml.g5.12xlarge\", \n",
    "    accept_eula=True, \n",
    "    endpoint_name=endpoint_name,\n",
    "    # endpoint_logging=False, \n",
    "    resources=resources_required,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791a7de-4f82-4288-b842-54742058c096",
   "metadata": {},
   "source": [
    "### Test the endpoint with a sample prompt\n",
    "Now we can invoke our endpoint with sample text to test its functionality and see the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b1ea2-78d9-4757-904b-037681cc4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1b6b3-314f-4e21-9803-08df622da8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_component_name = final_model.inference_component_name\n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d8031-7234-4fab-aa78-5931066fee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    component_name=inference_component_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b512-7474-4497-afba-d7ca62a4de78",
   "metadata": {},
   "source": [
    "## Automatically Scale To Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac2748-56a6-456d-afe4-f7af03149efc",
   "metadata": {},
   "source": [
    "## Scaling policies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bde477-bb35-4ece-a03f-330ca2065883",
   "metadata": {},
   "source": [
    "Once the endpoint is deployed and InService, you can then add the necessary scaling policies:\n",
    "\n",
    "* A [target tracking](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html) policy that can scale in the copy count for our inference component model copies to zero, and from 1 to n. \n",
    "* A [step scaling policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html) policy that will allow the endpoint to scale out from zero.\n",
    "\n",
    "These policies work together to provide cost-effective scaling - the endpoint can scale to zero when idle and automatically scale out as needed to handle incoming requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c210d-84f5-4c66-a7b5-52d66a1f70cd",
   "metadata": {},
   "source": [
    "### Scaling policy for inference components copies (target tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96f9f8-5980-47a8-a458-d991a2b617ad",
   "metadata": {},
   "source": [
    "We start with creating our target tracking policies for scaling the CopyCount of our inference component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2eef9-e0ff-4c89-8bd8-3845ac25174c",
   "metadata": {},
   "source": [
    "#### Register a new autoscaling target\n",
    "After you create your SageMaker endpoint and inference components, you register a new auto scaling target for Application Auto Scaling. In the following code block, you set **MinCapacity**  to **0**, which is required for your endpoint to scale down to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa406cfb-6953-4b35-b2eb-3d5c15b6ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client = sagemaker_session.boto_session.client(\"application-autoscaling\")\n",
    "cloudwatch_client = sagemaker_session.boto_session.client(\"cloudwatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoscaling parameters\n",
    "resource_id = f\"inference-component/{inference_component_name}\"\n",
    "service_namespace = \"sagemaker\"\n",
    "scalable_dimension = \"sagemaker:inference-component:DesiredCopyCount\"\n",
    "\n",
    "min_copy_count = 0\n",
    "max_copy_count = 8\n",
    "\n",
    "aas_client.register_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    MinCapacity=min_copy_count,\n",
    "    MaxCapacity=max_copy_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09b8ab-cdf1-4309-814a-f8f1d39bed6e",
   "metadata": {},
   "source": [
    "#### Configure Target Tracking Scaling Policy\n",
    "Once you have registered your new scalable target, the next step is to define your target tracking policy. In the code example that follows, we set the TargetValue to 5. This setting instructs the auto-scaling system to increase capacity when the number of concurrent requests per model reaches or exceeds 5. Here we are taking advantage of the more granular auto scaling metric `PredefinedMetricType`: `SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution` to more accurately monitor and react to changes in inference traffic. Take a look this [blog](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-launches-faster-auto-scaling-for-generative-ai-models/) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c5d8f-274e-4629-86a9-87189911b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client.describe_scalable_targets(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceIds=[resource_id],\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "\n",
    "# The policy name for the target traking policy\n",
    "target_tracking_policy_name = f\"Target-tracking-policy-llama3-8b-scale-to-zero-aas-{inference_component_name}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=target_tracking_policy_name,\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\",\n",
    "        },\n",
    "        # Low TPS + load TPS\n",
    "        \"TargetValue\": 5,  # you need to adjust this value based on your use case\n",
    "        \"ScaleInCooldown\": 300,  # default\n",
    "        \"ScaleOutCooldown\": 300,  # default\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abb304-e25f-4c5b-9b9e-f1e06ebdb91c",
   "metadata": {},
   "source": [
    "Application Auto Scaling creates two CloudWatch alarms per scaling target. The first triggers scale-out actions after 30 seconds (using 3 sub-minute data point), while the second triggers scale-in after 15 minutes (using 90 sub-minute data points). The time to trigger the scaling action is usually 1–2 minutes longer than those minutes because it takes time for the endpoint to publish metrics to CloudWatch, and it also takes time for AutoScaling to react. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1c281-eb3c-45cb-a66a-615ce42217e1",
   "metadata": {},
   "source": [
    "### Scale out from zero policy (step scaling policy )\n",
    "To enable your endpoint to scale out from zero instances, do the following:\n",
    "\n",
    "#### Configure Step Scaling Policy\n",
    "Create a step scaling policy that defines when and how to scale out from zero. This policy will add 1 model copy when triggered, enabling SageMaker to provision the instances required to handle incoming requests after being idle.  The following shows you how to define a step scaling policy. Here we have configured to scale out from 0 to 1 model copy (\"ScalingAdjustment\": 1), depending on your use case you can adjust ScalingAdjustment as required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7901d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy name for the step scaling policy\n",
    "step_scaling_policy_name = f\"Step-scaling-policy-llama3-8b-scale-to-zero-aas-{inference_component_name}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=step_scaling_policy_name,\n",
    "    PolicyType=\"StepScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    StepScalingPolicyConfiguration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"MetricAggregationType\": \"Maximum\",\n",
    "        \"Cooldown\": 60,\n",
    "        \"StepAdjustments\":\n",
    "          [\n",
    "             {\n",
    "               \"MetricIntervalLowerBound\": 0,\n",
    "               \"ScalingAdjustment\": 1\n",
    "             }\n",
    "          ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20def1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = aas_client.describe_scaling_policies(\n",
    "    PolicyNames=[step_scaling_policy_name],\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "step_scaling_policy_arn = resp['ScalingPolicies'][0]['PolicyARN']\n",
    "print(f\"step_scaling_policy_arn: {step_scaling_policy_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698c23f-b733-4b17-8b3d-e9b510392c9f",
   "metadata": {},
   "source": [
    "#### Create the CloudWatch alarm that will trigger our policy\n",
    "\n",
    "Finally, create a CloudWatch alarm with the metric **NoCapacityInvocationFailures**. When triggered, the alarm initiates the previously defined scaling policy. For more information about the NoCapacityInvocationFailures metric, see [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-inference-component).\n",
    "\n",
    "We have also set the following:\n",
    "- EvaluationPeriods to 1 \n",
    "- DatapointsToAlarm to 1 \n",
    "- ComparisonOperator to  GreaterThanOrEqualToThreshold\n",
    "\n",
    "This results in 1 min waiting for the step scaling policy to trigger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alarm name for the step scaling alarm\n",
    "step_scaling_alarm_name = f\"step-scaling-alarm-llama3-8b-scale-to-zero-aas-{inference_component_name}\"\n",
    "\n",
    "cloudwatch_client.put_metric_alarm(\n",
    "    AlarmName=step_scaling_alarm_name,\n",
    "    AlarmActions=[step_scaling_policy_arn],  # Replace with your actual ARN\n",
    "    MetricName='NoCapacityInvocationFailures',\n",
    "    Namespace='AWS/SageMaker',\n",
    "    Statistic='Maximum',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'InferenceComponentName',\n",
    "            'Value': inference_component_name  # Replace with actual InferenceComponentName\n",
    "        }\n",
    "    ],\n",
    "    Period=30, # Set a lower period \n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=1,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='missing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a65b28-839f-4c85-b167-e09a79459480",
   "metadata": {},
   "source": [
    "## Testing the behaviour\n",
    "Notice the `MinInstanceCount: 0` setting in the Endpoint configuration, which allows the endpoint to scale down to zero instances. With the scaling policy, CloudWatch alarm, and minimum instances set to zero, your SageMaker Inference Endpoint will now be able to automatically scale down to zero instances when not in use, helping you optimize your costs and resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5272565-b459-447d-a123-808c2455d593",
   "metadata": {},
   "source": [
    "### IC copy count scales in to zero\n",
    "We'll pause for a few minutes without making any invocations to our model. Based on our target tracking policy, when our SageMaker endpoint doesn't receive requests for about 10 to 15 minutes, it will automatically scale down to zero the number of model copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(900)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2493b2-010d-4cb7-831e-c39e73bac41c",
   "metadata": {},
   "source": [
    "### Endpoint's instances scale in to zero\n",
    "\n",
    "After 10 additional minutes of inactivity, SageMaker automatically terminates all underlying instances of the endpoint, eliminating all associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e82a84-c0a4-4fcd-be5b-7f51e23d5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 10mins instances will scale down to 0\n",
    "time.sleep(600)\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e163b2b-a4fb-42d9-909c-82264fed850b",
   "metadata": {},
   "source": [
    "### Invoke the endpoint with a sample prompt\n",
    "\n",
    "If we try to invoke our endpoint while instances are scaled down to zero, we get a validation error: `An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56bcfe-c12a-4479-aa80-df1cf03007fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%H:%M:%S\"))\n",
    "# create predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    component_name=inference_component_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa6b86-c683-42d2-bd29-f9da504903aa",
   "metadata": {},
   "source": [
    "### Scale out from zero kicks in\n",
    "However, after 1 minutes our step scaling policy should kick in. SageMaker will then start provisioning a new instance and deploy our inference component model copy to handle requests. This demonstrates the endpoint's ability to automatically scale out from zero when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2bed85-ce0a-43ed-8115-303dd5db7969",
   "metadata": {},
   "source": [
    "#### verify that our endpoint has succesfully scaled out from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d375f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    component_name=inference_component_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ffab4-db77-467f-b472-a621c229adee",
   "metadata": {},
   "source": [
    "## Optionally clean up the environment\n",
    "\n",
    "- Deregister scalable target\n",
    "- Delete cloudwatch alarms\n",
    "- Delete scaling policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff68b5e-a2f0-411e-9a13-04837bf035b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Deregister the scalable target for AAS\n",
    "    aas_client.deregister_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=scalable_dimension,\n",
    "    )\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] deregistered. ✅\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] not found!.\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "\n",
    "# Delete CloudWatch alarms created for Step scaling policy\n",
    "try:\n",
    "    cloudwatch_client.delete_alarms(AlarmNames=[step_scaling_alarm_name])\n",
    "    print(f\"Deleted CloudWatch step scaling scale-out alarm [b]{step_scaling_alarm_name} ✅\")\n",
    "except cloudwatch_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"CloudWatch scale-out alarm [b]{step_scaling_alarm_name}[/b] not found.\")\n",
    "\n",
    "\n",
    "# Delete step scaling policies\n",
    "print(\"---\" * 10)\n",
    "\n",
    "try:\n",
    "    aas_client.delete_scaling_policy(\n",
    "        PolicyName=step_scaling_policy_name,\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    )\n",
    "    print(f\"Deleted scaling policy [i green]{step_scaling_policy_name} ✅\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scaling policy [i]{step_scaling_policy_name}[/i] not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8d9e6-953d-47a8-94fd-4f142be39132",
   "metadata": {},
   "source": [
    "- Delete inference component\n",
    "- Delete endpoint\n",
    "- delete endpoint-config\n",
    "- Delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11e610-c8c1-4fb2-bd4d-5c71e76a1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
