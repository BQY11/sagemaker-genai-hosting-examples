{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660292bc-cea8-4bb9-965c-684ef3368679",
   "metadata": {},
   "source": [
    "# Unlock Cost Savings with New Scale-to-Zero Feature in SageMaker Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09668c9e-4794-4aa0-8a12-6fb134a66f8a",
   "metadata": {},
   "source": [
    "\n",
    "This demo notebook demonstrate how you can scale in your SageMaker endpoint to zero instances during idle periods, eliminating the previous requirement of maintaining at least one running instance.\n",
    "\n",
    "The new Scaling to Zero feature expands the possibilities for managing SageMaker Inference endpoints. It allows customers to configure the endpoints so they can scale to zero instances during periods of inactivity, providing an additional tool for resource management. Using this feature customers can closely match their compute resource usage to their actual needs, potentially reducing costs during times of low demand. This enhancement builds upon SageMaker's existing auto-scaling capabilities, offering more granular control over resource allocation. Customers can now configure their scaling policies to include scaling to zero, allowing for more precise management of their AI inference infrastructure. \n",
    "\n",
    "The Scaling to Zero feature presents new opportunities for how businesses can approach their cloud-based machine learning operations. It provides additional options for managing resources across various scenarios, from development and testing environments to production deployments with variable traffic patterns. As with any new feature, customers are encouraged to carefully evaluate how it fits into their overall architecture and operational needs, considering factors such as response times and the specific requirements of their applications.\n",
    "\n",
    "#### Determining When to Scale Down to Zero\n",
    "\n",
    "SageMaker's scale-to-zero capability is ideal for three scenarios:\n",
    "\n",
    "1. **Predictable traffic patterns:** If your inference traffic is predictable and follows a consistent schedule, you can use this scaling functionality to automatically scale in to zero during periods of low or no usage. This eliminates the need to manually delete and recreate inference components/endpoints.\n",
    "\n",
    "2. **Sporadic workloads:** For applications that experience sporadic or variable inference traffic patterns, scaling in to zero instances can provide significant cost savings. However, it's important to note that scaling out from zero instances to serving traffic is not instantaneous. During the scale-out process, any requests sent to the endpoint will fail, and these \"NoCapacityInvocationFailures\" will be captured in CloudWatch.\n",
    "\n",
    "3. **Development and testing:** The scale-to-zero functionality is also beneficial when testing and evaluating new machine learning models. During model development and experimentation, you may create temporary inference endpoints to test different configurations. However, it's easy to forget to delete these endpoints when you're done. Scaling to zero ensures these test endpoints automatically scale in to zero instances when not in use, preventing unwanted charges. This allows you to freely experiment without closely monitoring infrastructure usage or remembering to manually delete endpoints. The automatic scaling to zero provides a cost-effective way to test out ideas and iterate on your machine learning solutions.\n",
    "   \n",
    "**Note:** Scale-to-zero is only supported when using inference components. for more information on Inference Components see “[Reduce model deployment costs by 50% on average using the latest features of Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/reduce-model-deployment-costs-by-50-on-average-using-sagemakers-latest-features/)” blog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c926cc-d7b0-4eb7-8ac7-6d6d84c57c7d",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27b126-cf7f-4e87-bb6b-c9c328212b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 huggingface_hub --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1718a-1f73-45f0-bb79-e3d0d7761f5b",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4a700-336b-4bc6-acfd-374fddf614a0",
   "metadata": {},
   "source": [
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "We begin by creating the endpoint configuration and set MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an unique name for our endpoint config\n",
    "endpoint_config_name = f\"{prefix}-llama3-8b-scale-to-zero-aas-config\"\n",
    "print(f\"Endpoint config name: {endpoint_config_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e65d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure variant name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 3\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18901bc5-7df0-4675-aedf-a624792a00f6",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "Next, we create our endpoint using the above endpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-llama3-8b-scale-to-zero-aas-endpoint\"\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac73b99-c239-4b3d-b5c4-ac7fa548db40",
   "metadata": {},
   "source": [
    "#### We wait for our endpoint to go InService. This step can take ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d6b67-7dae-4c02-890e-c3c405b6521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = desc[\"EndpointStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e89259-9e8f-4a45-8dab-74f2c04f4a2a",
   "metadata": {},
   "source": [
    "## import the required libraries and set some variables for the model that we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2998320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "from rich import print\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358758d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session()\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(boto_session) # sagemaker session for interacting with different AWS APIs\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "model_bucket = sagemaker_session.default_bucket()  # bucket to house model artifacts\n",
    "\n",
    "region = sagemaker_session._region_name\n",
    "account_id = sagemaker_session.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea4450-c504-4985-8c48-2e85e90ad96c",
   "metadata": {},
   "source": [
    "#### Set the relevant Model Configurations and select the relevant Large Model Inference container image\n",
    "SageMaker offers optimized [large model inference containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) that contains different frameworks for model parallelism enabling inference of LLMs on multiple GPUs. For more information on the available options, please refer to the [DJL Serving - SageMaker Large Model Inference Configurations](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/configurations_large_model_inference_containers.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c91d5c-22ad-4c70-ac4f-766a7184a1ae",
   "metadata": {},
   "source": [
    "#### Select the container to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-lmi\", region=region, version=\"0.30.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bb0a5-e8a1-4e95-9ec1-2532d770b6f1",
   "metadata": {},
   "source": [
    "#### Set the model to use. In this example, we will use the llama3_8b \n",
    "1. `HF_MODEL_ID`: The model id of a pre-trained model hosted inside a model repository on huggingface.co (https://huggingface.co/models). The container uses this model id to download the corresponding model repository on huggingface.co.\n",
    "\n",
    "2. `HF_TOKEN`: Your HuggingFace token for accessing gated model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a751c84-b060-4923-b14f-2e41dd280628",
   "metadata": {},
   "source": [
    "As environment variables, we provide the correct HuggingFace model ID. Additionally, we also provide our HuggingFace token since this is a gated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b196fdd-fff2-460c-8ae1-fb0bfbd2d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv(\"HUGGING_FACE_HUB_TOKEN\") or getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "\n",
    "hf_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llama3model = {\n",
    "    \"Image\": inference_image_uri,\n",
    "    \"Environment\": {\n",
    "        \"HF_MODEL_ID\": hf_model_id,  # model_id from hf.co/models\n",
    "        \"HF_TOKEN\": HF_TOKEN,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe454e-1d62-462b-bcc4-0aa7b0800cc2",
   "metadata": {},
   "source": [
    "## Create an inference component for our llama3_8b model and invoke the model\n",
    "Inference components can reuse a SageMaker model that you may have already created. You also have the option to specify your artifacts and container directly when creating an inference component which we will show below. In this example we will also create a SageMaker model if you want to reference it later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fad11c-57d7-4c27-bcdb-57a8699136ca",
   "metadata": {},
   "source": [
    "### Create Inference Component (IC)\n",
    "We can now create our inference component. Note below that we specify an inference component name. You can use this name to update your inference compent or view metrics and logs on the inference component you create in CloudWatch. You will also want to set your \"ComputeResourceRequirements\". This will tell SageMaker how much of each resource you want to reserver for EACH COPY of your inference component. Finally we set the number of copies that we want to deploy. The number of copies can be managed through autoscaling policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an unique name for our IC\n",
    "inference_component_name = f\"{prefix}-llama3-8b-scale-to-zero-aas-ic-0\"\n",
    "print(f\"inference component name: {inference_component_name}\")\n",
    "\n",
    "model_name = f\"{prefix}-llama3-8b-scale-to-zero-aas\"\n",
    "print(f\"model name: {model_name}\")\n",
    "\n",
    "sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[llama3model],\n",
    ")\n",
    "\n",
    "initial_copy_count = 1\n",
    "max_copy_count_per_instance = 4  # up to 4 llama3-8b\n",
    "\n",
    "variant_name = \"AllTraffic\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "min_memory_required_in_mb = 1024  # max memory util is up to 85%\n",
    "number_of_accelerator_devices_required = 1\n",
    "\n",
    "sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"MinMemoryRequiredInMb\": min_memory_required_in_mb,\n",
    "            \"NumberOfAcceleratorDevicesRequired\": number_of_accelerator_devices_required,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": initial_copy_count,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d673e12-8b4e-4904-90f8-caf2461f5fc5",
   "metadata": {},
   "source": [
    "#### We wait for our IC to go InService This step can take ~6 mins\n",
    "Let's wait for the endpoint to be ready before proceeding with inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791a7de-4f82-4288-b842-54742058c096",
   "metadata": {},
   "source": [
    "### Test the endpoint with a sample prompt\n",
    "Now we can invoke our endpoint with sample text to test its functionality and see the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b1ea2-78d9-4757-904b-037681cc4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d8031-7234-4fab-aa78-5931066fee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    component_name=inference_component_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": hf_model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b512-7474-4497-afba-d7ca62a4de78",
   "metadata": {},
   "source": [
    "## Automatically Scale in To Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac2748-56a6-456d-afe4-f7af03149efc",
   "metadata": {},
   "source": [
    "## Scaling policies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bde477-bb35-4ece-a03f-330ca2065883",
   "metadata": {},
   "source": [
    "Once the endpoint is deployed and InService, you can then add the necessary scaling policies:\n",
    "\n",
    "* A [target tracking](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html) policy that can scale in the copy count for our inference component model copies to zero, and from 1 to n. \n",
    "* A [step scaling policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html) policy that will allow the endpoint to scale out from zero.\n",
    "\n",
    "These policies work together to provide cost-effective scaling - the endpoint can scale to zero when idle and automatically scale out as needed to handle incoming requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c210d-84f5-4c66-a7b5-52d66a1f70cd",
   "metadata": {},
   "source": [
    "### Scaling policy for inference components copies (target tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96f9f8-5980-47a8-a458-d991a2b617ad",
   "metadata": {},
   "source": [
    "We start with creating our target tracking policies for scaling the CopyCount of our inference component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2eef9-e0ff-4c89-8bd8-3845ac25174c",
   "metadata": {},
   "source": [
    "#### Register a new autoscaling target\n",
    "After you create your SageMaker endpoint and inference components, you register a new auto scaling target for Application Auto Scaling. In the following code block, you set **MinCapacity**  to **0**, which is required for your endpoint to scale down to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa406cfb-6953-4b35-b2eb-3d5c15b6ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client = sagemaker_session.boto_session.client(\"application-autoscaling\")\n",
    "cloudwatch_client = sagemaker_session.boto_session.client(\"cloudwatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoscaling parameters\n",
    "resource_id = f\"inference-component/{inference_component_name}\"\n",
    "service_namespace = \"sagemaker\"\n",
    "scalable_dimension = \"sagemaker:inference-component:DesiredCopyCount\"\n",
    "\n",
    "min_copy_count = 0\n",
    "max_copy_count = 8\n",
    "\n",
    "aas_client.register_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    MinCapacity=min_copy_count,\n",
    "    MaxCapacity=max_copy_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09b8ab-cdf1-4309-814a-f8f1d39bed6e",
   "metadata": {},
   "source": [
    "#### Configure Target Tracking Scaling Policy\n",
    "Once you have registered your new scalable target, the next step is to define your target tracking policy. In the code example that follows, we set the TargetValue to 5. This setting instructs the auto-scaling system to increase capacity when the number of concurrent requests per model reaches or exceeds 5. Here we are taking advantage of the more granular auto scaling metric `PredefinedMetricType`: `SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution` to more accurately monitor and react to changes in inference traffic. Take a look this [blog](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-launches-faster-auto-scaling-for-generative-ai-models/) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c5d8f-274e-4629-86a9-87189911b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client.describe_scalable_targets(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceIds=[resource_id],\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "\n",
    "# The policy name for the target traking policy\n",
    "target_tracking_policy_name = f\"Target-tracking-policy-llama3-8b-scale-to-zero-aas-{inference_component_name}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=target_tracking_policy_name,\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\",\n",
    "        },\n",
    "        # Low TPS + load TPS\n",
    "        \"TargetValue\": 5,  # you need to adjust this value based on your use case\n",
    "        \"ScaleInCooldown\": 300,  # default\n",
    "        \"ScaleOutCooldown\": 300,  # default\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abb304-e25f-4c5b-9b9e-f1e06ebdb91c",
   "metadata": {},
   "source": [
    "Application Auto Scaling creates two CloudWatch alarms per scaling target. The first triggers scale-out actions after 30 seconds (using 3 sub-minute data point), while the second triggers scale-in after 15 minutes (using 90 sub-minute data points). The time to trigger the scaling action is usually 1–2 minutes longer than those minutes because it takes time for the endpoint to publish metrics to CloudWatch, and it also takes time for AutoScaling to react. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1c281-eb3c-45cb-a66a-615ce42217e1",
   "metadata": {},
   "source": [
    "### Scale out from zero policy (step scaling policy )\n",
    "To enable your endpoint to scale out from zero instances, do the following:\n",
    "\n",
    "#### Configure Step Scaling Policy\n",
    "Create a step scaling policy that defines when and how to scale out from zero. This policy will add 1 model copy when triggered, enabling SageMaker to provision the instances required to handle incoming requests after being idle.  The following shows you how to define a step scaling policy. Here we have configured to scale out from 0 to 1 model copy (\"ScalingAdjustment\": 1), depending on your use case you can adjust ScalingAdjustment as required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7901d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy name for the step scaling policy\n",
    "step_scaling_policy_name = f\"Step-scaling-policy-llama3-8b-scale-to-zero-aas-{inference_component_name}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=step_scaling_policy_name,\n",
    "    PolicyType=\"StepScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    StepScalingPolicyConfiguration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"MetricAggregationType\": \"Maximum\",\n",
    "        \"Cooldown\": 60,\n",
    "        \"StepAdjustments\":\n",
    "          [\n",
    "             {\n",
    "               \"MetricIntervalLowerBound\": 0,\n",
    "               \"ScalingAdjustment\": 1\n",
    "             }\n",
    "          ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20def1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = aas_client.describe_scaling_policies(\n",
    "    PolicyNames=[step_scaling_policy_name],\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "step_scaling_policy_arn = resp['ScalingPolicies'][0]['PolicyARN']\n",
    "print(f\"step_scaling_policy_arn: {step_scaling_policy_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698c23f-b733-4b17-8b3d-e9b510392c9f",
   "metadata": {},
   "source": [
    "#### Create the CloudWatch alarm that will trigger our policy\n",
    "\n",
    "Finally, create a CloudWatch alarm with the metric **NoCapacityInvocationFailures**. When triggered, the alarm initiates the previously defined scaling policy. For more information about the NoCapacityInvocationFailures metric, see [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-inference-component).\n",
    "\n",
    "We have also set the following:\n",
    "- EvaluationPeriods to 1 \n",
    "- DatapointsToAlarm to 1 \n",
    "- ComparisonOperator to  GreaterThanOrEqualToThreshold\n",
    "\n",
    "This results in 1 min waiting for the step scaling policy to trigger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alarm name for the step scaling alarm\n",
    "step_scaling_alarm_name = f\"step-scaling-alarm-llama3-8b-scale-to-zero-aas-{inference_component_name}\"\n",
    "\n",
    "cloudwatch_client.put_metric_alarm(\n",
    "    AlarmName=step_scaling_alarm_name,\n",
    "    AlarmActions=[step_scaling_policy_arn],  # Replace with your actual ARN\n",
    "    MetricName='NoCapacityInvocationFailures',\n",
    "    Namespace='AWS/SageMaker',\n",
    "    Statistic='Maximum',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'InferenceComponentName',\n",
    "            'Value': inference_component_name  # Replace with actual InferenceComponentName\n",
    "        }\n",
    "    ],\n",
    "    Period=30, # Set a lower period \n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=1,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='missing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a65b28-839f-4c85-b167-e09a79459480",
   "metadata": {},
   "source": [
    "## Testing the behaviour\n",
    "Notice the `MinInstanceCount: 0` setting in the Endpoint configuration, which allows the endpoint to scale down to zero instances. With the scaling policy, CloudWatch alarm, and minimum instances set to zero, your SageMaker Inference Endpoint will now be able to automatically scale down to zero instances when not in use, helping you optimize your costs and resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5272565-b459-447d-a123-808c2455d593",
   "metadata": {},
   "source": [
    "### IC copy count scales in to zero\n",
    "We'll pause for a few minutes without making any invocations to our model. Based on our target tracking policy, when our SageMaker endpoint doesn't receive requests for 15 minutes, it will automatically scale down to zero the number of model copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(900)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2493b2-010d-4cb7-831e-c39e73bac41c",
   "metadata": {},
   "source": [
    "### Endpoint's instances scale in to zero\n",
    "\n",
    "After 10 additional minutes of inactivity, SageMaker automatically terminates all underlying instances of the endpoint, eliminating all associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e82a84-c0a4-4fcd-be5b-7f51e23d5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 10mins instances will scale down to 0\n",
    "\n",
    "time.sleep(600)\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e163b2b-a4fb-42d9-909c-82264fed850b",
   "metadata": {},
   "source": [
    "### Invoke the endpoint with a sample prompt\n",
    "\n",
    "If we try to invoke our endpoint while instances are scaled down to zero, we get a validation error: `An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56bcfe-c12a-4479-aa80-df1cf03007fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%H:%M:%S\"))\n",
    "# create predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    component_name=inference_component_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": hf_model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa6b86-c683-42d2-bd29-f9da504903aa",
   "metadata": {},
   "source": [
    "### Scale out from zero kicks in\n",
    "However, after 1 minutes our step scaling policy should kick in. SageMaker will then start provisioning a new instance and deploy our inference component model copy to handle requests. This demonstrates the endpoint's ability to automatically scale out from zero when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(60)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2bed85-ce0a-43ed-8115-303dd5db7969",
   "metadata": {},
   "source": [
    "#### verify that our endpoint has succesfully scaled out from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d375f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    component_name=inference_component_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": hf_model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ffab4-db77-467f-b472-a621c229adee",
   "metadata": {},
   "source": [
    "## Optionally clean up the environment\n",
    "\n",
    "- Deregister scalable target\n",
    "- Delete cloudwatch alarms\n",
    "- Delete scaling policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff68b5e-a2f0-411e-9a13-04837bf035b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Deregister the scalable target for AAS\n",
    "    aas_client.deregister_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=scalable_dimension,\n",
    "    )\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] deregistered. ✅\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] not found!.\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "\n",
    "# Delete CloudWatch alarms created for Step scaling policy\n",
    "try:\n",
    "    cloudwatch_client.delete_alarms(AlarmNames=[step_scaling_alarm_name])\n",
    "    print(f\"Deleted CloudWatch step scaling scale-out alarm [b]{step_scaling_alarm_name} ✅\")\n",
    "except cloudwatch_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"CloudWatch scale-out alarm [b]{step_scaling_alarm_name}[/b] not found.\")\n",
    "\n",
    "\n",
    "# Delete step scaling policies\n",
    "print(\"---\" * 10)\n",
    "\n",
    "try:\n",
    "    aas_client.delete_scaling_policy(\n",
    "        PolicyName=step_scaling_policy_name,\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    )\n",
    "    print(f\"Deleted scaling policy [i green]{step_scaling_policy_name} ✅\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scaling policy [i]{step_scaling_policy_name}[/i] not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8d9e6-953d-47a8-94fd-4f142be39132",
   "metadata": {},
   "source": [
    "- Delete inference component\n",
    "- Delete endpoint\n",
    "- delete endpoint-config\n",
    "- Delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11e610-c8c1-4fb2-bd4d-5c71e76a1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sagemaker_client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
