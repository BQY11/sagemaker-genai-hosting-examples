{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# How to deploy the Gemma 3 27B instruct for inference using Amazon SageMakerAI\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to deploy the Gemma 3 27 B instruct model (HuggingFace model ID: [google/gemma-3-27b-it](https://huggingface.co/google/gemma-3-27b-it)) using Amazon SageMaker AI. The inference image will be the SageMaker-managed [LMI (Large Model Inference)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
    "\n",
    "### License agreement\n",
    "* This model is gated on HuggingFace, please refer to the original [model card](https://huggingface.co/google/gemma-3-27b-it) for license.\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.242.0\n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "import logging\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83937110-ffc0-4c42-b67d-0021b829f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sagemaker_session  = sagemaker.Session()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3035e-f732-4429-a7a5-89bf8f822750",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL_ID = \"google/gemma-3-27b-it\"\n",
    "\n",
    "base_name = HF_MODEL_ID.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = HF_MODEL_ID.split(\"/\")[0]\n",
    "base_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5d428-e250-47e8-b751-c48f38fd6b55",
   "metadata": {},
   "source": [
    "## Configure Model Serving Properties\n",
    "\n",
    "Now we'll create a `serving.properties` file that configures how the model will be served. This configuration is crucial for optimal performance and memory utilization.\n",
    "\n",
    "Key configurations explained:\n",
    "- **Engine**: Python backend for model serving\n",
    "- **Model Settings**:\n",
    "  - Using gemma-3-27b-it model from Hugging Face\n",
    "  - Maximum sequence length of 32768 tokens\n",
    "  - model loading timeout of 1200 seconds (20 minutes)\n",
    "- **Performance Optimizations**:\n",
    "  - Tensor parallelism across all available GPUs\n",
    "  - 87% GPU memory utilization target\n",
    "  - vLLM rolling batch with max size of 16 for efficient batching\n",
    "  \n",
    "### Understanding KV Cache and Context Window\n",
    "\n",
    "The `max_model_len` parameter controls the maximum sequence length the model can handle, which directly affects the size of the KV (Key-Value) cache in GPU memory.\n",
    "\n",
    "1. Start with a conservative value (current: 32768)\n",
    "2. Monitor GPU memory usage\n",
    "3. Incrementally increase if memory permits\n",
    "4. Target the model's full context window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753dfbe-803b-478a-8dd7-97c8928eaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory that will contain the files\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('code')\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65dbfe-fe76-4420-8cc1-c36d0c2777db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/serving.properties\n",
    "engine=Python\n",
    "option.trust_remote_code=true\n",
    "option.tensor_parallel_degree=max\n",
    "option.gpu_memory_utilization=.87\n",
    "option.model_loading_timeout=1200\n",
    "option.max_model_len=32768\n",
    "option.max_rolling_batch_size=16\n",
    "option.rolling_batch=vllm\n",
    "option.model_id=google/gemma-3-27b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff13de0-159d-4d77-95bd-362735c2ef08",
   "metadata": {},
   "source": [
    "## Configure vLLM Requirements\n",
    "\n",
    "(Optional) The `requirements.txt` file specifies the vLLM version needed for model inference. vLLM inference framework provides optimized serving capabilities.\n",
    "\n",
    "### Version Considerations\n",
    "- **vLLM 0.8.1**: Required for the gemma-3 models\n",
    "- **transformers 4.50.0** we also update transformers\n",
    "\n",
    "### Performance Impact\n",
    "Different vLLM versions can affect:\n",
    "- Inference speed\n",
    "- Memory utilization\n",
    "- Batch processing efficiency\n",
    "- Compatibility with other libraries\n",
    "\n",
    "### API Considerations\n",
    "\n",
    "The vLLM API has changed in newer versions, so we need to patch the default DJL implementation of the VLLMRollingBatch.inference method with the following:\n",
    "\n",
    "```self.engine.add_request(request_id, prompt=prompt_inputs, params=sampling_params, **request_params)```\n",
    "\n",
    "So provide a custom model.py file that contains the patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5369e-c4bf-42c8-959a-360a6cfb0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "transformers==4.50.0\n",
    "vllm==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6065c-a6ae-4bfe-8af9-7234aab56655",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from djl_python.huggingface import HuggingFaceService\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.rolling_batch.vllm_rolling_batch import VLLMRollingBatch\n",
    "from djl_python.rolling_batch.rolling_batch_vllm_utils import (\n",
    "    update_request_cache_with_output, create_lora_request, get_lora_request,\n",
    "    get_prompt_inputs)\n",
    "import types\n",
    "import logging\n",
    "\n",
    "# Create the service\n",
    "_service = HuggingFaceService()\n",
    "\n",
    "# Define the patched inference method for VLLMRollingBatch\n",
    "def patched_inference(self, new_requests):\n",
    "    \"\"\"\n",
    "    Patched version of the inference method that works with vLLM 0.8.1\n",
    "    \"\"\"\n",
    "    # Import necessary classes from vllm within the function to ensure they're available\n",
    "    from vllm.sampling_params import RequestOutputKind\n",
    "    from vllm import SamplingParams\n",
    "    self.add_new_requests(new_requests)\n",
    "    # step 0: register new requests to engine\n",
    "    for request in new_requests:\n",
    "        from vllm.utils import random_uuid\n",
    "        request_id = random_uuid()\n",
    "        # Chat completions request route\n",
    "        if request.parameters.get(\"sampling_params\") is not None:\n",
    "            prompt_inputs = request.parameters.get(\"engine_prompt\")\n",
    "            sampling_params = request.parameters.get(\"sampling_params\")\n",
    "            sampling_params.output_kind = RequestOutputKind.DELTA\n",
    "        # LMI request route\n",
    "        else:\n",
    "            prompt_inputs = get_prompt_inputs(request)\n",
    "            params = self.translate_vllm_params(request.parameters)\n",
    "            sampling_params = SamplingParams(**params)\n",
    "        request_params = dict()\n",
    "        if request.adapter is not None:\n",
    "            adapter_name = request.adapter.get_property(\"name\")\n",
    "            request_params[\"lora_request\"] = get_lora_request(\n",
    "                adapter_name, self.lora_requests)\n",
    "        \n",
    "        # This is the key change: using the new API format for add_request\n",
    "        # Changed from:\n",
    "        # self.engine.add_request(request_id=request_id, inputs=prompt_inputs, params=sampling_params, **request_params)\n",
    "        # To:\n",
    "        self.engine.add_request(request_id, prompt=prompt_inputs, params=sampling_params, **request_params)\n",
    "        \n",
    "        self.request_cache[request_id] = {\n",
    "            \"request_output\": request.request_output\n",
    "        }\n",
    "    request_outputs = self.engine.step()\n",
    "\n",
    "    # step 1: put result to cache and request_output\n",
    "    for request_output in request_outputs:\n",
    "        self.request_cache = update_request_cache_with_output(\n",
    "            self.request_cache, request_output, self.get_tokenizer())\n",
    "\n",
    "    for request in self.active_requests:\n",
    "        request_output = request.request_output\n",
    "        if request_output.finished:\n",
    "            request.last_token = True\n",
    "\n",
    "    return self.postprocess_results()\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    \"\"\"\n",
    "    Default handler function\n",
    "    \"\"\"\n",
    "    if not _service.initialized:\n",
    "        # Apply the monkey patch to VLLMRollingBatch.inference\n",
    "        try:\n",
    "            # Import the necessary modules from vllm\n",
    "            import vllm\n",
    "            from vllm import SamplingParams\n",
    "            from vllm.sampling_params import RequestOutputKind\n",
    "            \n",
    "            # Patch for vLLM 0.8.1\n",
    "            logging.info(\"Patching VLLMRollingBatch.inference for vLLM 0.8.1\")\n",
    "            VLLMRollingBatch.inference = patched_inference\n",
    "            logging.info(\"Successfully patched VLLMRollingBatch.inference\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to patch VLLMRollingBatch.inference: {e}\")\n",
    "        \n",
    "        # Initialize the service\n",
    "        props = inputs.get_properties()\n",
    "        _service.initialize(props)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # initialization request\n",
    "        return None\n",
    "\n",
    "    return _service.inference(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dd00a-d351-4825-a8e4-6e7629e1c1fc",
   "metadata": {},
   "source": [
    "## Upload uncompress Artifacts to S3\n",
    "SageMaker AI allows us to provide [uncompress files](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html). Thus, we directly upload the folder that contains ``model.py``,  ``serving.properties`` and ``requirements.txt`` to s3\n",
    "\n",
    "This process:\n",
    "1. Determines the S3 bucket location (using SageMaker default bucket)\n",
    "2. Defines a prefix path for organization\n",
    "3. Uploads the packaged artifacts\n",
    "\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34b933-26ad-4017-9cda-a4450d90f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "code_model_uri = S3Uploader.upload(\n",
    "    local_path=\"code\",\n",
    "    desired_s3_uri=f\"s3://{sagemaker_default_bucket}/lmi/{base_name}/code\"\n",
    ")\n",
    "\n",
    "print(f\"code_model_uri: {code_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78d08f-407c-4c57-aa61-172fc28729f0",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Gemma-3-27B-it, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "- **[G6e Instance](https://aws.amazon.com/ec2/instance-types/g6e/)**: AWS's GPU instance type powered by NVIDIA L40S Tensor Core GPUs \n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.g6e.48xlarge` instance which offer:\n",
    "  - 8 NVIDIA L40S Tensor Core GPUs\n",
    "  - 384 GB of total GPU memory (48 GB of memory per GPU)\n",
    "  - up to 400 Gbps of network bandwidth\n",
    "  - up to 1.536 TB of system memory\n",
    "  - and up to 7.6 TB of local NVMe SSD storage.\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a3759-4cce-4a69-9f77-68251aabbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_instance_type = \"ml.g6e.48xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c013786-ac4e-4213-b4a0-29c851077aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu126\".format(sagemaker_session.boto_session.region_name)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4104f5e-883a-4ab3-a82e-93b3b85b43f4",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Model Data** (`model_data`): Our configuration files in S3\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n",
    "\n",
    "### Required Permissions\n",
    "The IAM role needs:\n",
    "- S3 read access for model artifacts\n",
    "- CloudWatch permissions for logging\n",
    "- ECR permissions to pull the container\n",
    "\n",
    "#### HUGGING_FACE_HUB_TOKEN \n",
    "Gemma-3-27B-Instruct is a gated model so you will need to provide your Hugging face ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4ca9e-142d-41cb-82a8-15820c7232e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 URI for your uncompressed code files \n",
    "model_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": f\"{code_model_uri}/\",\n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1bb53-f5f3-490e-aca9-a9790481798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_HUB_TOKEN = \"hf_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527937f2-43e9-428a-b201-ce299894390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = name_from_base(base_name, short=True)\n",
    "\n",
    "# Create model\n",
    "gemma_3_model = Model(\n",
    "    name = model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,  # Path to uncompressed code files\n",
    "    role=role,\n",
    "    env={\n",
    "        \"HF_TASK\": \"Image-Text-to-Text\",\n",
    "        'HUGGING_FACE_HUB_TOKEN': HUGGING_FACE_HUB_TOKEN # Gemma-3-27B-Instruct is a gated model so you will need to provide your Hugging face ID\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fe9cf-a47c-4406-acbd-fd335ac08253",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G6e instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.g6e.48xlarge` for high-performance inference\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment will take 15-20 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb2f42-4790-4c12-850e-b7482c05be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(base_name, short=True)\n",
    "\n",
    "gemma_3_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=gpu_instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83950a-ff8d-4344-add3-6c88b48b8d36",
   "metadata": {},
   "source": [
    "### Use the code below to create a predictor from an existing endpoint and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66831e-dc02-487f-a253-5caa915a98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "endpoint_name = \"\"# replace with your enpoint name \"gemma-3-27b-it-250328-1022\"\n",
    "\n",
    "gemma_3_predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc088b5-0681-411b-9e1d-650736b34723",
   "metadata": {},
   "source": [
    "## Text only Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6a906-d894-4916-b5b0-87ed91fe8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"messages\" : [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write me a poem about Machine Learning.\"}]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\":300,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 250\n",
    "}\n",
    "\n",
    "response = gemma_3_predictor.predict(payload)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"=== Token Usage ===\")\n",
    "usage = response['usage']\n",
    "print(f\"Prompt Tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion Tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total Tokens: {usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158db2a-96db-42c5-8c28-4942513a6950",
   "metadata": {},
   "source": [
    "## Multimodality\n",
    "\n",
    "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0cdac6-ef35-48d0-961c-97cea2717e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "IPyImage(url=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a37dfd-041a-4151-af66-62f2471a1bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\", \n",
    "          \"image_url\": {\"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "response = gemma_3_predictor.predict(payload)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"=== Token Usage ===\")\n",
    "usage = response['usage']\n",
    "print(f\"Prompt Tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion Tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total Tokens: {usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfed40-a054-4704-9612-325faa021ace",
   "metadata": {},
   "source": [
    "## Implement Streaming\n",
    "\n",
    "This section implements a streaming chat interface for real-time interaction with the Gemma 3-27B-it model. The implementation includes:\n",
    "\n",
    "1. **Streaming Infrastructure**:\n",
    "   - Custom `LineIterator` for efficient stream processing\n",
    "   - Real-time token processing\n",
    "   - Performance monitoring (tokens per second)\n",
    "\n",
    "2. **Performance Features**:\n",
    "   - Live response streaming\n",
    "   - Token speed monitoring\n",
    "   - Memory-efficient processing\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### Streaming Parameters\n",
    "- `max_tokens`: 8189 (default)\n",
    "- `temperature`: 0.7 Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling.\n",
    "- `top_p`: 0.9 Float that controls the cumulative probability of the top tokens to consider.\n",
    "- Real-time TPS (Tokens Per Second) monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c377b-e741-484a-8c78-1cbaf28b9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# SageMaker Runtime client\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "# Replace with your SageMaker endpoint name if needed\n",
    "endpoint_name = # replace with your enpoint name \"gemma-3-27b-it-250328-1022\"\n",
    "\n",
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\" + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "\n",
    "def stream_chat_response(endpoint_name, inputs, max_tokens=8189, temperature=0.7, top_p=0.9):    \n",
    "    body = {\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\"type\": \"text\", \"text\": inputs}\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "        \n",
    "        \"max_tokens\":max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "\n",
    "    resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(body),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "\n",
    "    event_stream = resp[\"Body\"]\n",
    "    start_json = b\"{\"\n",
    "    full_response = \"\"\n",
    "    start_time = time.time()\n",
    "    token_count = 0\n",
    "\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b\"\" and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode(\"utf-8\"))\n",
    "            token_text = data['choices'][0]['delta'].get('content', '')\n",
    "            full_response += token_text\n",
    "            token_count += 1\n",
    "\n",
    "            # Calculate tokens per second\n",
    "            elapsed_time = time.time() - start_time\n",
    "            tps = token_count / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "            # Clear the output and reprint everything\n",
    "            clear_output(wait=True)\n",
    "            print(\"Bot:\", full_response)\n",
    "            print(f\"\\nTokens per Second: {tps:.2f}\", end=\"\")\n",
    "\n",
    "    print(\"\\n\") # Add a newline after response is complete\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "def chat(endpoint_name):\n",
    "    print(\"Welcome to the SageMaker Streaming Chat! Type 'exit' to quit.\")\n",
    "    chat_history = []\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        bot_response = stream_chat_response(endpoint_name, user_input)\n",
    "        \n",
    "        # Update chat history\n",
    "        chat_history.append({\n",
    "            'user': user_input,\n",
    "            'assistant': bot_response\n",
    "        })\n",
    "\n",
    "\n",
    "# Start the chat\n",
    "chat(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984819c-e3ec-47d9-92a8-d91fa4998b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "gemma_3_predictor.delete_model()\n",
    "gemma_3_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
