{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ccb432-f6e2-472a-96d3-164ef5ecdcc3",
   "metadata": {},
   "source": [
    "# LightGBM Model Training, Deployment, and SageMaker Features\n",
    "\n",
    "This notebook demonstrates an end-to-end machine learning workflow using Amazon SageMaker AI, focusing on the following key steps:\n",
    "\n",
    "1. Environment Setup: Importing necessary libraries and setting up the SageMaker session.\n",
    "2. Data Preparation: Generating synthetic data for a regression problem.\n",
    "3. Model Training: Training a LightGBM model using SageMaker's built-in algorithm.\n",
    "4. Model Deployment: Deploying the trained model to a SageMaker endpoint for real-time inference.\n",
    "5. Inference Simulation: Simulating thousands of inference requests to the deployed endpoint.\n",
    "6. SageMaker Features:\n",
    "   - Training a second model with different hyperparameters.\n",
    "   - Conducting a shadow test to compare the performance of two model versions.\n",
    "   - Implementing a canary deployment to gradually shift traffic to the new model version.\n",
    "\n",
    "This notebook showcases best practices for model development, testing, and deployment using Amazon SageMaker AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e34953-2f27-4168-b929-84ffd25f021b",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "Import Libraries and Set Up SageMaker Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca5b992-055d-4bbe-8798-cae004403c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role, image_uris, script_uris, model_uris\n",
    "from sagemaker.session import Session\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up SageMaker session\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/lightgbm-demo'\n",
    "aws_region = boto3.Session().region_name\n",
    "print(f\"SageMaker session initialized. Using bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5a487-d573-41a3-9916-bdb01a66cb2b",
   "metadata": {},
   "source": [
    "# Data Preparation and Upload\n",
    "\n",
    "Generate synthetic data for a regression problem, split it into training and test sets, and upload to S3.\n",
    "\n",
    "The synthetic data is generated using the following equation:\n",
    "\n",
    "y = 3 * X₁ + 2 * X₂ - 5 * X₃ + ε\n",
    "\n",
    "where X₁, X₂, X₃ are the first three features, and ε is Gaussian noise with a standard deviation of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824f519-d548-462d-9b1d-6f5bb2d2ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare your data (replace this with your actual data)\n",
    "np.random.seed(0)\n",
    "n_samples = 1000\n",
    "X = np.random.randn(n_samples, 5)\n",
    "y = 3 * X[:, 0] + 2 * X[:, 1] - 5 * X[:, 2] + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Combine features and target for training data\n",
    "train_data = np.column_stack((y_train, X_train_scaled))\n",
    "\n",
    "# Save the training data to a CSV file\n",
    "train_file = 'train.csv'\n",
    "np.savetxt(train_file, train_data, delimiter=',')\n",
    "\n",
    "# Upload the training data to S3\n",
    "train_s3_path = session.upload_data(train_file, bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print(f\"Training data uploaded to: {train_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c4abb-d9dd-4d0d-a559-2a2bbc4bdb53",
   "metadata": {},
   "source": [
    "## Configure LightGBM Model\n",
    "\n",
    "This section sets up the LightGBM model configuration for training. It includes:\n",
    "\n",
    "- Specifying the model ID, version, and training instance type\n",
    "- Retrieving necessary URIs for the training image, source code, and model artifacts\n",
    "- Setting hyperparameters for the LightGBM model, including learning rate, max depth, and other key parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f6571-3828-4299-a840-1f85959ee400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "train_model_id = \"lightgbm-regression-model\"\n",
    "train_model_version = \"*\"\n",
    "train_scope = \"training\"\n",
    "training_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve URIs\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type\n",
    ")\n",
    "\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, \n",
    "    model_version=train_model_version, \n",
    "    script_scope=train_scope\n",
    ")\n",
    "\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, \n",
    "    model_version=train_model_version, \n",
    "    model_scope=train_scope\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    \"num_boost_round\": \"500\",\n",
    "    \"learning_rate\": \"0.1\",\n",
    "    \"max_depth\": \"6\",\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"early_stopping_rounds\": \"30\",\n",
    "    \"feature_fraction\": \"0.74\",\n",
    "    \"bagging_fraction\": \"0.53\",\n",
    "    \"bagging_freq\": \"5\",\n",
    "    \"num_leaves\": \"67\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0146827-8c7d-4e1a-89f1-a729b4695de3",
   "metadata": {},
   "source": [
    "## Train LightGBM Model with Amazon SageMaker AI's built-in algorithm.\n",
    "\n",
    "- Creating a SageMaker Estimator with the specified configuration\n",
    "- Setting up the training job with a unique name\n",
    "- Launching the model training process using the prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43236f2e-2f28-40a5-be00-a847acc298eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "training_job_name = f\"tx-train-{timestamp}\"[:63]\n",
    "\n",
    "# Create SageMaker Estimator\n",
    "tabular_estimator = Estimator(\n",
    "    role=role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=f's3://{bucket}/{prefix}/output'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "tabular_estimator.fit({\"training\": train_s3_path}, logs=True, job_name=training_job_name, wait=True)\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0e61d-2c5c-4c7f-b9ef-5f4d1756f41a",
   "metadata": {},
   "source": [
    "## Deploy Trained Model to SageMaker Endpoint\n",
    "\n",
    "This section handles the deployment of the trained LightGBM model to a SageMaker endpoint for real-time inference. The process includes:\n",
    "\n",
    "1. Retrieving the model artifact from the training job\n",
    "2. Creating a SageMaker model using the trained artifact\n",
    "3. Setting up an endpoint configuration\n",
    "4. Creating and deploying the endpoint\n",
    "\n",
    "The deployment uses an ml.m5.xlarge instance for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ef107-399b-4549-82cb-2ac4704e67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model artifact from training\n",
    "training_job = boto3.client('sagemaker').describe_training_job(\n",
    "    TrainingJobName=training_job_name\n",
    ")\n",
    "model_artifact = training_job['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "# Create shorter unique names for deployment resources\n",
    "model_name = f\"tx-model-{timestamp}\"[:63]\n",
    "endpoint_config_name = f\"tx-config-{timestamp}\"[:63]\n",
    "endpoint_name = f\"tx-endpoint-{timestamp}\"[:63]\n",
    "\n",
    "# Get inference image and source\n",
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, \n",
    "    model_version=train_model_version, \n",
    "    script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Create model\n",
    "print(f\"Creating model: {model_name}\")\n",
    "create_model_response = boto3.client('sagemaker').create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        'Image': deploy_image_uri,\n",
    "        'ModelDataUrl': model_artifact,\n",
    "        'Environment': {\n",
    "            'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "            'SAGEMAKER_SUBMIT_DIRECTORY': deploy_source_uri,\n",
    "            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
    "            'SAGEMAKER_REGION': aws_region\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create endpoint configuration\n",
    "print(f\"Creating endpoint configuration: {endpoint_config_name}\")\n",
    "create_endpoint_config_response = boto3.client('sagemaker').create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': inference_instance_type,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Create endpoint\n",
    "print(f\"Creating endpoint: {endpoint_name}\")\n",
    "create_endpoint_response = boto3.client('sagemaker').create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "# Wait for endpoint to be ready\n",
    "print(\"Waiting for endpoint to be ready...\")\n",
    "waiter = boto3.client('sagemaker').get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "print(\"Endpoint is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c2e1e-60d1-4eb0-bce8-be088b39e4d5",
   "metadata": {},
   "source": [
    "## Real-time Inference with SageMaker Endpoint\n",
    "\n",
    "This section sets up the infrastructure for making real-time predictions using the deployed SageMaker endpoint. It includes:\n",
    "\n",
    "1. Creating a Predictor object for the endpoint\n",
    "2. Implementing a mock feature store for demonstration purposes\n",
    "3. Defining functions to retrieve features and make predictions\n",
    "4. Demonstrating the prediction process with example calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a2c7d-58ba-45d0-aa4a-6ff7a2fb4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import random\n",
    "\n",
    "# Create a predictor\n",
    "predictor = Predictor(endpoint_name=endpoint_name, serializer=CSVSerializer(), deserializer=JSONDeserializer())\n",
    "\n",
    "# Mock feature store\n",
    "mock_feature_store = {\n",
    "    '123': {\n",
    "        'model_v1': [0.5, -0.3, 1.2, 0.8, -0.1],\n",
    "        'model_v2': [0.6, -0.2, 1.1, 0.9, -0.2]\n",
    "    },\n",
    "    '456': {\n",
    "        'model_v1': [-0.1, 0.7, -0.5, 1.3, 0.2],\n",
    "        'model_v2': [-0.2, 0.8, -0.4, 1.2, 0.3]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to retrieve features from online feature store (sample implementation)\n",
    "def get_features_from_feature_store(id_identifier, id_model):\n",
    "    # Check if the id_identifier exists in our mock feature store\n",
    "    if id_identifier in mock_feature_store:\n",
    "        # Check if the id_model exists for this identifier\n",
    "        if id_model in mock_feature_store[id_identifier]:\n",
    "            return mock_feature_store[id_identifier][id_model]\n",
    "    return [random.uniform(-1, 1) for _ in range(5)]\n",
    "\n",
    "# Function to make predictions\n",
    "def predict(id_identifier, id_model):\n",
    "    # Retrieve features from feature store\n",
    "    features = get_features_from_feature_store(id_identifier, id_model)\n",
    "    \n",
    "    # Scale the features\n",
    "    features_scaled = scaler.transform(np.array(features).reshape(1, -1))\n",
    "    \n",
    "    # Prepare payload\n",
    "    payload = ','.join(map(str, features_scaled.flatten()))\n",
    "    \n",
    "    # Make prediction\n",
    "    response = predictor.predict(payload)\n",
    "    \n",
    "    return {\n",
    "        'id_identifier': id_identifier,\n",
    "        'id_model': id_model,\n",
    "        'features': features,\n",
    "        'prediction': response\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result1 = predict('123', 'model_v1')\n",
    "print(\"Result 1:\", result1)\n",
    "\n",
    "result2 = predict('456', 'model_v2')\n",
    "print(\"Result 2:\", result2)\n",
    "\n",
    "result3 = predict('789', 'model_v3')  # This will use random features\n",
    "print(\"Result 3:\", result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb4707-ab7a-4184-b86d-89dce4cdfda9",
   "metadata": {},
   "source": [
    "## Simulate Inference Requests\n",
    "\n",
    "This section demonstrates how to simulate a large number of inference requests to the deployed SageMaker endpoint. Key features include:\n",
    "\n",
    "- A function to generate and process multiple random prediction requests\n",
    "- Timing the overall process and calculating average request time\n",
    "- Simulating 5000 requests to stress-test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0266b9b-3cd9-415f-84e8-e9025c5232c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def simulate_requests(num_requests=1000):\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(num_requests):\n",
    "        id_identifier = str(random.randint(100, 999))\n",
    "        id_model = f\"model_v{random.randint(1, 2)}\"\n",
    "        result = predict(id_identifier, id_model)\n",
    "        results.append(result)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Completed {num_requests} requests in {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per request: {(total_time / num_requests) * 1000:.2f} ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Simulate requests\n",
    "simulation_results = simulate_requests(5000)  # Simulate 5000 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9511cf-0fe6-46ef-b206-b7a074f354e9",
   "metadata": {},
   "source": [
    "## Train and Create Second LightGBM Model\n",
    "\n",
    "This section demonstrates the process of training and creating a second LightGBM model with different hyperparameters. Key steps include:\n",
    "\n",
    "1. Defining new hyperparameters for the second model\n",
    "2. Creating a new SageMaker Estimator with these hyperparameters\n",
    "3. Training the second model using the same training data\n",
    "4. Retrieving the model artifact from the new training job\n",
    "5. Creating a new SageMaker model using the trained artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9512ad3-fd29-41a1-82a5-9bec3601c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for the second model\n",
    "hyperparameters_v2 = {\n",
    "    \"num_boost_round\": \"1000\",\n",
    "    \"learning_rate\": \"0.05\",\n",
    "    \"max_depth\": \"8\",\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"early_stopping_rounds\": \"50\",\n",
    "    \"feature_fraction\": \"0.8\",\n",
    "    \"bagging_fraction\": \"0.7\",\n",
    "    \"bagging_freq\": \"3\",\n",
    "    \"num_leaves\": \"100\"\n",
    "}\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "training_job_name_v2 = f\"tx-train-v2-{timestamp}\"[:63]\n",
    "\n",
    "# Create SageMaker Estimator for the second model\n",
    "tabular_estimator_v2 = Estimator(\n",
    "    role=role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters_v2,\n",
    "    output_path=f's3://{bucket}/{prefix}/output'\n",
    ")\n",
    "\n",
    "# Train the second model\n",
    "print(\"Starting model v2 training...\")\n",
    "tabular_estimator_v2.fit({\"training\": train_s3_path}, logs=True, job_name=training_job_name_v2)\n",
    "print(\"Model v2 training completed.\")\n",
    "\n",
    "# Get the model artifact for the second model\n",
    "training_job_v2 = boto3.client('sagemaker').describe_training_job(\n",
    "    TrainingJobName=training_job_name_v2\n",
    ")\n",
    "model_artifact_v2 = training_job_v2['ModelArtifacts']['S3ModelArtifacts\n",
    "\n",
    "# Create the second model\n",
    "\n",
    "model_name_v2 = f\"tx-model-v2-{timestamp}\"[:63]\n",
    "\n",
    "create_model_response_v2 = boto3.client('sagemaker').create_model(\n",
    "    ModelName=model_name_v2,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        'Image': deploy_image_uri,\n",
    "        'ModelDataUrl': model_artifact_v2,\n",
    "        'Environment': {\n",
    "            'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "            'SAGEMAKER_SUBMIT_DIRECTORY': deploy_source_uri,\n",
    "            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
    "            'SAGEMAKER_REGION': aws_region\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created second model: {model_name_v2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fb7dc-6404-4bb2-b25d-b4d66c02641b",
   "metadata": {},
   "source": [
    "## Shadow Test Experiment Setup\n",
    "\n",
    "This section creates a shadow test experiment to compare the performance of two model versions. It sets up a one-hour experiment where 5% of the production traffic is routed to the shadow model (model_v2) for evaluation. The experiment uses ml.m5.xlarge instances for both production and shadow variants, with results stored in S3 for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a17f8-65ab-471f-a8f9-f742221e86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Create shadow test experiment\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M-%S')\n",
    "experiment_name = f\"tx-shadow-test-{timestamp}\"[:63]\n",
    "\n",
    "# Set up S3 destination for shadow test results\n",
    "shadow_test_results_path = f\"s3://{bucket}/{prefix}/shadow-test-results\"\n",
    "\n",
    "# Set start time and end time (1 hour)\n",
    "start_time = datetime.now()\n",
    "end_time = start_time + timedelta(hours=1)\n",
    "\n",
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "response = boto3.client('sagemaker').create_inference_experiment(\n",
    "    Name=experiment_name,\n",
    "    Type='ShadowMode',\n",
    "    RoleArn=role,\n",
    "    EndpointName=endpoint_name,\n",
    "    ModelVariants=[\n",
    "        {\n",
    "            'ModelName': model_name,\n",
    "            'VariantName': 'Production',\n",
    "            'InfrastructureConfig': {\n",
    "                'InfrastructureType': 'RealTimeInference',\n",
    "                'RealTimeInferenceConfig': {\n",
    "                    'InstanceType': inference_instance_type,\n",
    "                    'InstanceCount': 1\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'ModelName': model_name_v2,\n",
    "            'VariantName': 'Shadow',\n",
    "            'InfrastructureConfig': {\n",
    "                'InfrastructureType': 'RealTimeInference',\n",
    "                'RealTimeInferenceConfig': {\n",
    "                    'InstanceType': inference_instance_type,\n",
    "                    'InstanceCount': 1\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    Schedule={\n",
    "        'StartTime': start_time,\n",
    "        'EndTime': end_time\n",
    "    },\n",
    "    DataStorageConfig={\n",
    "        'Destination': shadow_test_results_path,\n",
    "        'ContentType': {\n",
    "            'CsvContentTypes': ['text/csv'],\n",
    "            'JsonContentTypes': ['application/json']\n",
    "        }\n",
    "    },\n",
    "    ShadowModeConfig={\n",
    "        'SourceModelVariantName': 'Production',\n",
    "        'ShadowModelVariants': [\n",
    "            {\n",
    "                'ShadowModelVariantName': 'Shadow',\n",
    "                'SamplingPercentage': 5\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created inference experiment: {experiment_name}\")\n",
    "print(f\"Experiment ARN: {response.get('ExperimentArn', 'N/A')}\")\n",
    "print(f\"Start time: {start_time}\")\n",
    "print(f\"End time: {end_time}\")\n",
    "print(f\"Duration: 1 hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a1ed6-1a32-4874-9e1f-5ffd9acf04db",
   "metadata": {},
   "source": [
    "## Canary Deployment\n",
    "\n",
    "This section implements a canary deployment strategy for updating the SageMaker endpoint. It gradually shifts traffic from the old model to the new one, allocating 20% capacity to the new version in each step with a 5-minute interval between steps. This approach allows for monitoring the new model's performance and minimizes risk during the deployment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40daf6d2-240f-4cd7-8fd5-7a281c43c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "##canary deployment\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Get the current endpoint configuration\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "endpoint_description = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "current_endpoint_config_name = endpoint_description['EndpointConfigName']\n",
    "\n",
    "# Get the current endpoint configuration details\n",
    "current_endpoint_config = sagemaker_client.describe_endpoint_config(\n",
    "    EndpointConfigName=current_endpoint_config_name\n",
    ")\n",
    "current_variant_name = current_endpoint_config['ProductionVariants'][0]['VariantName']\n",
    "print(f\"Current variant name: {current_variant_name}\")\n",
    "\n",
    "# Create a new endpoint configuration for the canary deployment\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "canary_config_name = f\"tx-config-canary-{timestamp}\"[:63]\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=canary_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': current_variant_name,  \n",
    "            'ModelName': model_name_v2,  \n",
    "            'InstanceType': 'ml.m5.xlarge',\n",
    "            'InitialInstanceCount': 2,\n",
    "            'InitialVariantWeight': 1  \n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the canary deployment configuration\n",
    "canary_config = {\n",
    "    \"BlueGreenUpdatePolicy\": {\n",
    "        \"TrafficRoutingConfiguration\": {\n",
    "            \"Type\": \"CANARY\",\n",
    "            \"CanarySize\": {\n",
    "                \"Type\": \"CAPACITY_PERCENT\",\n",
    "                \"Value\": 20  # Increase traffic to new version by 20% each step\n",
    "            },\n",
    "            \"WaitIntervalInSeconds\": 300  # 5 minutes between steps\n",
    "        },\n",
    "        \"TerminationWaitInSeconds\": 600,  # Wait 10 minutes before terminating old instances\n",
    "        \"MaximumExecutionTimeoutInSeconds\": 3600  # 60 minutes maximum for the entire deployment\n",
    "    }\n",
    "}\n",
    "\n",
    "# Start canary deployment\n",
    "print(\"Starting canary deployment...\")\n",
    "update_response = sagemaker_client.update_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=canary_config_name,\n",
    "    DeploymentConfig=canary_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f7b08-1d58-43bf-8590-ac45678f4f57",
   "metadata": {},
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "Don't forget to delete the endpoint when you're done to avoid incurring unnecessary costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b296bc1-1fc2-447f-8e76-6502eca3a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After successful deployment, you may want to clean up the old model and endpoint configuration\n",
    "def cleanup_old_resources():\n",
    "    try:\n",
    "        sagemaker_client.delete_model(ModelName=model_name)\n",
    "        sagemaker_client.delete_endpoint_config(EndpointConfigName=current_endpoint_config_name)\n",
    "        print(\"Cleaned up old model and endpoint configuration.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup: {str(e)}\")\n",
    "\n",
    "# Uncomment the following lines when you're ready to clean up\n",
    "#cleanup_old_resources()\n",
    "#sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
