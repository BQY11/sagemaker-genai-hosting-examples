{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a067e6d7-dc42-4d7b-b7d7-f65480b2cfaa",
   "metadata": {},
   "source": [
    "# Enhancing deployment guardrails with inference component rolling update for Amazon SageMaker AI Inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In today's machine learning landscape, deploying models efficiently, reliably, and cost-effectively is a critical challenge for organizations of all sizes. As organizations increasingly deploy foundation models (FMs) and other ML models to production, they face challenges related to resource utilization, cost efficiency, and maintaining high availability during updates. Amazon SageMaker AI introduced Inference Component (IC) functionality that can help organizations reduce model deployment costs by optimizing resource utilization through intelligent model packing and scaling. Inference components abstract ML models and enable assigning dedicated resources and specific scaling policies per model.\n",
    "\n",
    "However, updating these models—especially in production environments with strict latency SLAs—has historically risked downtime or resource bottlenecks. Traditional blue/green (B/G) deployments often struggle with capacity constraints, making updates unpredictable for GPU-heavy models. To address this, we're excited to announce another powerful enhancement to Amazon SageMaker AI: rolling update for inference components, a feature designed to streamline updates for models of all sizes while minimizing operational overhead. \n",
    "\n",
    "In this blog post, we will first discuss the challenges faced by organizations when updating models in production. Then we will deep dive into the new rolling update feature for inference components and provide practical examples using DeepSeek distilled models to demonstrate this feature. Finally we will explore how to setup rolling update in different scenarios.  \n",
    "\n",
    "## Challenges with Blue/Green Deployment\n",
    "\n",
    "Traditionally, SageMaker AI inference has supported the blue/green deployment pattern for updating ICs in production. While effective for many scenarios, this approach comes with specific challenges:\n",
    "\n",
    "* Resource Inefficiency: Blue/Green deployment requires provisioning resources for both the current (blue) and new (green) environments simultaneously. For inference components running on expensive GPU instances like P4d or G5, this means potentially doubling the resource requirements during deployments. Consider an example where a customer has 10 copies of an inference component spread across 5 x ml.p4d.24xlarge instances, all operating at full capacity. With Blue/Green deployment, SageMaker would need to provision 5 additional ml.p4d.24xlarge instances to host the new version of the inference component before switching traffic and decommissioning the old instances.\n",
    "* Limited Computing Resources: For customers using powerful GPU instances like P and/or G series, the required capacity might simply not be available in a given Availability Zone or Region. This often results in Instance Capacity Exceptions (ICE) during deployments, causing update failures and rollbacks.\n",
    "* All-or-Nothing Transitions: Traditional blue/green deployments shift all traffic at once or based on a configured schedule. This leaves limited room for gradual validation and increases the blast radius if issues arise with the new deployment.\n",
    "\n",
    "While blue/green deployment has been a reliable strategy for zero-downtime updates, its limitations become glaring when deploying large-scale LLMs or high-throughput models on premium GPU instances. These challenges demand a more nuanced approach—one that incrementally validates updates while optimizing resource usage. Enter rolling updates for inference components, a paradigm designed to eliminate the rigidity of blue/green. By updating models in controlled batches, dynamically scaling infrastructure, and integrating real-time safety checks, this strategy ensures deployments remain cost-effective, reliable, and adaptable—even for GPU-heavy workloads. Let’s explore how it works.\n",
    "\n",
    "\n",
    "## Introducing Rolling Deployment for Inference Component Update\n",
    "\n",
    "\n",
    "Before diving deeper, let's briefly recap what Inference Components are. Introduced as a SageMaker feature to optimize costs, Inference Components (ICs) allow you to define and deploy the specific resources needed for your model inference workload. By right-sizing compute resources to match your model's requirements, you can achieve significant cost savings compared to traditional deployment approaches.\n",
    "\n",
    "\n",
    "In this notebook, we demonstrate the process of a rolling upgrade for an Inference Component by updating it from running Meta's Llama 3.1 8B Instruct to DeepSeek's DeepSeek R1 Distill Llama 8B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec3f5e-bbee-48ce-b98a-b39b758f45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c8e91-9d74-48ee-8c00-fdaee503a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "sagemaker_runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "session=sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"rolling-upgrade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b36422-fde5-48e4-a3ff-dee80a681089",
   "metadata": {},
   "source": [
    "Inference Components allow us to create a SageMaker AI Endpoint that does not initally have a model running when deployed.\n",
    "\n",
    "Below we create an endpoint using the `ml.g5.2xlarge` instance with 1 GPU per instance. We configure Managed Instance Scaling from a minimum of 1 to a maximum of 4. SageMaker AI will automatically scale the number of instances behind the endpoint to ensure we can fit the requested inference component count on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a8fe8-06b6-4e66-b05f-40cc1f811c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "\n",
    "variant_name=\"AllTraffic\"\n",
    "instance_type=\"ml.g5.2xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 4\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 2,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71076cd0-fcce-455c-9b71-95c4d6b91db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa98d65-56d3-4b93-816a-2e74da018177",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11875293-edba-49d7-a5d2-65f40168e61a",
   "metadata": {},
   "source": [
    "Once the endpoint is created, we create our inference componnet and request that it runs using 1 accelerator device with 2 copies. We use Hugging Face's Text Generation Inference (TGI) container which is able to download our model from the Hugging Face Hub and load it onto the GPU. We also use the `'MESSAGES_API_ENABLED'` to allow the inference interface to accept the Messages API format.\n",
    "\n",
    "We set the `HF_MODEL_ID` environment variable to `'meta-llama/Llama-3.1-8B-Instruct'` and configure the `HF_TOKEN` environment variable to a valid token. This will allow the endpoint to download the gated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f88fb-7928-4f62-8f7a-1ac5c1208cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "region = session._region_name\n",
    "image_uri = image_uris.retrieve(framework='huggingface-llm', region=region, version='2.2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb8c11-4448-491c-8a32-a540661f523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_component_name=f\"llama-3-1-8b-{prefix}\"\n",
    "# variant_name=\"AllTraffic\"\n",
    "number_of_accelerator_devices_required=1\n",
    "min_memory_required_in_mb=1024\n",
    "\n",
    "hf_token = \"<INSERT_TOKEN_HERE>\"\n",
    "\n",
    "assert hf_token != \"<INSERT_TOKEN_HERE>\"\n",
    "\n",
    "sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        'Container': {\n",
    "            'Image': image_uri,\n",
    "            'Environment': {\n",
    "                'SM_NUM_GPUS': str(number_of_accelerator_devices_required),\n",
    "                'HF_MODEL_ID': 'meta-llama/Llama-3.1-8B-Instruct',\n",
    "                'HF_TOKEN': hf_token,\n",
    "                \"MESSAGES_API_ENABLED\": \"true\",\n",
    "            }\n",
    "        },\n",
    "        'ComputeResourceRequirements': {\n",
    "            'NumberOfAcceleratorDevicesRequired': number_of_accelerator_devices_required,\n",
    "            'MinMemoryRequiredInMb': min_memory_required_in_mb\n",
    "        }\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        'CopyCount': 2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72ebfc-57e4-44e9-947b-06c621243af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0991626-a28e-40ab-ba1f-a88e110e0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ea9c8-a9e0-4137-97a3-7c586de7bb18",
   "metadata": {},
   "source": [
    "Once our Inference Component is `InService` we can invoke with the InvokeEndpoint API as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde86ef4-0c72-49e1-ba03-09b5a9dea3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "    ]\n",
    "\n",
    "payload = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.6\n",
    "}\n",
    "\n",
    "response_model = sagemaker_runtime_client.invoke_endpoint( \n",
    "    InferenceComponentName=inference_component_name,\n",
    "    EndpointName=endpoint_name, \n",
    "    Body=json.dumps(payload), \n",
    "    ContentType=\"application/json\", Accept=\"application/json\")\n",
    "\n",
    "print(json.loads(response_model['Body'].read().decode('utf-8'))['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66acbd-d808-43c8-a981-b2136ac2bb64",
   "metadata": {},
   "source": [
    "We can configure our Inference Component update to track a CloudWatch Alarm during the update period. If the alarm is triggered then the deployment rolls back to the previous state.\n",
    "\n",
    "We can configure an Amazon CloudWatch Alarm to alarm when we see more than 5 4xx errors from our Inference Component. When using TGI, you can see 4xx errors when there's an input mismatch (ie. if the container isn't configured to support the Messages API).\n",
    "\n",
    "You can configure the alarm to track any CloudWatch metric and should ensure the metrics are accurate to track and alarm on potential deployment errors to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af8ac7-a620-43bd-9e51-551c4c372184",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "# Create alarm\n",
    "cloudwatch.put_metric_alarm(\n",
    "    AlarmName=f'SageMaker-{endpoint_name}-4xx-errors',\n",
    "    ComparisonOperator='GreaterThanThreshold',\n",
    "    EvaluationPeriods=1,\n",
    "    MetricName='Invocation4XXErrors',\n",
    "    Namespace='AWS/SageMaker',\n",
    "    Period=300,\n",
    "    Statistic='Sum',\n",
    "    Threshold=5.0,\n",
    "    ActionsEnabled=True,\n",
    "    AlarmDescription='Alarm when greather than 5 4xx errors',\n",
    "    Dimensions=[\n",
    "        {\n",
    "          'Name': 'InferenceComponentName',\n",
    "          'Value': inference_component_name\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bca65-0da5-41c4-b355-7325613ec097",
   "metadata": {},
   "source": [
    "Below we update our inference component to use `'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'` from the Hugging Face Hub. \n",
    "\n",
    "We do not set the environment variable `MESSAGES_API_ENABLED` which means the previous invocations will fail with 4xx errors as the invocation payload is not compatible.\n",
    "\n",
    "When updating an inference component, the new copies must have backwards compatible APIs as the service can route to any copy that is both old or new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2968c-35f6-477e-990d-d08143202fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.update_inference_component(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    Specification={\n",
    "        'Container': {\n",
    "            'Image': image_uri,\n",
    "            'Environment': {\n",
    "                'SM_NUM_GPUS': str(number_of_accelerator_devices_required),\n",
    "                'HF_MODEL_ID': 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
    "                # \"MESSAGES_API_ENABLED\": \"true\",\n",
    "            }\n",
    "        },\n",
    "        'ComputeResourceRequirements': {\n",
    "            'NumberOfAcceleratorDevicesRequired': number_of_accelerator_devices_required,\n",
    "            'MinMemoryRequiredInMb': min_memory_required_in_mb\n",
    "        }\n",
    "    },\n",
    "    DeploymentConfig={\n",
    "        \"RollingUpdatePolicy\": {\n",
    "            \"MaximumBatchSize\": {\n",
    "                \"Type\": \"COPY_COUNT\",\n",
    "                \"Value\": 1\n",
    "            },\n",
    "            \"WaitIntervalInSeconds\": 120,\n",
    "            \"RollbackMaximumBatchSize\": {\n",
    "                \"Type\": \"COPY_COUNT\",\n",
    "                \"Value\": 1\n",
    "            }\n",
    "        },\n",
    "        'AutoRollbackConfiguration': {\n",
    "            \"Alarms\": [\n",
    "                {\"AlarmName\": f'SageMaker-{endpoint_name}-4xx-errors'}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a49fbc-8745-41fb-9902-13ffe2b10c9b",
   "metadata": {},
   "source": [
    "Below will invoke the endpoint in a loop and report any failures. Due to the incompatible API format, invocations to the new Inference Component will fail and print `\"invocation failed\"`. Successful invocations to the original copy will print the result.\n",
    "\n",
    "We can see the progress in the deployment with the `\"RuntimeConfig\"` output from the `DescribeInferenceComponent` API. The new versions are deployed as additional copies that the service will route a percentage of traffic to. \n",
    "\n",
    "When the CloudWatch Alarm fires, we will see that the Inference Component will revert to `InService` status without any changes, allowing our invocations to continue successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fcd69-98ef-4788-8513-17320cf1b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "while desc['InferenceComponentStatus'] == 'Updating':\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.6\n",
    "    }\n",
    "    try:\n",
    "        response_model = sagemaker_runtime_client.invoke_endpoint( \n",
    "            InferenceComponentName=inference_component_name,\n",
    "            EndpointName=endpoint_name, \n",
    "            Body=json.dumps(payload), \n",
    "            ContentType=\"application/json\", Accept=\"application/json\")\n",
    "        print(json.loads(response_model['Body'].read().decode('utf-8'))['choices'][0]['message']['content'])\n",
    "    except:\n",
    "        print('invocation failed')\n",
    "    alarm_info = cloudwatch.describe_alarms(\n",
    "        AlarmNames=[\n",
    "            f'SageMaker-{endpoint_name}-4xx-errors'\n",
    "        ],\n",
    "    )\n",
    "    print(f\"Alarm state:{alarm_info['MetricAlarms'][0]['StateValue']}\")\n",
    "    \n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    print(desc['InferenceComponentStatus'])\n",
    "    print(desc['RuntimeConfig'])\n",
    "    time.sleep(10)\n",
    "\n",
    "response_model = sagemaker_runtime_client.invoke_endpoint( \n",
    "        InferenceComponentName=inference_component_name,\n",
    "        EndpointName=endpoint_name, \n",
    "        Body=json.dumps(payload), \n",
    "        ContentType=\"application/json\", Accept=\"application/json\")\n",
    "    \n",
    "print(json.loads(response_model['Body'].read().decode('utf-8'))['choices'][0]['message']['content'])\n",
    "print(desc['InferenceComponentStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533afe2a-17e6-424f-ae7f-db06b392aa2e",
   "metadata": {},
   "source": [
    "Below we perform the same update again while also correctly setting the `\"MESSAGES_API_ENABLED\"` environment variable. Once again, we loop through the invocations and print the result. \n",
    "\n",
    "DeepSeek R1 is a reasoning model that will think before it responds where the thinking is visible in the model output. As our Inference Component is updated, we can see which invocations are routed to the DeepSeek variant by the thinking shown in the response. Once again, we can also track the deployment process with the `CurrentCopyCount` field for the Inference Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c2d52-1e3a-4768-88a1-8c300145be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.update_inference_component(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    Specification={\n",
    "        'Container': {\n",
    "            'Image': image_uri,\n",
    "            'Environment': {\n",
    "                'SM_NUM_GPUS': str(number_of_accelerator_devices_required),\n",
    "                'HF_MODEL_ID': 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
    "                \"MESSAGES_API_ENABLED\": \"true\",\n",
    "            }\n",
    "        },\n",
    "        'ComputeResourceRequirements': {\n",
    "            'NumberOfAcceleratorDevicesRequired': number_of_accelerator_devices_required,\n",
    "            'MinMemoryRequiredInMb': min_memory_required_in_mb\n",
    "        }\n",
    "    },\n",
    "    DeploymentConfig={\n",
    "        \"RollingUpdatePolicy\": {\n",
    "            \"MaximumBatchSize\": {\n",
    "                \"Type\": \"COPY_COUNT\",\n",
    "                \"Value\": 1\n",
    "            },\n",
    "            \"WaitIntervalInSeconds\": 120,\n",
    "            \"RollbackMaximumBatchSize\": {\n",
    "                \"Type\": \"COPY_COUNT\",\n",
    "                \"Value\": 1\n",
    "            }\n",
    "        },\n",
    "        'AutoRollbackConfiguration': {\n",
    "            \"Alarms\": [\n",
    "                {\"AlarmName\": f'SageMaker-{endpoint_name}-4xx-errors'}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0440749-f681-44e8-816d-1cba11f1eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "while desc['InferenceComponentStatus'] == 'Updating':\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.6\n",
    "    }\n",
    "    try:\n",
    "        response_model = sagemaker_runtime_client.invoke_endpoint( \n",
    "            InferenceComponentName=inference_component_name,\n",
    "            EndpointName=endpoint_name, \n",
    "            Body=json.dumps(payload), \n",
    "            ContentType=\"application/json\", Accept=\"application/json\")\n",
    "        print(json.loads(response_model['Body'].read().decode('utf-8'))['choices'][0]['message']['content'])\n",
    "    except:\n",
    "        print('invocation failed')\n",
    "    alarm_info = cloudwatch.describe_alarms(\n",
    "        AlarmNames=[\n",
    "            f'SageMaker-{endpoint_name}-4xx-errors'\n",
    "        ],\n",
    "    )\n",
    "    print(f\"Alarm state:{alarm_info['MetricAlarms'][0]['StateValue']}\")\n",
    "    \n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    print(desc['InferenceComponentStatus'])\n",
    "    print(desc['RuntimeConfig'])\n",
    "    time.sleep(10)\n",
    "\n",
    "response_model = sagemaker_runtime_client.invoke_endpoint( \n",
    "        InferenceComponentName=inference_component_name,\n",
    "        EndpointName=endpoint_name, \n",
    "        Body=json.dumps(payload), \n",
    "        ContentType=\"application/json\", Accept=\"application/json\")\n",
    "    \n",
    "print(json.loads(response_model['Body'].read().decode('utf-8'))['choices'][0]['message']['content'])\n",
    "print(desc['InferenceComponentStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943af01-d5e8-4343-ba7c-b5fbee23b952",
   "metadata": {},
   "source": [
    "Once the Inference Component has successfully updated, we can call `DescribeInferenceComponent` and see that the parameters now reflect our most recent update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359c856-6b33-498b-b6f9-e81f4bf407fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4816ed3-91ae-4e4b-98ca-b58f98ccebdb",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Finally we delete our resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d2210-9f80-4c45-ab7b-85af078927ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "cloudwatch.delete_alarms(AlarmNames=[f'SageMaker-{endpoint_name}-4xx-errors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d8683-a174-4f46-bcb4-8963e6a94c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
