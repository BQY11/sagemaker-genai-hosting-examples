{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Using SageMaker Efficient Multi-Adapter Serving to host LoRA adapters at Scale\n",
    "\n",
    "Multi-Adapter serving allows for multiple fine-tuned models to be hosted in a cost efficient manner on a singular endpoint. Via a multi-adapter approach we can tackle multiple different tasks with a singular base LLM. In this example you will use a pre-trained LoRA adapter that was fine tuned from Llama 3.1 8B Instruct on the [ECTSum dataset](https://huggingface.co/datasets/mrSoul7766/ECTSum).\n",
    "\n",
    "You will also see how to dynamically load these adapters using [SageMaker Inference Components](https://aws.amazon.com/blogs/aws/amazon-sagemaker-adds-new-inference-capabilities-to-help-reduce-foundation-model-deployment-costs-and-latency/), in this example we specifically explore the Inference Component Adapter feature which will allow for us to load hundreds of adapters on a SageMaker real-time endpoint.\n",
    "\n",
    "![](./images/ic-adapter-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974eee7-cc31-4dac-8795-aec6b8765051",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b2b57-37e9-4c8b-882e-8c728c01dc70",
   "metadata": {},
   "source": [
    "### Fetch and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.35.68 --quiet --upgrade\n",
    "!pip install sagemaker==2.235.2 --quiet --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3504dde-39c4-411f-86c9-c473d46b0831",
   "metadata": {},
   "source": [
    "## Restart kernel before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a27613-3975-4ddd-80ee-d22990647978",
   "metadata": {},
   "source": [
    "### Configure development environment and boto3 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eba2f6-e1b6-41c6-94d1-2b2bfbe3308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "sm_runtime = boto3.client(service_name=\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 2: Create a model, endpoint configuration and endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1973d9d-b1d2-4457-bcff-3856b09a4649",
   "metadata": {},
   "source": [
    "### Download your model\n",
    "\n",
    "Download the base model from the HuggingFace model hub. Since [Llama 3.1 8B Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) is a gated model, you will need a HuggingFace access token and submit a request for model access on the model page. Follow the [HuggingFace instructions on accessing gated models for details](https://huggingface.co/docs/transformers.js/en/guides/private)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cc8de-a24f-4545-b9a0-15f2efba0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"<<YOUR HF TOKEN HERE>>\"\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_id_pathsafe = model_id.replace(\"/\",\"-\")\n",
    "local_model_path = f\"./models/{model_id_pathsafe}\"\n",
    "s3_model_path = f\"s3://{bucket}/models/{model_id_pathsafe}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aec636-94e8-43df-b295-481e40221884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=model_id, use_auth_token=HF_TOKEN, local_dir=local_model_path, allow_patterns=[\"*.json\", \"*.safetensors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9258b-8d98-4470-aeea-d85b20dfc750",
   "metadata": {},
   "source": [
    "Copy your model artifact to S3 to improve model load time during deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91d478-59cd-4279-a345-12f2bed164a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive {local_model_path} {s3_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52d91b-af19-4763-8584-0e1c8f3d531e",
   "metadata": {},
   "source": [
    "### Select a Large Model Inference (LMI) container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbef0bb-14ce-47e9-b99f-7cdd4afe7f7c",
   "metadata": {},
   "source": [
    "Select one of the [available Large Model Inference (LMI) container images for hosting](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers). Efficient adapter inference capability is available in `0.31.0-lmi13.0.0` and higher. Ensure that you are using the image URI for the region that corresponds with your deployment region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\"\n",
    "\n",
    "print(f\"Inference container image:: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09491292-b7cc-47ac-a1ca-136b5ad5fa86",
   "metadata": {},
   "source": [
    "### Configure model container environment\n",
    "\n",
    "Create an container environment for the hosting container. LMI container parameters can be found in the [LMI User Guides](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/index.html).\n",
    "\n",
    "By using the `OPTION_MAX_LORAS` and `OPTION_MAX_CPU_LORAS` parameters, you can control how adapters are loaded and unloaded into GPU/CPU memory. The `OPTION_MAX_LORAS` parameter defines the number of adapters that will be held in GPU memory, and any additional adapters will be offloaded to CPU memory. The `OPTION_MAX_CPU_LORAS` parameter controls the number of adapters that will be held in CPU memory, with any additional adapters being offloaded to local SSD. In the following example, the container will hold 30 adapters in GPU memory, and 70 adapters in CPU memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352195bd-faab-4e0d-8c2e-49d0202bfbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    \"HF_MODEL_ID\": f\"{s3_model_path}\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_ENABLE_LORA\": \"true\",\n",
    "    \"OPTION_MAX_LORAS\": \"30\",\n",
    "    \"OPTION_MAX_CPU_LORAS\": \"70\",\n",
    "    \"OPTION_DTYPE\": \"fp16\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"6000\"\n",
    "}\n",
    "\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3cb52-1e73-4b02-bc9d-9d5de5db4f9c",
   "metadata": {},
   "source": [
    "### Create a model object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e75e15-301d-4abc-b26d-5299d2c98097",
   "metadata": {},
   "source": [
    "With your container image and environment defined, you can create a SageMaker model object that you will use to create an inference component later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13a02d-4e4f-4b1e-84a6-5f9d908f4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = sagemaker.utils.name_from_base(\"llama-3-1-8b-instruct\")\n",
    "print(f'Model name: {model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66472473-8b8f-4db4-996c-e7de9487b100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": inference_image_uri, \n",
    "        \"Environment\": env,\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model ARN: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a25b5a-44b4-47d0-9762-a0453f0f1036",
   "metadata": {},
   "source": [
    "### Create an endpoint configuration\n",
    "\n",
    "To create a SageMaker endpoint, you need an endpoint configuration. When using Inference Components, you do not specify a model in the endpoint configuration. You will load the model as a component later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de370a4c-e522-4e34-9c6b-69ad435147ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variant name and instance type for hosting\n",
    "endpoint_config_name = f\"{model_name}\"\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "model_data_download_timeout_in_seconds = 900\n",
    "container_startup_health_check_timeout_in_seconds = 900\n",
    "\n",
    "initial_instance_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d67461-7753-4c92-8179-88c47cd11120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    ProductionVariants = [\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": initial_instance_count,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Created Endpoint Config ARN: {create_endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda0c0a-7e70-440d-a916-78804a6231ae",
   "metadata": {},
   "source": [
    "### Create inference endpoint\n",
    "\n",
    "Create your empty SageMaker endpoint. You will use this to host your base model and adapter inference components later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993dd7b-792e-4992-add7-8d3d49492cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = f\"{model_name}\"\n",
    "\n",
    "print(f'Endpoint name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b56f3-ba5a-4423-ba75-bdc40f597ba6",
   "metadata": {},
   "source": [
    "#### This step can take around 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0a6c8-a1dc-4e4d-81a8-526c723231dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName = endpoint_name, EndpointConfigName = endpoint_config_name\n",
    ")\n",
    "\n",
    "sess.wait_for_endpoint(endpoint_name)\n",
    "\n",
    "print(f\"Created Endpoint ARN: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f32dda-db04-459c-97af-010f214473d0",
   "metadata": {},
   "source": [
    "### Create base model inference component\n",
    "\n",
    "With your endpoint created, you can now create the IC for the base model. This will be the base component that the adapter components you create later will depend on. \n",
    "\n",
    "Notable parameters here are [`ComputeResourceRequirements`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_InferenceComponentComputeResourceRequirements.html). These are a component level configuration that determine the amount of resources that the component needs (Memory, vCPUs, Accelerators). The adapters will share these resources with the base component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8019d-f8ed-4408-9e94-8c4351da65a6",
   "metadata": {},
   "source": [
    "#### This step can take around 7 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318969f4-ee22-421a-91bc-60db8975ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "base_inference_component_name = f\"base-{model_name}\"\n",
    "print(f\"Base inference component name: {base_inference_component_name}\")\n",
    "\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "initial_copy_count = 1\n",
    "min_memory_required_in_mb = 32000\n",
    "number_of_accelerator_devices_required = 4\n",
    "\n",
    "base_create_inference_component_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName = base_inference_component_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    VariantName = variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"MinMemoryRequiredInMb\": min_memory_required_in_mb,\n",
    "            \"NumberOfAcceleratorDevicesRequired\": number_of_accelerator_devices_required,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": initial_copy_count,\n",
    "    },\n",
    ")\n",
    "\n",
    "sess.wait_for_inference_component(base_inference_component_name)\n",
    "\n",
    "print(f\"Created Base inference component ARN: {base_create_inference_component_response['InferenceComponentArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761490c5-fd56-4251-88ca-4479a7254032",
   "metadata": {},
   "source": [
    "### View logs for the base inference component (and adapters after they're loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5d691-7ca6-4947-8286-bade442fc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "cw_path = urllib.parse.quote_plus(f'/aws/sagemaker/InferenceComponents/{base_inference_component_name}', safe='', encoding=None, errors=None)\n",
    "\n",
    "print(f'You can view your inference component logs here:\\n\\n https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logsV2:log-groups/log-group/{cw_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0f789-57fc-4016-8758-5b60ac7d797c",
   "metadata": {},
   "source": [
    "### Create the Inference Components (ICs) for the adapters\n",
    "\n",
    "In this example you’ll create a single adapter, but you could host up to hundreds of them per endpoint. They will need to be compressed and uploaded to S3.\n",
    "\n",
    "The adapter package has the following files at the root of the archive with no sub-folders:\n",
    "\n",
    "![](./images/adapter_files.png)\n",
    "\n",
    "For this example, an adapter was fine tuned using QLoRA and [Fully Sharded Data Parallel (FSDP)](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-v2.html) on the training split of the [ECTSum dataset](https://huggingface.co/datasets/mrSoul7766/ECTSum). Training took 21 minutes on a ml.p4d.24xlarge and cost ~$13 using current [on-demand pricing](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed24d7-5af5-4703-aaed-bc1ca2515de6",
   "metadata": {},
   "source": [
    "#### Copy locally downloaded adapters to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc5c97-e2cd-4665-8174-33c718035f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ectsum_adapter_filename = \"ectsum-adapter.tar.gz\"\n",
    "ectsum_adapter_s3_uri = f\"s3://{bucket}/adapters/{ectsum_adapter_filename}\"\n",
    "\n",
    "!aws s3 cp ./adapters/{ectsum_adapter_filename} {ectsum_adapter_s3_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46534a-056a-4710-9cdd-d71bd42b883b",
   "metadata": {},
   "source": [
    "### Create ECTSum adapter inference component\n",
    "\n",
    "For each adapter you are going to deploy, you need to specify an `InferenceComponentName`, an `ArtifactUrl` with the S3 location of the adapter archive, and a `BaseInferenceComponentName` to create the connection between the base model IC and the new adapter ICs. You will repeat this process for each additional adapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e64b79-605c-4888-9ec0-6e395550b13c",
   "metadata": {},
   "source": [
    "#### This step can take around 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade7085-0d71-4ac3-a4cc-a96774a57f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_ectsum_name = f\"ic-ectsum-{base_inference_component_name}\"\n",
    "\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName = ic_ectsum_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    Specification={\n",
    "        \"BaseInferenceComponentName\": base_inference_component_name,\n",
    "        \"Container\": {\n",
    "            \"ArtifactUrl\": ectsum_adapter_s3_uri\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "sess.wait_for_inference_component(ic_ectsum_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49ff5d-9ef8-4bd5-9537-bb057a6d941e",
   "metadata": {},
   "source": [
    "Look at base inference component logs again.\n",
    "\n",
    "It should show a line that looks like:\n",
    "\n",
    "`Registered adapter <ADAPTER_NAME> from /opt/ml/models/ ... successfully`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be20f4-15ba-4659-b8a9-358e79e7c119",
   "metadata": {},
   "source": [
    "## Step 3: Invoking the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd6344-6ecc-4abd-90e5-1dac9c0c9d5d",
   "metadata": {},
   "source": [
    "First you will pull a random datapoint form the ECTSum test split. You'll use the `text` field to invoke the model and the `summary` filed to compare with ground truth later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b84715-2d62-4180-9ab9-4d257be94b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"mrSoul7766/ECTSum\"\n",
    "\n",
    "test_dataset = load_dataset(dataset_name, trust_remote_code=True, split=\"test\")\n",
    "\n",
    "test_item = test_dataset.shuffle().select(range(1))\n",
    "\n",
    "ground_truth_response = test_item[\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b32ab6-38ec-4137-bf1c-0b30aa25f32a",
   "metadata": {},
   "source": [
    "Next you will build a prompt to invoke the model for earnings summarization, filling in the source text with a random item from the ECTSum dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688e97f-92be-426b-9e34-cf90c8e51d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                You are an AI assistant trained to summarize earnings calls. Provide a concise summary of the call, capturing the key points and overall context. Focus on quarter over quarter revenue, earnings per share, changes in debt, highlighted risks, and growth opportunities.\n",
    "                <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "                Summarize the following earnings call:\n",
    "\n",
    "                {test_item[\"text\"]}\n",
    "                <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df59c49c-e5d0-4567-b301-7d36bf4a70b9",
   "metadata": {},
   "source": [
    "### Plain base model with no adapters\n",
    "\n",
    "To test the base model, specify the `EndpointName` for the endpoint you created earlier and the name of the base inference component as `InferenceComponentName` along with your prompt and other inference parameters in the `Body` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ac417-6a69-4ad9-a311-0bc7f0de5b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "component_to_invoke = base_inference_component_name\n",
    "\n",
    "response_model = sm_runtime.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    InferenceComponentName = component_to_invoke,\n",
    "    Body = json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 125, \"temperature\":0.9}\n",
    "        }\n",
    "    ),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "base_response = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))[\"generated_text\"]\n",
    "\n",
    "print(f'Ground Truth:\\n\\n {test_item[\"summary\"]}\\n\\n')\n",
    "print(f'Base Model Response:\\n\\n {base_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcad263-cf02-464b-ade8-6a9753a955b4",
   "metadata": {},
   "source": [
    "### Invoke ECTSum adapter\n",
    "\n",
    "To invoke the adapter, use the adapter inference component name in your `invoke_endpoint` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500fed91-cdb1-4756-accb-621ca3c5fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "component_to_invoke = ic_ectsum_name\n",
    "\n",
    "response_model = sm_runtime.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    InferenceComponentName = component_to_invoke,\n",
    "    Body = json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 125, \"temperature\":0.9}\n",
    "        }\n",
    "    ),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "adapter_response = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))[\"generated_text\"]\n",
    "\n",
    "print(f'Ground Truth:\\n\\n {test_item[\"summary\"]}\\n\\n')\n",
    "print(f'Adapter Model Response:\\n\\n {adapter_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfac2aa-94b0-41db-9a41-e974c16ac1ba",
   "metadata": {},
   "source": [
    "### Compare outputs\n",
    "\n",
    "Compare the outputs of the base model and adapter to ground truth. In this test, notice that while the base model looks subjectively more visually attractive, the adapter response is significantly closer to ground truth; which is what you are looking for. This will be proven with metrics in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480b7f7-0d1f-482e-bbfb-0bf2ea1e87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ground Truth:\\n\\n {test_item[\"summary\"][0]}\\n\\n')\n",
    "print(\"\\n----------------------------------\\n\")\n",
    "print(f'Base Model Response:\\n\\n {base_response}')\n",
    "print(\"\\n----------------------------------\\n\")\n",
    "print(f'Adapter Model Response:\\n\\n {adapter_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8fab41-e8b0-45e3-b2ca-e3ac52762531",
   "metadata": {},
   "source": [
    "To validate the true adapter performance, you can use a tool like [fmeval](https://github.com/aws/fmeval) to run an evaluation of summarization accuracy. This will calculate the METEOR, ROUGE, and BertScore metrics for the adapter versus the base model. Doing so against the test split of ECTSum yields the following results:\n",
    "\n",
    "![](./images/fmeval-overall.png)\n",
    "\n",
    "The fine-tuned adapter shows a 59% increase in METEOR score, 159% increase in ROUGE score, and 8.6% in BertScore. The following diagram shows the frequency distribution of scores for the different metrics, with the adapter consistently scoring better more often in all metrics. \n",
    "\n",
    "Model latency is largely unaffected, with only a difference of 2% between direct base model invocation and the adapter.\n",
    "\n",
    "![](./images/fmeval-histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6c5f2-de0e-4ac6-b945-f78b71372f1e",
   "metadata": {},
   "source": [
    "### Upload a new ECTSum adapter artifact and update the live adapter inference component\n",
    "\n",
    "Since adapters are managed as Inference Components, you can update them on a running endpoint. SageMaker handles the unloading/deregistering of the old adapter and loading/registering of the new adapter onto every base ICs on all of the instances that it is running on for this endpoint. To update an adapter IC, use the  update_inference_component  API and supply the existing IC name and the S3 path to the new compressed adapter archive. \n",
    "\n",
    "You can train a new adapter, or re-upload the existing adapter artifact to test this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd902155-20ce-4fc4-9263-0403d1df6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ectsum_adapter_s3_uri = f\"s3://{bucket}/lora-adapters/new-ectsum-adapter.tar.gz\"\n",
    "\n",
    "!aws s3 cp ./adapters/ectsum-adapter.tar.gz {new_prt_adapter_s3_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acecc06-6235-478e-b65b-e1a9a471f5a4",
   "metadata": {},
   "source": [
    "#### This step can take around 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccb5b6-c730-4eb9-9165-3a749521afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "update_inference_component_response = sm_client.update_inference_component(\n",
    "    InferenceComponentName = ic_ectsum_name,\n",
    "    Specification={\n",
    "        \"Container\": {\n",
    "            \"ArtifactUrl\": new_ectsum_adapter_s3_uri\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "sess.wait_for_inference_component(ic_ectsum_name)\n",
    "\n",
    "print(f'Updated inference component adapter ARN: {update_inference_component_response[\"InferenceComponentArn\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358595b2-67d1-469e-b6c9-1d504e04a65a",
   "metadata": {},
   "source": [
    "If you view your inference component logs (link below), you will see log entries for the deregistration of the old adapter and the registration of the new one.\n",
    "\n",
    "You should see something similar to:\n",
    "`[INFO ] PyProcess - W-200-0d1e4741a42db26-stdout: [1,0]<stdout>:INFO::Unregistered adapter ic-ectsum-base-llama-3-1-8b-instruct-2024-11-25-20-41-07-401 successfully`\n",
    "\n",
    "`[INFO ] PyProcess - W-200-0d1e4741a42db26-stdout: [1,0]<stdout>:INFO::Registered adapter ic-ectsum-base-llama-3-1-8b-instruct-2024-11-25-20-41-07-401 from /opt/ml/models/container_340043819279-ic-ectsum-base-llama-3-1-8b-instruct-2024-11-25-20-41-07-401-1732570150851-MaeveWestworldService-1.0.9353.0 successfully`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49a2ad-56f8-4004-9eeb-295524ab6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'You can view your inference component logs here:\\n\\n https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logsV2:log-groups/log-group/{cw_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366a4b7-c9d8-499c-9f4a-35b4d074cf6f",
   "metadata": {},
   "source": [
    "### Retest with updated adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15b342-79bc-4b1c-ab21-9a1fbc34f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "component_to_invoke = ic_ectsum_name\n",
    "\n",
    "response_model = sm_runtime.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    InferenceComponentName = component_to_invoke,\n",
    "    Body = json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 125, \"temperature\":0.9}\n",
    "        }\n",
    "    ),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Step 4: Clean up the environment\n",
    "\n",
    "If you need to delete an adapter, call the `delete_inference_component` API with the IC name to remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1abd0-eb91-4717-a116-a87f81654fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_inference_component(ic_ectsum_name, wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65ab56-3290-4dde-84bb-4c8efcaf87d9",
   "metadata": {},
   "source": [
    "Deleting the base model IC will automatically delete the base IC and any associated adapter ICs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d8b60-3aea-493b-b257-874a5e4c05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_inference_component(base_inference_component_name, wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2a06b-7f59-4a12-be34-467e313d2ab0",
   "metadata": {},
   "source": [
    "Clean up the running endpoint and its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
